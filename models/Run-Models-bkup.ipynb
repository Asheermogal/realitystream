{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNYDh2lRKAie"
      },
      "source": [
        "# **Run Models Colab**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxZiI7xcrT4B"
      },
      "source": [
        "# Our 6 Models include Random Forests and XGBoost\n",
        "\n",
        "ONLY ADD GENERIC PROCESSES HERE. ALL CUSTOM SETTINGS ARRIVE VIA parameters.yaml, which is pasted several steps down.\n",
        "\n",
        "We're gradually makeing reusable process for putput added here for FOREST COVERAGE and BEE POPULATION HEALTH.\n",
        "\n",
        "Choose models to run at https://model.earth/realitystream/models  \n",
        "Documentation https://model.earth/realitystream  \n",
        "Backup resides in the: [RealityStream models folder](https://github.com/ModelEarth/realitystream/tree/main/models)  \n",
        "Notes on running locally and in the cloud reside in our [cloud repo](https://github.com/modelearth/cloud/).\n",
        "\n",
        "PARTIALLY DONE: Add files to a \"report\" folder. We send it's content to GitHub in the last step.\n",
        "\n",
        "TO DO: For a unified html report, as each model completes, update a dataset with the performance accuracy scores and other metrics for all the models, then send to a report.md file. Save in the left 'report' folder which we push to Github in last step.\n",
        "\n",
        "DONE: Include the time it took to run each model in report.md. - TARUN\n",
        "\n",
        "DONE: Generate features-importance reports for available models. - Bin(Melody)\n",
        "\n",
        "DONE: Performance metrics—including accuracy, ROC-AUC, G-Mean, best threshold, and classification reports—were aggregated into a modelResults dictionary using abbreviated keys. Top 10 feature importances for applicable models were included, and results were formatted into summary tables. - Yogesh Gajula\n",
        "\n",
        "DONE: Function to calculate and append Correlation values to Unified Aggregation Results and Visual chart with prefix's for the top 10 Feature importances. - Yogesh Gajula\n",
        "\n",
        "TO DO: Fix the error: name 'save_dir' is not defined. - Is this still occuring?  \n",
        "Occured previously for both rbf and xgboost. Maybe others.\n",
        "\n",
        "DONE Aashish: Used Pandas for integrated_df (became df) when save_training = False.  \n",
        "DONE Loren: Loaded parameters.yaml and saved locally for customization.  \n",
        "https://chatgpt.com/share/e4a2ee73-ab74-4551-9868-37b9b5b6b359  \n",
        "DONE Tarun: Allow save-training to be set in the parameters.yaml values. Default to false. Use dash instead of underscore in yaml.\n",
        "\n",
        "TO DO: Test that default target path for bee data works by deleting in left panel after pullin in parameters.yaml. Then test that panels 15 and 16 work.  \n",
        "if param.targets.path: # Override with value from yaml  \n",
        "    target_url = param.targets.path\n",
        "\n",
        "target_df got problems.\n",
        "\n",
        "TO DO: Pull 2-column target zip code UN topics directly from Google Data Commons based DCID target value in parameters.yaml\n",
        "\n",
        "DONE Ivy: In the same panel as each accuracy report, call a new function called displayModelHeader to display the model name (as a bold header) and the file paths for features and targets above the report.\n",
        "\n",
        "DONE Ivy: Show the parameter values below each path at the top of each accuracy report. So under the Feature path we'd have:  \n",
        "startyear: 2017, endyear: 2021, naics: [6], state: ME\n",
        "\n",
        "DONE Lily: Add support for multiple states. After running the third panel, you can edit the custom yaml on the right to set state: CT, ME, MA, NH, RI, VT.  Then add a loop that runs when there are multiple states. We'll add a file called parameters-new-england.yaml in the root of the RealityStream repo with the six states as features.states. Load here and add python to loop through the states.\n",
        "\n",
        "TO DO: Add more parameters.yaml files that pull features/targets and join on the county Fips column. Add a path parameter that pulls from \"all-years\" which are generated by our [Industry Features CoLab](https://colab.research.google.com/drive/1HJnuilyEFjBpZLrgxDa4S0diekwMeqnh?usp=sharing). All years on GitHub:  \n",
        "https://github.com/ModelEarth/community-timelines/tree/main/training/all-years\n",
        "(These were created by Ronan)\n",
        "\n",
        "DONE: Load blinks/parameters-blinks.yaml and use target.column to limit to y column\n",
        "\n",
        "SAVE FOR LATER: Dropdown in webpage to send parameters.yaml 1 of these 4 bee targets (years).  \n",
        "https://github.com/ModelEarth/bee-data/tree/main/targets\n",
        "\n",
        "Done: Avoid sorting incoming parameters.yaml alphabetically. Attempt using  OrderedDict is commented out is several places below. Comment out prior alphabetical technique - we can provide a bool to toggle to it if it provides better security when requests are submitted through webpages. - Soham\n",
        "\n",
        "DONE: Only import models requested by parameters.yaml. Move \"from sklearn\" imports to step after parameters are edited in textbox. - Tarun\n",
        "\n",
        "IN PROGRESS: Creating install for Flask application with Google Cloud Run cmds at [github.com/modelearth/cloud](https://github.com/modelearth/cloud)\n",
        "\n",
        "DONE: Send the params loaded from the default path to the widget diplay. - Prathyusha\n",
        "\n",
        "DONE: Create an object that holds the 5 sample parameters.yaml paths that are on the RealityStream main page. When choosing one, send the path and the yaml it points at to the textarea below the path select menu. - Prathyusha\n",
        "\n",
        "DONE: Parameter files displayed in select menu. Instead pull the select options from parameter-paths.csv - Prathyusha\n",
        "\n",
        "DONE: Deactivate the right-side display of the yaml values and have the editing occur in the widget textbox. - Melody\n",
        "\n",
        "TODO: Find a way to delete the existing files in the colab environment which interfers with the code when we re-run\n",
        "\n",
        "TODO: imblearn import for cuML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HABqp9DCVQ1n"
      },
      "source": [
        " ⚠️ Please change your runtime type to T4 GPU under Runtime > Change runtime type."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries and Intital Set-up"
      ],
      "metadata": {
        "id": "LKV4En8ueA1E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing dependencies"
      ],
      "metadata": {
        "id": "xBVNQ4fvEPr9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToYmR0DBMLxv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21dd3385-657c-45c5-fb1b-d37647105675"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: jax 0.5.3\n",
            "Uninstalling jax-0.5.3:\n",
            "  Successfully uninstalled jax-0.5.3\n",
            "Found existing installation: jaxlib 0.5.3\n",
            "Uninstalling jaxlib-0.5.3:\n",
            "  Successfully uninstalled jaxlib-0.5.3\n",
            "Found existing installation: tensorflow 2.19.0\n",
            "Uninstalling tensorflow-2.19.0:\n",
            "  Successfully uninstalled tensorflow-2.19.0\n",
            "Found existing installation: treescope 0.1.10\n",
            "Uninstalling treescope-0.1.10:\n",
            "  Successfully uninstalled treescope-0.1.10\n",
            "Found existing installation: pymc 5.25.1\n",
            "Uninstalling pymc-5.25.1:\n",
            "  Successfully uninstalled pymc-5.25.1\n",
            "Found existing installation: thinc 8.3.6\n",
            "Uninstalling thinc-8.3.6:\n",
            "  Successfully uninstalled thinc-8.3.6\n",
            "Found existing installation: flax 0.10.6\n",
            "Uninstalling flax-0.10.6:\n",
            "  Successfully uninstalled flax-0.10.6\n",
            "Found existing installation: optax 0.2.5\n",
            "Uninstalling optax-0.2.5:\n",
            "  Successfully uninstalled optax-0.2.5\n",
            "Found existing installation: chex 0.1.90\n",
            "Uninstalling chex-0.1.90:\n",
            "  Successfully uninstalled chex-0.1.90\n",
            "Found existing installation: orbax-checkpoint 0.11.24\n",
            "Uninstalling orbax-checkpoint-0.11.24:\n",
            "  Successfully uninstalled orbax-checkpoint-0.11.24\n",
            "Found existing installation: dopamine_rl 4.1.2\n",
            "Uninstalling dopamine_rl-4.1.2:\n",
            "  Successfully uninstalled dopamine_rl-4.1.2\n",
            "Found existing installation: tensorflow_decision_forests 1.12.0\n",
            "Uninstalling tensorflow_decision_forests-1.12.0:\n",
            "  Successfully uninstalled tensorflow_decision_forests-1.12.0\n",
            "Found existing installation: tables 3.10.2\n",
            "Uninstalling tables-3.10.2:\n",
            "  Successfully uninstalled tables-3.10.2\n",
            "Found existing installation: spacy 3.8.7\n",
            "Uninstalling spacy-3.8.7:\n",
            "  Successfully uninstalled spacy-3.8.7\n",
            "Found existing installation: mlxtend 0.23.4\n",
            "Uninstalling mlxtend-0.23.4:\n",
            "  Successfully uninstalled mlxtend-0.23.4\n",
            "Found existing installation: fastai 2.8.4\n",
            "Uninstalling fastai-2.8.4:\n",
            "  Successfully uninstalled fastai-2.8.4\n",
            "Found existing installation: blosc2 3.7.2\n",
            "Uninstalling blosc2-3.7.2:\n",
            "  Successfully uninstalled blosc2-3.7.2\n",
            "Finished: uninstall\n",
            "Collecting numpy<3.0a0\n",
            "  Downloading numpy-2.3.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.1/62.1 kB 2.9 MB/s eta 0:00:00\n",
            "Collecting scikit-learn<1.6,>=1.4\n",
            "  Downloading scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting imbalanced-learn<0.13,>=0.12\n",
            "  Downloading imbalanced_learn-0.12.4-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting scipy>=1.6.0 (from scikit-learn<1.6,>=1.4)\n",
            "  Downloading scipy-1.16.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.0/62.0 kB 5.6 MB/s eta 0:00:00\n",
            "Collecting joblib>=1.2.0 (from scikit-learn<1.6,>=1.4)\n",
            "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn<1.6,>=1.4)\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading numpy-2.3.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.6/16.6 MB 98.5 MB/s eta 0:00:00\n",
            "Downloading scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.9/12.9 MB 124.6 MB/s eta 0:00:00\n",
            "Downloading imbalanced_learn-0.12.4-py3-none-any.whl (258 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 258.3/258.3 kB 26.5 MB/s eta 0:00:00\n",
            "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 308.4/308.4 kB 26.3 MB/s eta 0:00:00\n",
            "Downloading scipy-1.16.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.2 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 35.2/35.2 MB 17.1 MB/s eta 0:00:00\n",
            "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: threadpoolctl, numpy, joblib, scipy, scikit-learn, imbalanced-learn\n",
            "  Attempting uninstall: threadpoolctl\n",
            "    Found existing installation: threadpoolctl 3.6.0\n",
            "    Uninstalling threadpoolctl-3.6.0:\n",
            "      Successfully uninstalled threadpoolctl-3.6.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.5.2\n",
            "    Uninstalling joblib-1.5.2:\n",
            "      Successfully uninstalled joblib-1.5.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.1\n",
            "    Uninstalling scipy-1.16.1:\n",
            "      Successfully uninstalled scipy-1.16.1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "  Attempting uninstall: imbalanced-learn\n",
            "    Found existing installation: imbalanced-learn 0.14.0\n",
            "    Uninstalling imbalanced-learn-0.14.0:\n",
            "      Successfully uninstalled imbalanced-learn-0.14.0\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.2 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.2 which is incompatible.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.2 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.2 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.2 which is incompatible.\n",
            "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.5.2 which is incompatible.\n",
            "Successfully installed imbalanced-learn-0.12.4 joblib-1.5.2 numpy-2.3.2 scikit-learn-1.5.2 scipy-1.16.1 threadpoolctl-3.6.0\n",
            "Finished: install (NumPy/sklearn/imbalanced-learn)\n",
            "Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n",
            "Collecting cudf-cu12==25.2.*\n",
            "  Downloading https://pypi.nvidia.com/cudf-cu12/cudf_cu12-25.2.2-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (2.2 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/2.2 MB 47.2 MB/s eta 0:00:00\n",
            "Collecting cuml-cu12==25.2.*\n",
            "  Downloading https://pypi.nvidia.com/cuml-cu12/cuml_cu12-25.2.1-cp312-cp312-manylinux_2_28_x86_64.whl (9.5 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.5/9.5 MB 159.5 MB/s eta 0:00:00\n",
            "Collecting dask-cudf-cu12==25.2.*\n",
            "  Downloading https://pypi.nvidia.com/dask-cudf-cu12/dask_cudf_cu12-25.2.2-py3-none-any.whl (50 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.4/50.4 kB 86.9 MB/s eta 0:00:00\n",
            "Collecting dask-cuda==25.2.*\n",
            "  Downloading https://pypi.nvidia.com/dask-cuda/dask_cuda-25.2.0-py3-none-any.whl (133 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.9/133.9 kB 103.5 MB/s eta 0:00:00\n",
            "Collecting rapids-dask-dependency==25.2.*\n",
            "  Downloading https://pypi.nvidia.com/rapids-dask-dependency/rapids_dask_dependency-25.2.0-py3-none-any.whl (22 kB)\n",
            "Collecting raft-dask-cu12==25.2.*\n",
            "  Downloading https://pypi.nvidia.com/raft-dask-cu12/raft_dask_cu12-25.2.0-cp312-cp312-manylinux_2_28_x86_64.whl (293.5 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 293.5/293.5 MB 106.7 MB/s eta 0:00:00\n",
            "Collecting rmm-cu12==25.2.*\n",
            "  Downloading https://pypi.nvidia.com/rmm-cu12/rmm_cu12-25.2.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (2.4 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/2.4 MB 212.6 MB/s eta 0:00:00\n",
            "Collecting librmm-cu12==25.2.*\n",
            "  Downloading https://pypi.nvidia.com/librmm-cu12/librmm_cu12-25.2.0-py3-none-any.whl (4.2 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.2/4.2 MB 121.0 MB/s eta 0:00:00\n",
            "Collecting pylibcudf-cu12==25.2.*\n",
            "  Downloading https://pypi.nvidia.com/pylibcudf-cu12/pylibcudf_cu12-25.2.2-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.2 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 27.2/27.2 MB 171.6 MB/s eta 0:00:00\n",
            "Collecting libraft-cu12==25.2.*\n",
            "  Downloading https://pypi.nvidia.com/libraft-cu12/libraft_cu12-25.2.0-py3-none-manylinux_2_28_x86_64.whl (22.3 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.3/22.3 MB 158.0 MB/s eta 0:00:00\n",
            "Collecting pylibraft-cu12==25.2.*\n",
            "  Downloading https://pypi.nvidia.com/pylibraft-cu12/pylibraft_cu12-25.2.0-cp312-cp312-manylinux_2_28_x86_64.whl (851 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 851.2/851.2 kB 208.7 MB/s eta 0:00:00\n",
            "Collecting libcuvs-cu12==25.2.*\n",
            "  Downloading https://pypi.nvidia.com/libcuvs-cu12/libcuvs_cu12-25.2.1-py3-none-manylinux_2_28_x86_64.whl (1184.5 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 GB 43.6 MB/s eta 0:00:00\n",
            "Collecting cuvs-cu12==25.2.*\n",
            "  Downloading https://pypi.nvidia.com/cuvs-cu12/cuvs_cu12-25.2.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (2.3 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 MB 196.6 MB/s eta 0:00:00\n",
            "Collecting ucx-py-cu12==0.42.*\n",
            "  Downloading https://pypi.nvidia.com/ucx-py-cu12/ucx_py_cu12-0.42.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (2.2 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/2.2 MB 150.0 MB/s eta 0:00:00\n",
            "Collecting ucxx-cu12==0.42.*\n",
            "  Downloading https://pypi.nvidia.com/ucxx-cu12/ucxx_cu12-0.42.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (712 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 712.7/712.7 kB 142.6 MB/s eta 0:00:00\n",
            "Collecting distributed-ucxx-cu12==0.42.*\n",
            "  Downloading https://pypi.nvidia.com/distributed-ucxx-cu12/distributed_ucxx_cu12-0.42.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from cudf-cu12==25.2.*) (5.5.2)\n",
            "Requirement already satisfied: cuda-python<13.0a0,>=12.6.2 in /usr/local/lib/python3.12/dist-packages (from cudf-cu12==25.2.*) (12.6.2.post1)\n",
            "Requirement already satisfied: cupy-cuda12x>=12.0.0 in /usr/local/lib/python3.12/dist-packages (from cudf-cu12==25.2.*) (13.3.0)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from cudf-cu12==25.2.*) (2025.3.0)\n",
            "Collecting libcudf-cu12==25.2.* (from cudf-cu12==25.2.*)\n",
            "  Downloading https://pypi.nvidia.com/libcudf-cu12/libcudf_cu12-25.2.2-py3-none-manylinux_2_28_x86_64.whl (557.7 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 557.7/557.7 MB 48.4 MB/s eta 0:00:00\n",
            "Collecting numba-cuda<0.3.0a0,>=0.2.0 (from cudf-cu12==25.2.*)\n",
            "  Downloading numba_cuda-0.2.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: numba<0.61.0a0,>=0.59.1 in /usr/local/lib/python3.12/dist-packages (from cudf-cu12==25.2.*) (0.60.0)\n",
            "Requirement already satisfied: numpy<3.0a0,>=1.23 in /usr/local/lib/python3.12/dist-packages (from cudf-cu12==25.2.*) (2.3.2)\n",
            "Requirement already satisfied: nvtx>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from cudf-cu12==25.2.*) (0.2.13)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from cudf-cu12==25.2.*) (25.0)\n",
            "Requirement already satisfied: pandas<2.2.4dev0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from cudf-cu12==25.2.*) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<20.0.0a0,>=14.0.0 in /usr/local/lib/python3.12/dist-packages (from cudf-cu12==25.2.*) (18.1.0)\n",
            "Requirement already satisfied: pynvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from cudf-cu12==25.2.*) (0.7.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from cudf-cu12==25.2.*) (13.9.4)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from cudf-cu12==25.2.*) (4.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.12/dist-packages (from cuml-cu12==25.2.*) (1.5.2)\n",
            "Collecting libcuml-cu12==25.2.* (from cuml-cu12==25.2.*)\n",
            "  Downloading https://pypi.nvidia.com/libcuml-cu12/libcuml_cu12-25.2.1-py3-none-manylinux_2_28_x86_64.whl (405.0 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 405.0/405.0 MB 91.6 MB/s eta 0:00:00\n",
            "Requirement already satisfied: nvidia-cublas-cu12 in /usr/local/lib/python3.12/dist-packages (from cuml-cu12==25.2.*) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12 in /usr/local/lib/python3.12/dist-packages (from cuml-cu12==25.2.*) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12 in /usr/local/lib/python3.12/dist-packages (from cuml-cu12==25.2.*) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12 in /usr/local/lib/python3.12/dist-packages (from cuml-cu12==25.2.*) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12 in /usr/local/lib/python3.12/dist-packages (from cuml-cu12==25.2.*) (12.5.4.2)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from cuml-cu12==25.2.*) (1.16.1)\n",
            "Requirement already satisfied: treelite==4.4.1 in /usr/local/lib/python3.12/dist-packages (from cuml-cu12==25.2.*) (4.4.1)\n",
            "Requirement already satisfied: pynvml<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.12/dist-packages (from dask-cudf-cu12==25.2.*) (12.0.0)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.12/dist-packages (from dask-cuda==25.2.*) (8.2.1)\n",
            "Requirement already satisfied: zict>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from dask-cuda==25.2.*) (3.0.0)\n",
            "Collecting dask==2024.12.1 (from rapids-dask-dependency==25.2.*)\n",
            "  Downloading dask-2024.12.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting distributed==2024.12.1 (from rapids-dask-dependency==25.2.*)\n",
            "  Downloading distributed-2024.12.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting dask-expr==1.1.21 (from rapids-dask-dependency==25.2.*)\n",
            "  Downloading dask_expr-1.1.21-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: libucx-cu12<1.19,>=1.15.0 in /usr/local/lib/python3.12/dist-packages (from ucx-py-cu12==0.42.*) (1.18.1)\n",
            "Collecting libucxx-cu12==0.42.* (from ucxx-cu12==0.42.*)\n",
            "  Downloading https://pypi.nvidia.com/libucxx-cu12/libucxx_cu12-0.42.0-py3-none-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (514 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 514.8/514.8 kB 232.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from dask==2024.12.1->rapids-dask-dependency==25.2.*) (3.1.1)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from dask==2024.12.1->rapids-dask-dependency==25.2.*) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from dask==2024.12.1->rapids-dask-dependency==25.2.*) (6.0.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from dask==2024.12.1->rapids-dask-dependency==25.2.*) (0.12.1)\n",
            "Requirement already satisfied: jinja2>=2.10.3 in /usr/local/lib/python3.12/dist-packages (from distributed==2024.12.1->rapids-dask-dependency==25.2.*) (3.1.6)\n",
            "Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from distributed==2024.12.1->rapids-dask-dependency==25.2.*) (1.0.0)\n",
            "Requirement already satisfied: msgpack>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from distributed==2024.12.1->rapids-dask-dependency==25.2.*) (1.1.1)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from distributed==2024.12.1->rapids-dask-dependency==25.2.*) (5.9.5)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.12/dist-packages (from distributed==2024.12.1->rapids-dask-dependency==25.2.*) (2.4.0)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from distributed==2024.12.1->rapids-dask-dependency==25.2.*) (3.1.0)\n",
            "Requirement already satisfied: tornado>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from distributed==2024.12.1->rapids-dask-dependency==25.2.*) (6.4.2)\n",
            "Requirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.12/dist-packages (from distributed==2024.12.1->rapids-dask-dependency==25.2.*) (2.5.0)\n",
            "Collecting libkvikio-cu12==25.2.* (from libcudf-cu12==25.2.*->cudf-cu12==25.2.*)\n",
            "  Downloading https://pypi.nvidia.com/libkvikio-cu12/libkvikio_cu12-25.2.1-py3-none-manylinux_2_28_x86_64.whl (2.1 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 193.8 MB/s eta 0:00:00\n",
            "Collecting nvidia-nvcomp-cu12==4.2.0.11 (from libcudf-cu12==25.2.*->cudf-cu12==25.2.*)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-nvcomp-cu12/nvidia_nvcomp_cu12-4.2.0.11-py3-none-manylinux_2_28_x86_64.whl (46.3 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 46.3/46.3 MB 128.8 MB/s eta 0:00:00\n",
            "Collecting numpy<3.0a0,>=1.23 (from cudf-cu12==25.2.*)\n",
            "  Downloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.0/62.0 kB 3.6 MB/s eta 0:00:00\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.12/dist-packages (from cupy-cuda12x>=12.0.0->cudf-cu12==25.2.*) (0.8.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba<0.61.0a0,>=0.59.1->cudf-cu12==25.2.*) (0.43.0)\n",
            "  Downloading numpy-2.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.9/60.9 kB 6.0 MB/s eta 0:00:00\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<2.2.4dev0,>=2.0->cudf-cu12==25.2.*) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<2.2.4dev0,>=2.0->cudf-cu12==25.2.*) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<2.2.4dev0,>=2.0->cudf-cu12==25.2.*) (2025.2)\n",
            "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.12/dist-packages (from pynvml<13.0.0a0,>=12.0.0->dask-cudf-cu12==25.2.*) (12.575.51)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cufft-cu12->cuml-cu12==25.2.*) (12.6.85)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->cudf-cu12==25.2.*) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->cudf-cu12==25.2.*) (2.19.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=2.10.3->distributed==2024.12.1->rapids-dask-dependency==25.2.*) (3.0.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->cudf-cu12==25.2.*) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<2.2.4dev0,>=2.0->cudf-cu12==25.2.*) (1.17.0)\n",
            "Downloading dask-2024.12.1-py3-none-any.whl (1.3 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 37.3 MB/s eta 0:00:00\n",
            "Downloading dask_expr-1.1.21-py3-none-any.whl (244 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 244.3/244.3 kB 25.7 MB/s eta 0:00:00\n",
            "Downloading distributed-2024.12.1-py3-none-any.whl (1.0 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 65.6 MB/s eta 0:00:00\n",
            "Downloading numba_cuda-0.2.0-py3-none-any.whl (443 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 443.7/443.7 kB 36.5 MB/s eta 0:00:00\n",
            "Downloading numpy-2.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.2 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.2/19.2 MB 23.9 MB/s eta 0:00:00\n",
            "Installing collected packages: librmm-cu12, libkvikio-cu12, nvidia-nvcomp-cu12, numpy, libucxx-cu12, ucx-py-cu12, rmm-cu12, libcudf-cu12, dask, ucxx-cu12, pylibcudf-cu12, numba-cuda, libraft-cu12, distributed, dask-expr, rapids-dask-dependency, pylibraft-cu12, libcuvs-cu12, cudf-cu12, libcuml-cu12, distributed-ucxx-cu12, dask-cudf-cu12, dask-cuda, cuvs-cu12, raft-dask-cu12, cuml-cu12\n",
            "  Attempting uninstall: librmm-cu12\n",
            "    Found existing installation: librmm-cu12 25.6.0\n",
            "    Uninstalling librmm-cu12-25.6.0:\n",
            "      Successfully uninstalled librmm-cu12-25.6.0\n",
            "  Attempting uninstall: libkvikio-cu12\n",
            "    Found existing installation: libkvikio-cu12 25.6.0\n",
            "    Uninstalling libkvikio-cu12-25.6.0:\n",
            "      Successfully uninstalled libkvikio-cu12-25.6.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.3.2\n",
            "    Uninstalling numpy-2.3.2:\n",
            "      Successfully uninstalled numpy-2.3.2\n",
            "  Attempting uninstall: libucxx-cu12\n",
            "    Found existing installation: libucxx-cu12 0.44.0\n",
            "    Uninstalling libucxx-cu12-0.44.0:\n",
            "      Successfully uninstalled libucxx-cu12-0.44.0\n",
            "  Attempting uninstall: ucx-py-cu12\n",
            "    Found existing installation: ucx-py-cu12 0.44.0\n",
            "    Uninstalling ucx-py-cu12-0.44.0:\n",
            "      Successfully uninstalled ucx-py-cu12-0.44.0\n",
            "  Attempting uninstall: rmm-cu12\n",
            "    Found existing installation: rmm-cu12 25.6.0\n",
            "    Uninstalling rmm-cu12-25.6.0:\n",
            "      Successfully uninstalled rmm-cu12-25.6.0\n",
            "  Attempting uninstall: libcudf-cu12\n",
            "    Found existing installation: libcudf-cu12 25.6.0\n",
            "    Uninstalling libcudf-cu12-25.6.0:\n",
            "      Successfully uninstalled libcudf-cu12-25.6.0\n",
            "  Attempting uninstall: dask\n",
            "    Found existing installation: dask 2025.5.0\n",
            "    Uninstalling dask-2025.5.0:\n",
            "      Successfully uninstalled dask-2025.5.0\n",
            "  Attempting uninstall: ucxx-cu12\n",
            "    Found existing installation: ucxx-cu12 0.44.0\n",
            "    Uninstalling ucxx-cu12-0.44.0:\n",
            "      Successfully uninstalled ucxx-cu12-0.44.0\n",
            "  Attempting uninstall: pylibcudf-cu12\n",
            "    Found existing installation: pylibcudf-cu12 25.6.0\n",
            "    Uninstalling pylibcudf-cu12-25.6.0:\n",
            "      Successfully uninstalled pylibcudf-cu12-25.6.0\n",
            "  Attempting uninstall: numba-cuda\n",
            "    Found existing installation: numba-cuda 0.11.0\n",
            "    Uninstalling numba-cuda-0.11.0:\n",
            "      Successfully uninstalled numba-cuda-0.11.0\n",
            "  Attempting uninstall: libraft-cu12\n",
            "    Found existing installation: libraft-cu12 25.6.0\n",
            "    Uninstalling libraft-cu12-25.6.0:\n",
            "      Successfully uninstalled libraft-cu12-25.6.0\n",
            "  Attempting uninstall: distributed\n",
            "    Found existing installation: distributed 2025.5.0\n",
            "    Uninstalling distributed-2025.5.0:\n",
            "      Successfully uninstalled distributed-2025.5.0\n",
            "  Attempting uninstall: rapids-dask-dependency\n",
            "    Found existing installation: rapids-dask-dependency 25.6.0\n",
            "    Uninstalling rapids-dask-dependency-25.6.0:\n",
            "      Successfully uninstalled rapids-dask-dependency-25.6.0\n",
            "  Attempting uninstall: pylibraft-cu12\n",
            "    Found existing installation: pylibraft-cu12 25.6.0\n",
            "    Uninstalling pylibraft-cu12-25.6.0:\n",
            "      Successfully uninstalled pylibraft-cu12-25.6.0\n",
            "  Attempting uninstall: libcuvs-cu12\n",
            "    Found existing installation: libcuvs-cu12 25.6.1\n",
            "    Uninstalling libcuvs-cu12-25.6.1:\n",
            "      Successfully uninstalled libcuvs-cu12-25.6.1\n",
            "  Attempting uninstall: cudf-cu12\n",
            "    Found existing installation: cudf-cu12 25.6.0\n",
            "    Uninstalling cudf-cu12-25.6.0:\n",
            "      Successfully uninstalled cudf-cu12-25.6.0\n",
            "  Attempting uninstall: libcuml-cu12\n",
            "    Found existing installation: libcuml-cu12 25.6.0\n",
            "    Uninstalling libcuml-cu12-25.6.0:\n",
            "      Successfully uninstalled libcuml-cu12-25.6.0\n",
            "  Attempting uninstall: distributed-ucxx-cu12\n",
            "    Found existing installation: distributed-ucxx-cu12 0.44.0\n",
            "    Uninstalling distributed-ucxx-cu12-0.44.0:\n",
            "      Successfully uninstalled distributed-ucxx-cu12-0.44.0\n",
            "  Attempting uninstall: dask-cudf-cu12\n",
            "    Found existing installation: dask-cudf-cu12 25.6.0\n",
            "    Uninstalling dask-cudf-cu12-25.6.0:\n",
            "      Successfully uninstalled dask-cudf-cu12-25.6.0\n",
            "  Attempting uninstall: dask-cuda\n",
            "    Found existing installation: dask-cuda 25.6.0\n",
            "    Uninstalling dask-cuda-25.6.0:\n",
            "      Successfully uninstalled dask-cuda-25.6.0\n",
            "  Attempting uninstall: cuvs-cu12\n",
            "    Found existing installation: cuvs-cu12 25.6.1\n",
            "    Uninstalling cuvs-cu12-25.6.1:\n",
            "      Successfully uninstalled cuvs-cu12-25.6.1\n",
            "  Attempting uninstall: raft-dask-cu12\n",
            "    Found existing installation: raft-dask-cu12 25.6.0\n",
            "    Uninstalling raft-dask-cu12-25.6.0:\n",
            "      Successfully uninstalled raft-dask-cu12-25.6.0\n",
            "  Attempting uninstall: cuml-cu12\n",
            "    Found existing installation: cuml-cu12 25.6.0\n",
            "    Uninstalling cuml-cu12-25.6.0:\n",
            "      Successfully uninstalled cuml-cu12-25.6.0\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
            "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
            "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\n",
            "cudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\n",
            "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.5.2 which is incompatible.\n",
            "Successfully installed cudf-cu12-25.2.2 cuml-cu12-25.2.1 cuvs-cu12-25.2.1 dask-2024.12.1 dask-cuda-25.2.0 dask-cudf-cu12-25.2.2 dask-expr-1.1.21 distributed-2024.12.1 distributed-ucxx-cu12-0.42.0 libcudf-cu12-25.2.2 libcuml-cu12-25.2.1 libcuvs-cu12-25.2.1 libkvikio-cu12-25.2.1 libraft-cu12-25.2.0 librmm-cu12-25.2.0 libucxx-cu12-0.42.0 numba-cuda-0.2.0 numpy-2.0.2 nvidia-nvcomp-cu12-4.2.0.11 pylibcudf-cu12-25.2.2 pylibraft-cu12-25.2.0 raft-dask-cu12-25.2.0 rapids-dask-dependency-25.2.0 rmm-cu12-25.2.0 ucx-py-cu12-0.42.0 ucxx-cu12-0.42.0\n",
            "Finished: install (RAPIDS 25.2)\n",
            "NumPy: 2.0.2\n",
            "Finished: NumPy version check\n",
            "cuML import OK\n",
            "Finished: cuML import check\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "# === Minimal RAPIDS 25.2 setup (Py3.12-safe) with reliable streaming =========\n",
        "verbose = True   # True => live logs; False => compact \"Finished: ...\" lines\n",
        "\n",
        "import os, sys, shlex, subprocess\n",
        "\n",
        "# Encourage immediate flushing from Python-based tools (e.g., pip)\n",
        "os.environ[\"PYTHONUNBUFFERED\"] = \"1\"\n",
        "\n",
        "def _run(cmd, label=None, use_shell=False):\n",
        "    \"\"\"\n",
        "    When verbose=True: stream stdout/stderr line-by-line (no buffering surprises).\n",
        "    When verbose=False: capture output and print a compact status line.\n",
        "    Raises on non-zero exit; on failure with verbose=False, prints captured logs.\n",
        "    \"\"\"\n",
        "    if isinstance(cmd, str) and not use_shell:\n",
        "        cmd = shlex.split(cmd)\n",
        "    if label is None:\n",
        "        label = (cmd[1] if isinstance(cmd, list) and len(cmd) >= 2 else \"command\")\n",
        "\n",
        "    if verbose:\n",
        "        # Stream live\n",
        "        proc = subprocess.Popen(\n",
        "            cmd, shell=use_shell,\n",
        "            stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
        "            text=True, bufsize=1, env=os.environ.copy()\n",
        "        )\n",
        "        for line in proc.stdout:\n",
        "            print(line, end=\"\")\n",
        "        rc = proc.wait()\n",
        "        if rc != 0:\n",
        "            raise subprocess.CalledProcessError(rc, cmd)\n",
        "        print(f\"Finished: {label}\")\n",
        "    else:\n",
        "        try:\n",
        "            res = subprocess.run(\n",
        "                cmd, shell=use_shell, check=True,\n",
        "                capture_output=True, text=True\n",
        "            )\n",
        "            print(f\"Finished: {label}\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            if e.stdout: print(e.stdout)\n",
        "            if e.stderr: print(e.stderr)\n",
        "            raise\n",
        "\n",
        "def pip(*args):\n",
        "    # Use the current interpreter; quiet pip when not verbose; unbuffer Python (-u)\n",
        "    args = list(args)\n",
        "    if not verbose and \"-q\" not in args and \"--quiet\" not in args:\n",
        "        args.insert(0, \"-q\")\n",
        "    return [sys.executable, \"-u\", \"-m\", \"pip\", *args]\n",
        "\n",
        "# --- 1) Uninstall packages that commonly conflict with RAPIDS wheels ----------\n",
        "conflicts = [\n",
        "    \"jax\", \"jaxlib\", \"tensorflow\", \"treescope\", \"pymc\", \"thinc\", \"flax\", \"optax\", \"chex\",\n",
        "    \"orbax-checkpoint\", \"dopamine-rl\", \"tensorflow-decision-forests\", \"tables\",\n",
        "    \"spacy\", \"mlxtend\", \"fastai\", \"blosc2\"\n",
        "]\n",
        "_run(pip(\"uninstall\", \"-y\", *conflicts), label=\"uninstall\")\n",
        "\n",
        "# --- 2) Pin CPU-side stack (choose pins based on Python version) -------------\n",
        "PY312_PLUS = sys.version_info >= (3, 12)\n",
        "\n",
        "# Colab switched to Python 3.12 in made to late August. Check https://github.com/googlecolab/colabtools/issues/5483.\n",
        "# So the else block is not required if we don't run the notebook elsewhere.\n",
        "if PY312_PLUS:\n",
        "    # Py3.12-friendly pins\n",
        "    NUMPY_SPEC   = \"numpy<3.0a0\"           # allows NumPy 2.x\n",
        "    SKLEARN_SPEC = \"scikit-learn>=1.4,<1.6\"\n",
        "    IMB_SPEC     = \"imbalanced-learn>=0.12,<0.13\"\n",
        "else:\n",
        "    NUMPY_SPEC   = \"numpy==1.24.4\"\n",
        "    SKLEARN_SPEC = \"scikit-learn==1.2.2\"\n",
        "    IMB_SPEC     = \"imbalanced-learn==0.11.0\"\n",
        "\n",
        "_run(pip(\"install\", \"--force-reinstall\", NUMPY_SPEC, SKLEARN_SPEC, IMB_SPEC),\n",
        "     label=\"install (NumPy/sklearn/imbalanced-learn)\")\n",
        "\n",
        "# --- 3) Install RAPIDS 25.2 (CUDA 12) from NVIDIA's index --------------------\n",
        "rapids_pkgs = [\n",
        "    \"cudf-cu12==25.2.*\", \"cuml-cu12==25.2.*\", \"dask-cudf-cu12==25.2.*\", \"dask-cuda==25.2.*\",\n",
        "    \"rapids-dask-dependency==25.2.*\", \"raft-dask-cu12==25.2.*\",\n",
        "    \"rmm-cu12==25.2.*\", \"librmm-cu12==25.2.*\", \"pylibcudf-cu12==25.2.*\",\n",
        "    \"libraft-cu12==25.2.*\", \"pylibraft-cu12==25.2.*\", \"libcuvs-cu12==25.2.*\",\n",
        "    \"cuvs-cu12==25.2.*\", \"ucx-py-cu12==0.42.*\", \"ucxx-cu12==0.42.*\", \"distributed-ucxx-cu12==0.42.*\"\n",
        "]\n",
        "_run(pip(\"install\", \"--extra-index-url\", \"https://pypi.nvidia.com\", *rapids_pkgs),\n",
        "     label=\"install (RAPIDS 25.2)\")\n",
        "\n",
        "# --- 4) Quick checks ----------------------------------------------------------\n",
        "_run([sys.executable, \"-c\", \"import numpy as np; print('NumPy:', np.__version__)\"],\n",
        "     label=\"NumPy version check\")\n",
        "_run([sys.executable, \"-c\", \"import cuml; print('cuML import OK')\"],\n",
        "     label=\"cuML import check\")\n",
        "\n",
        "print(\"Done.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(\"Root directories:\", os.listdir())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VV3iQKIW-UYy",
        "outputId": "2ffec597-1be3-40b3-e8e3-e9caa8a4abb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root directories: ['.config', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning up old report folder before each run\n",
        "import shutil, os\n",
        "\n",
        "to_clear = [\"report\"]\n",
        "\n",
        "for d in to_clear:\n",
        "    if os.path.exists(d):\n",
        "        shutil.rmtree(d)\n",
        "        print(f\"Removed old directory: {d}\")\n",
        "\n",
        "# Recreate the clean report folder\n",
        "os.makedirs(\"report\", exist_ok=True)\n",
        "print(\"Recreated clean 'report/' folder\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hcBr5pD98-U",
        "outputId": "9ec1201c-b3ce-4b84-dc23-8cfa4816df1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recreated clean 'report/' folder\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzezuMJhraVG",
        "outputId": "6d09e97a-6e54-4e58-992d-d8985834b33f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " All imports successful. GPU ready for cuML and cuDF!\n"
          ]
        }
      ],
      "source": [
        "save_training = False\n",
        "STOP_AT_PARAMS = True\n",
        "\n",
        "# Required libraries\n",
        "import os # Tarun 07/27/25\n",
        "import cudf\n",
        "import cuml\n",
        "import cupy as cp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import os\n",
        "import regex as re\n",
        "import logging\n",
        "import pickle\n",
        "import csv\n",
        "import requests\n",
        "import yaml\n",
        "import ipywidgets as widgets\n",
        "import pprint\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import base64\n",
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "import time # Tarun 6/2/25\n",
        "\n",
        "from google.colab import _message\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "from io import StringIO\n",
        "from collections import OrderedDict\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "\n",
        "os.makedirs(\"report\", exist_ok=True) # Tarun 07/27/25\n",
        "\n",
        "print(\" All imports successful. GPU ready for cuML and cuDF!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdJKwgi77Lsi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7466b94b-db16-42f1-fbf5-ca9c22fc1edc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Runtime environment is ready.\n"
          ]
        }
      ],
      "source": [
        "# GPU-Optimized Model Imports\n",
        "from cuml.ensemble import RandomForestClassifier\n",
        "from cuml.linear_model import LogisticRegression\n",
        "from cuml.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier   # MLP remains CPU-based\n",
        "from xgboost import XGBClassifier                   # Will set GPU parameters during model creation\n",
        "from imblearn.over_sampling import SMOTE            # SMOTE stays on CPU\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_curve, roc_auc_score\n",
        "from xgboost import plot_importance\n",
        "\n",
        "print(\" Runtime environment is ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "REPORT_FOLDER = \"report\"  # Default path to the report folder in colab left-nav.\n",
        "\n",
        "def setup_report_folder(report_folder=REPORT_FOLDER):\n",
        "    \"\"\"\n",
        "    Create the report folder if it doesn't exist and download the report.html template and save as index.html.\n",
        "    Returns the number of files in the folder.\n",
        "    \"\"\"\n",
        "    # Create the report folder if it doesn't exist\n",
        "    if not os.path.exists(report_folder):\n",
        "        os.makedirs(report_folder)\n",
        "        print(f\"Created new directory: {report_folder}\")\n",
        "\n",
        "    # Check if index.html exists, if not download it\n",
        "    index_file_path = os.path.join(report_folder, \"index.html\")\n",
        "    if not os.path.exists(index_file_path):\n",
        "        template_url = \"https://raw.githubusercontent.com/ModelEarth/localsite/refs/heads/main/start/template/report.html\"\n",
        "        try:\n",
        "            response = requests.get(template_url)\n",
        "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "\n",
        "            with open(index_file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(response.text)\n",
        "            print(f\"Downloaded index.html template to {index_file_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading template: {e}\")\n",
        "\n",
        "    add_readme_to_report_folder(report_folder)\n",
        "\n",
        "def add_readme_to_report_folder(report_folder=REPORT_FOLDER):\n",
        "    \"\"\"\n",
        "    Create a README.md file in the report folder if it doesn't exist yet.\n",
        "    \"\"\"\n",
        "    readme_path = os.path.join(report_folder, \"README.md\")\n",
        "\n",
        "    if not os.path.exists(readme_path):\n",
        "        readme_content = \"# Run Models Report\\n\\nThis folder contains generated reports from model executions.\"\n",
        "\n",
        "        with open(readme_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(readme_content)\n",
        "        print(f\"Created README.md in {report_folder}\")\n",
        "\n",
        "    return readme_path\n",
        "\n",
        "setup_report_folder(REPORT_FOLDER)"
      ],
      "metadata": {
        "id": "i9jwqUTmU_10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e086b542-b3b4-4b0d-d4a2-fcfe2b786959"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded index.html template to report/index.html\n",
            "Created README.md in report\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fE9Xji693Fs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "markdown_lines = []"
      ],
      "metadata": {
        "id": "wydSc8N4Popq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EstYTopG4IH"
      },
      "source": [
        "\n",
        "# Parameter Loader and Editor UI\n",
        "\n",
        "This section builds an interactive user interface (UI) for loading, editing, and comparing YAML-based parameter files.\n",
        "\n",
        "**Main functionalities:**\n",
        "- Load available parameter sets from a remote CSV file (name → link).\n",
        "- Display the URL and YAML contents of the selected parameter set.\n",
        "- Allow users to edit YAML content directly in a text box.\n",
        "- Detect and display:\n",
        "  - Changes in the selected parameter source URL.\n",
        "  - Differences between the previous and current remote YAML defaults.\n",
        "  - Changes made to the YAML content in the text box.\n",
        "- Safely update and store the current parameter state for further usage.\n",
        "- Handle special cases like converting a single model string into a list.\n",
        "- Expose key values like `param` (object-based access) and `save_training` (boolean flag) for downstream workflows."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Google Data Commons Test**\n",
        "\n",
        "If you want to test the google data commons data pull. Follow these steps.\n",
        "\n",
        "1. Copy the following YAML config:\n",
        "\n",
        "```\n",
        "folder: gdc-states-test\n",
        "features:\n",
        "  dcid:\n",
        "    - geoId/13  # Georgia\n",
        "    - geoId/06  # California  \n",
        "    - geoId/36  # New York\n",
        "    - geoId/48  # Texas\n",
        "    - geoId/12  # Florida\n",
        "  variables:\n",
        "    - Count_Person\n",
        "    - Median_Income_Person\n",
        "    - UnemploymentRate_Person\n",
        "  common: Fips\n",
        "  year: 2020\n",
        "targets:\n",
        "  dcid:\n",
        "    - geoId/13\n",
        "    - geoId/06\n",
        "    - geoId/36\n",
        "    - geoId/48\n",
        "    - geoId/12\n",
        "  variables:\n",
        "    - Count_Person\n",
        "  common: Fips\n",
        "  year: 2020\n",
        "models:\n",
        "  - RFC\n",
        "  - XGBoost\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "2. Paste it into your parameter widget's text area\n",
        "3. Click Update\n",
        "4. Run the cells under \"Data Pull from Google Data Commons from yaml files - Prathyusha\""
      ],
      "metadata": {
        "id": "qMExYC4_4JNZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhdoU3hXI0Wi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729,
          "referenced_widgets": [
            "d37bdb0080bb454d9857e0234df43eec",
            "1bcacc57d8cd4dda865b2edbc841f0ec",
            "8da877c0a66644ed9d5e26fee6105429",
            "d080b766671f409a928065c2b08b6634",
            "34f08580178a48fd88b3654b0ccf7b38",
            "f3d9b0bf355c4359ba438f068852b329",
            "e592bfb2e4bf4679b483514c77326869",
            "bd9eebf384ef458aad0832cedabc6854",
            "27425d2c43da456c9580fef459116649",
            "f8d020ea01c749219e9b33d43ab40c97",
            "6574e8ecb3844955b48174cdfca3bdec",
            "b09161c3bd204db49eba959714069613",
            "7264cdcdbc4e4319b194ad47061fb8b3",
            "52dfaaf358064d06a3fdcef285723556",
            "6ff9d2593af246028c03c13879db8638",
            "2df6a5912e2c4cba91633fd6a6efe312",
            "7af3635b00394a7cb5ca480957ffbbce",
            "055818bbc99440f188f7271326430b5e",
            "f44b82ce215f4e58bef977ced359fc9e",
            "d1a020c51bf446b0b191b02575ab4a17",
            "0fe12007c2df490a909c98c6e85e700e",
            "8a994d6c051f4d19bf630a78c97f5d25",
            "520a294e60e140ae84dc0f639b48b994",
            "e8c10d0d453449ad9ca70e4b914c37ec",
            "adf6a5e72df24316bf3d02827ac709e0",
            "1e0e22369e6b4cf8a18438371c8aa8b2",
            "3391185616294d11b7ee53780d743864",
            "184e70e2b24b4732ab3833727bbafa34",
            "91be8bd4ad3e49398beadfbe9e05b2c8",
            "690b96bc8929435ab50439caaa14eef5",
            "e7a1889385b74f9ba8e0afcdc77e44a1",
            "94705b118ca845398982f9f74bc3ffcf",
            "af4144fafcdf48aabad19597fd5b6dba",
            "4971da0d9b834ee38f899adf06c0cbea",
            "cfab788e51134f18907c4e704afda5df",
            "6f114277db254341832b3242bb40f384",
            "0860b3bc18034af89ed280fca64571c6",
            "4c921978f95d43e7bd9b5515963ef925",
            "9d492ee052594641a47162aac5892682",
            "9759c760ec2b42599cafac7488086433",
            "dc0993f481b54a818195b8a15300f961"
          ]
        },
        "outputId": "4c58f8c8-84f3-4943-a4a2-1a26f1c703cd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Dropdown(description='Params Path', options=('\\ufeffparameters.yaml', 'parameters-simple.yaml',…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d37bdb0080bb454d9857e0234df43eec"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from pprint import pformat\n",
        "\n",
        "# @title 🔧 Parameter Widget Setup { display-mode: \"code\" }\n",
        "models = ['LR','RFC', 'RBF', 'SVM', 'MLP', 'XGBoost']\n",
        "\n",
        "with open(os.path.join(REPORT_FOLDER, \"model-options.csv\"), 'w', newline='') as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  writer.writerow(['model_name'])\n",
        "  for model in models:\n",
        "    writer.writerow([model])\n",
        "\n",
        "# ----------- Functions -------------\n",
        "def load_parameter_paths_csv(url):\n",
        "    \"\"\"\n",
        "    Download a CSV file from the given URL, read its contents, and return\n",
        "    a dictionary where each entry maps the first column (name)\n",
        "    to the second column (link).\n",
        "    \"\"\"\n",
        "    resp = requests.get(url)\n",
        "    resp.raise_for_status()\n",
        "    reader = csv.reader(StringIO(resp.text))\n",
        "    return {name: link for name, link in reader if len((name, link)) == 2}\n",
        "\n",
        "def compute_diffs(dict_a, dict_b):\n",
        "    \"\"\"\n",
        "    Compare two dictionaries and return a list of differences.\n",
        "    Each difference is a tuple: (key, old_value, new_value).\n",
        "    \"\"\"\n",
        "    diffs = []\n",
        "    for key in sorted(set(dict_a) | set(dict_b)):\n",
        "        old = dict_a.get(key)\n",
        "        new = dict_b.get(key)\n",
        "        if old != new:\n",
        "            diffs.append((key, old, new))\n",
        "    return diffs\n",
        "\n",
        "def pretty_print_diff(title, diffs):\n",
        "    \"\"\"\n",
        "    Nicely format and print differences with separate old/new fields.\n",
        "    \"\"\"\n",
        "    if not diffs:\n",
        "        return\n",
        "    print(f\"\\n=== {title} ===\")\n",
        "    for key, old, new in diffs:\n",
        "        print(f\"• {key}:\")\n",
        "        print(f\"    Old: {pprint.pformat(old, indent=8)}\")\n",
        "        print(f\"    New: {pprint.pformat(new, indent=8)}\\n\")\n",
        "\n",
        "class DictToObject:\n",
        "    \"\"\"\n",
        "    Helper class that recursively converts a dictionary into an object\n",
        "    with attributes, allowing access with dot notation.\n",
        "    \"\"\"\n",
        "    def __init__(self, d):\n",
        "        for k, v in d.items():\n",
        "            setattr(self, k, DictToObject(v) if isinstance(v, dict) else v)\n",
        "\n",
        "    def to_dict(self):\n",
        "        return {k: v.to_dict() if isinstance(v, DictToObject) else v for k, v in vars(self).items()}\n",
        "\n",
        "    def __repr__(self):\n",
        "        body = pformat(self.to_dict(), indent=2, width=80, compact=False, sort_dicts=True)\n",
        "        return f\"DictToObject(\\n{body}\\n)\"\n",
        "\n",
        "\n",
        "# Melody 06/26/2025\n",
        "def save_parameters_to_report():\n",
        "  \"\"\"\n",
        "  Save current parameters to report/parameters.yaml\n",
        "  Reuses existing report folder setup logic\n",
        "  \"\"\"\n",
        "  setup_report_folder(REPORT_FOLDER)\n",
        "  current_params = last_edited_dict.copy()\n",
        "  selected_models = [cb.description for cb in model_checkboxes if cb.value]\n",
        "\n",
        "  if selected_models:\n",
        "    current_params['models'] = selected_models\n",
        "\n",
        "  yaml_file = os.path.join(REPORT_FOLDER, 'parameters.yaml')\n",
        "\n",
        "  with open(yaml_file, 'w', encoding='utf-8') as f:\n",
        "    yaml.safe_dump(current_params, f, sort_keys=False)\n",
        "  print(f'Parameters saved to {yaml_file}')\n",
        "\n",
        "# --- Load Parameter Paths & Default Values ---\n",
        "parameter_csv_url = (\n",
        "    'https://raw.githubusercontent.com/ModelEarth/RealityStream/main/parameters/parameter-paths.csv'\n",
        ")\n",
        "parameter_paths = load_parameter_paths_csv(parameter_csv_url)\n",
        "\n",
        "# Pick the first entry as the default\n",
        "default_name = next(iter(parameter_paths))\n",
        "default_url  = parameter_paths[default_name]\n",
        "\n",
        "# Load Model Names from CSV\n",
        "model_names = []\n",
        "with open(os.path.join(REPORT_FOLDER, \"model-options.csv\"), 'r') as f:\n",
        "    reader = csv.DictReader(f)\n",
        "    for row in reader:\n",
        "        model_names.append(row['model_name'])\n",
        "\n",
        "# --- Load and process the default YAML content ---\n",
        "default_yaml_text = requests.get(default_url).text\n",
        "default_yaml_dict = yaml.safe_load(default_yaml_text) or {}\n",
        "\n",
        "# Extract and process default models\n",
        "default_models = default_yaml_dict.get('models', [])\n",
        "if isinstance(default_models, str):\n",
        "    default_models = [default_models]\n",
        "default_models_lower = [model.lower() for model in default_models]\n",
        "\n",
        "# Remove 'models' key from the YAML dictionary\n",
        "default_yaml_dict.pop('models', None)\n",
        "\n",
        "# Convert the modified dictionary back to a YAML string\n",
        "processed_yaml_text = yaml.safe_dump(default_yaml_dict, sort_keys=False)\n",
        "\n",
        "# --- Widget Definitions ---\n",
        "\n",
        "# Dropdown to select which parameter set to load\n",
        "chooseParams_widget = widgets.Dropdown(\n",
        "    options=list(parameter_paths.keys()),\n",
        "    value=default_name,\n",
        "    description='Params Path',\n",
        "    layout=widgets.Layout(_view_count=1, _view_module='jupyter-widgets'), # Add metadata for github rendering\n",
        ")\n",
        "\n",
        "# Text field showing the URL of the selected YAML file\n",
        "parametersSource_widget = widgets.Text(\n",
        "    value=default_url,\n",
        "    description='Params From',\n",
        "    layout=widgets.Layout(width='1200px', _view_count=1, _view_module='jupyter-widgets'), # Add metadata for github rendering\n",
        ")\n",
        "\n",
        "load_url_button = widgets.Button(\n",
        "    description='↓',\n",
        "    tooltip='Load parameters from URL into editor',\n",
        "    button_style='',\n",
        "    layout=widgets.Layout(\n",
        "        width='28px',\n",
        "        height='28px',\n",
        "        padding='0',\n",
        "        margin='0 0 0 8px',\n",
        "        min_width='28px',\n",
        "        _view_count=1, _view_module='jupyter-widgets' # Add metadata for github rendering\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Text area allowing inline editing of the YAML content\n",
        "params_widget = widgets.Textarea(\n",
        "    value=processed_yaml_text,\n",
        "    description='Params',\n",
        "    layout=widgets.Layout(width='1200px', height='200px', _view_count=1, _view_module='jupyter-widgets'), # Add metadata for github rendering\n",
        ")\n",
        "\n",
        "# Button to trigger loading and diffing\n",
        "apply_button = widgets.Button(\n",
        "    description='Update',\n",
        "    button_style='primary',\n",
        "    layout=widgets.Layout(_view_count=1, _view_module='jupyter-widgets') # Add metadata for github rendering\n",
        ")\n",
        "\n",
        "# Output area to display diffs and status\n",
        "output = widgets.Output()\n",
        "\n",
        "# --- Global State: Last URL and Parameter Content ---\n",
        "\n",
        "# Track the last-used URL and parsed dictionaries,\n",
        "# so we can diff against them on each Update click\n",
        "last_url         = default_url\n",
        "last_remote_dict = yaml.safe_load(requests.get(default_url).text) or {}\n",
        "last_params_text = processed_yaml_text\n",
        "last_edited_dict = default_yaml_dict\n",
        "default_models = last_remote_dict.get('models', [])\n",
        "if isinstance(default_models, str):\n",
        "    default_models = [default_models]\n",
        "default_models_lower = [model.lower() for model in default_models]\n",
        "\n",
        "# Flag to track if the user has edited the params_widget\n",
        "user_edited = False\n",
        "\n",
        "# --- Create Model Checkboxes ---\n",
        "\n",
        "model_checkboxes = []\n",
        "for name in model_names:\n",
        "    checked = name.lower() in default_models_lower\n",
        "    cb = widgets.Checkbox(value=checked, description=name, layout=widgets.Layout(_view_count=1, _view_module='jupyter-widgets')) # Add metadata for github rendering\n",
        "    model_checkboxes.append(cb)\n",
        "\n",
        "model_selection_box = widgets.VBox(model_checkboxes, layout=widgets.Layout(_view_count=1, _view_module='jupyter-widgets')) # Add metadata for github rendering\n",
        "\n",
        "# --- Event Callbacks ---\n",
        "\n",
        "def on_path_change(change):\n",
        "    \"\"\"\n",
        "    When the dropdown selection changes, update the URL field\n",
        "    and load the new YAML into the editable text area.\n",
        "    \"\"\"\n",
        "    if change['name'] == 'value' and change['type'] == 'change':\n",
        "        name = change['new']\n",
        "        url  = parameter_paths[name]\n",
        "        parametersSource_widget.value = url\n",
        "        yaml_text = requests.get(url).text\n",
        "        yaml_dict = yaml.safe_load(yaml_text) or {}\n",
        "\n",
        "        # Update default models\n",
        "        global default_models\n",
        "        default_models = yaml_dict.get('models', [])\n",
        "        if isinstance(default_models, str):\n",
        "            default_models = [default_models]\n",
        "        default_models_lower = [model.lower() for model in default_models]\n",
        "\n",
        "        # Update checkboxes\n",
        "        for cb in model_checkboxes:\n",
        "            cb.value = cb.description.lower() in default_models_lower\n",
        "\n",
        "        # Remove 'models' key from the YAML dictionary\n",
        "        yaml_dict.pop('models', None)\n",
        "\n",
        "        # Update the text area with the modified YAML\n",
        "        params_widget.value = yaml.safe_dump(yaml_dict, sort_keys=False)\n",
        "\n",
        "        # Reset the user_edited flag\n",
        "        global user_edited\n",
        "        user_edited = False\n",
        "\n",
        "        save_parameters_to_report()\n",
        "\n",
        "chooseParams_widget.observe(on_path_change)\n",
        "\n",
        "def on_load_url_clicked(_):\n",
        "    global last_params_text, last_edited_dict, default_models, last_url, last_remote_dict, user_edited\n",
        "\n",
        "    url = parametersSource_widget.value\n",
        "    try:\n",
        "        remote_full = yaml.safe_load(requests.get(url).text) or {}\n",
        "    except Exception as e:\n",
        "        with output:\n",
        "            print(f\"Error fetching parameters from URL: {e}\")\n",
        "        return\n",
        "\n",
        "    remote_models = remote_full.get('models', [])\n",
        "    if isinstance(remote_models, str):\n",
        "        remote_models = [remote_models]\n",
        "    remote_for_editor = dict(remote_full)\n",
        "    remote_for_editor.pop('models', None)\n",
        "\n",
        "    params_widget.value = yaml.safe_dump(remote_for_editor, sort_keys=False)\n",
        "    last_params_text    = params_widget.value\n",
        "    last_edited_dict    = remote_for_editor\n",
        "    default_models      = remote_models\n",
        "    user_edited         = False\n",
        "\n",
        "    # sync checkboxes\n",
        "    lower = [m.lower() for m in remote_models]\n",
        "    for cb in model_checkboxes:\n",
        "        cb.value = cb.description.lower() in lower\n",
        "\n",
        "    last_url         = url\n",
        "    last_remote_dict = remote_full\n",
        "\n",
        "    with output:\n",
        "        clear_output()\n",
        "        print(\"Loaded parameters from URL and updated model checkboxes.\")\n",
        "\n",
        "load_url_button.on_click(on_load_url_clicked)\n",
        "\n",
        "def on_params_change(change):\n",
        "    \"\"\"\n",
        "    Set the user_edited flag to True when the user edits the params_widget.\n",
        "    \"\"\"\n",
        "    global user_edited\n",
        "    user_edited = True\n",
        "\n",
        "params_widget.observe(on_params_change, names='value')\n",
        "\n",
        "def on_update_clicked(_):\n",
        "    \"\"\"\n",
        "    Each time the Update button is clicked:\n",
        "    1. Compare the edited YAML text to the last edit and print any key/value changes.\n",
        "    2. Compare the current URL to the last URL and print any change.\n",
        "    3. Diff the remote defaults for both old & new URLs.\n",
        "    4. Update the 'last_' state variables for the next click.\n",
        "    \"\"\"\n",
        "    global last_url, last_remote_dict, last_params_text, last_edited_dict, param, save_training, default_models, user_edited\n",
        "\n",
        "    with output:\n",
        "        clear_output()\n",
        "\n",
        "        current_url  = parametersSource_widget.value\n",
        "        current_text = params_widget.value\n",
        "        print(\"\\n\")\n",
        "\n",
        "        # Parse current text up-front\n",
        "        try:\n",
        "            current_edit = yaml.safe_load(current_text) or {}\n",
        "        except yaml.YAMLError as e:\n",
        "            print(f\"Error parsing edited YAML: {e}\")\n",
        "            return\n",
        "\n",
        "        # 🔧 Treat dropdown-driven changes as updates too\n",
        "        if user_edited or current_text != last_params_text or current_url != last_url:\n",
        "            content_diffs = compute_diffs(last_edited_dict, current_edit)\n",
        "            if content_diffs:\n",
        "                pretty_print_diff(\"YAML edits since last update\", content_diffs)\n",
        "            else:\n",
        "                print(\"No key/value differences.\\n\")\n",
        "            last_params_text = current_text\n",
        "            last_edited_dict = current_edit\n",
        "            user_edited = False\n",
        "        else:\n",
        "            print(\"YAML content unchanged since last update.\\n\")\n",
        "\n",
        "        # 2) URL change detection\n",
        "        if current_url != last_url:\n",
        "            print(f\"\\n=== URL changed ===\\n\")\n",
        "            print(f\"  {last_url!r} → {current_url!r}\\n\")\n",
        "            try:\n",
        "                new_remote = yaml.safe_load(requests.get(current_url).text) or {}\n",
        "            except Exception as e:\n",
        "                print(f\"Error fetching new remote parameters: {e}\")\n",
        "                return\n",
        "            path_diffs = compute_diffs(last_remote_dict, new_remote)\n",
        "            if path_diffs:\n",
        "                pretty_print_diff(\"Default parameters changed between URLs\", path_diffs)\n",
        "            else:\n",
        "                print(\"No default-parameter differences between those URLs.\\n\")\n",
        "            last_url = current_url\n",
        "            last_remote_dict = new_remote\n",
        "        else:\n",
        "            print(f\"URL unchanged: {current_url!r}\\n\")\n",
        "\n",
        "        # 3) Update models from checkboxes\n",
        "        selected_models = [cb.description for cb in model_checkboxes if cb.value]\n",
        "        if selected_models:\n",
        "            last_edited_dict['models'] = selected_models\n",
        "            print(f\"Selected models: {selected_models}\")\n",
        "        else:\n",
        "            print(\"No models selected.\")\n",
        "\n",
        "        # Compare selected models with default models (case-insensitive)\n",
        "        selected_models_lower = [model.lower() for model in selected_models]\n",
        "        default_models_lower = [model.lower() for model in default_models]\n",
        "\n",
        "        added_models = [model for model in selected_models if model.lower() not in default_models_lower]\n",
        "        removed_models = [model for model in default_models if model.lower() not in selected_models_lower]\n",
        "\n",
        "        if added_models or removed_models:\n",
        "            print(\"\\n=== Model Selection Changes ===\")\n",
        "            if added_models:\n",
        "                print(f\"Added models: {added_models}\")\n",
        "            if removed_models:\n",
        "                print(f\"Removed models: {removed_models}\")\n",
        "        else:\n",
        "            print(\"Model selection unchanged.\")\n",
        "\n",
        "        # Update default_models for next comparison\n",
        "        default_models = selected_models.copy()\n",
        "\n",
        "        # 4) Build updated param and save_training\n",
        "        param = DictToObject(OrderedDict(last_edited_dict))\n",
        "        save_training = getattr(param, 'save_training', False)\n",
        "\n",
        "        save_pickle = getattr(param, 'save_pickle', False)  # Tarun\n",
        "        print(f\"save_pickle set to: {save_pickle}\")  # Tarun\n",
        "\n",
        "        # Changes tarun\n",
        "        # Define mapping of model keys to full import\n",
        "\n",
        "        import importlib\n",
        "\n",
        "        model_import_paths = {\n",
        "            \"RFC\": \"sklearn.ensemble.RandomForestClassifier\",\n",
        "            \"RBF\": \"sklearn.ensemble.RandomForestClassifier\",  # alias\n",
        "            \"LR\": \"sklearn.linear_model.LogisticRegression\",\n",
        "            \"LogisticRegression\": \"sklearn.linear_model.LogisticRegression\",\n",
        "            \"SVM\": \"sklearn.svm.SVC\",\n",
        "            \"MLP\": \"sklearn.neural_network.MLPClassifier\",\n",
        "            \"XGBoost\": \"xgboost.XGBClassifier\"\n",
        "        }\n",
        "\n",
        "\n",
        "        # Create a dictionary to store dynamically imported model classes\n",
        "        loaded_model_classes = {}\n",
        "\n",
        "        # Use param_dict for safe access\n",
        "        requested_models = last_edited_dict.get(\"models\", [])\n",
        "\n",
        "        for model_name in requested_models:\n",
        "            if model_name not in model_import_paths:\n",
        "                print(f\" Unknown model: {model_name}\")\n",
        "                continue\n",
        "\n",
        "            full_path = model_import_paths[model_name]\n",
        "            module_name, class_name = full_path.rsplit('.', 1)\n",
        "\n",
        "            try:\n",
        "                module = importlib.import_module(module_name)\n",
        "                model_class = getattr(module, class_name)\n",
        "                loaded_model_classes[model_name] = model_class\n",
        "                print(f\" Loaded {model_name} from {module_name}\")\n",
        "            except (ImportError, AttributeError) as e:\n",
        "                print(f\" Failed to import {model_name}: {e}\")\n",
        "\n",
        "        # 5) Fix single model case: always make models a list\n",
        "        if isinstance(last_edited_dict.get(\"models\"), str):\n",
        "            last_edited_dict[\"models\"] = [last_edited_dict[\"models\"]]\n",
        "            param = DictToObject(OrderedDict(last_edited_dict))  # Rebuild after fix\n",
        "\n",
        "        save_parameters_to_report()\n",
        "\n",
        "apply_button.on_click(on_update_clicked)\n",
        "\n",
        "# --- Display the UI ---\n",
        "\n",
        "ui = widgets.VBox([\n",
        "    chooseParams_widget,\n",
        "    widgets.HBox([parametersSource_widget, load_url_button]),\n",
        "    params_widget,\n",
        "    model_selection_box,\n",
        "    apply_button,\n",
        "    output\n",
        "])\n",
        "display(ui)\n",
        "on_update_clicked(None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "`if STOP_AT_PARAMS:\n",
        "    raise SystemExit(\"Stopped at parameter edit step. Your variables will still be available. \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "9Ubr1zqd08WG",
        "outputId": "1958334a-ca8e-400d-e9b3-8d10aff190fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-3933854021.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-3933854021.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    `if STOP_AT_PARAMS:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alauCxr5yHF7"
      },
      "outputs": [],
      "source": [
        "# Assuming 'param' is an instance of DictToObject from previous code blocks\n",
        "# Define necessary adjustments to your setup\n",
        "\n",
        "# Settings\n",
        "model_name = \"RandomForest\"  # Specify the model to be trained\n",
        "all_model_list = [\"LogisticRegression\", \"SVM\", \"MLP\", \"RandomForest\", \"XGBoost\"]  # All usable models\n",
        "assert model_name in all_model_list, \"Model not supported\"\n",
        "valid_report_list = [\"RandomForest\", \"XGBoost\"]  # Valid models for feature-importance report\n",
        "\n",
        "random_state = 42  # Random state for reproducibility\n",
        "# print(param.features.path)\n",
        "#print(param.targets.__dict__)\n",
        "\n",
        "# Tarun changes\n",
        "# Dynamically import only the models specified in param.models\n",
        "available_model_classes = {}\n",
        "\n",
        "# Normalize all names to lowercase to match YAML inputs\n",
        "requested_models = [m.lower() for m in last_edited_dict.get('models', [])]\n",
        "\n",
        "if 'randomforest' in requested_models:\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    available_model_classes['RandomForest'] = RandomForestClassifier\n",
        "\n",
        "if 'svm' in requested_models:\n",
        "    from sklearn.svm import SVC\n",
        "    available_model_classes['SVM'] = SVC\n",
        "\n",
        "if 'logisticregression' in requested_models:\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    available_model_classes['LogisticRegression'] = LogisticRegression\n",
        "\n",
        "if 'mlp' in requested_models:\n",
        "    from sklearn.neural_network import MLPClassifier\n",
        "    available_model_classes['MLP'] = MLPClassifier\n",
        "\n",
        "if 'xgboost' in requested_models:\n",
        "    import xgboost as xgb\n",
        "    available_model_classes['XGBoost'] = xgb.XGBClassifier\n",
        "\n",
        "if hasattr(param.features, \"target_column\"):\n",
        "    target_column = param.features.target_column\n",
        "    target_url = None\n",
        "else:\n",
        "  print(param.targets.path)\n",
        "  # Access the 'path' key within the 'targets' object safely\n",
        "  target_url = param.targets.path\n",
        "  target_df = pd.read_csv(target_url) #why?\n",
        "  print(target_df.head())\n",
        "\n",
        "  # Tarun Changes\n",
        "  # Normalize and search for “fips” in any case or with stray whitespace\n",
        "  cols = [col.strip() for col in target_df.columns]\n",
        "  match = next((col for col in cols if col.lower() == \"fips\"), None)\n",
        "  if not match:\n",
        "      raise ValueError(\"No valid location column found (expected something like 'FIPS').\")\n",
        "  location_column = match\n",
        "  print(f\"Location column identified: {location_column!r}\")\n",
        "\n",
        "\n",
        "  # Dynamically identify the location column\n",
        "  # location_columns = [\"Country\", \"State\", \"Fips\", \"Zip\", \"Voxel\"]\n",
        "  # location_column = next((col for col in target_df.columns if col in location_columns), None)\n",
        "  # if not location_column:\n",
        "  #     raise ValueError(\"No valid location column found in the target dataset.\")\n",
        "  # print(f\"Location column identified: {location_column}\")\n",
        "\n",
        "  # Dynamically identify the target column\n",
        "  # TO DO: Convert all incoming to lowercase to column name \"target\" also works.\n",
        "  target_column = \"Target\" if \"Target\" in target_df.columns else None\n",
        "if not target_column:\n",
        "    #raise ValueError(\"The 'Target' column is not found in the target dataset.\")\n",
        "    print(\"The 'Target' column is not found in the target dataset.\")\n",
        "print(f\"Target column identified: {target_column}\")\n",
        "\n",
        "# Directory Information\n",
        "dataset_name = \"Name needs to be added\"\n",
        "merged_save_dir = f\"../process/{dataset_name}/states-{target_column}-{dataset_name}\"  # Directory for state-separate dataset\n",
        "full_save_dir = f\"../output/{dataset_name}/training\"  # Directory for the integrated dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIth7Fegr26z"
      },
      "outputs": [],
      "source": [
        "# STEP: Create Functions\n",
        "def rename_columns(df, year):\n",
        "    rename_mapping = {}\n",
        "    for column in df.columns:\n",
        "      if column not in df.columns[:2]:\n",
        "          new_column_name = column + f'-{year}'\n",
        "          rename_mapping[column] = new_column_name\n",
        "    df.rename(columns=rename_mapping, inplace=True)\n",
        "\n",
        "def check_directory(directory_path): # Check whether the given directory exists, if not, then create it\n",
        "    if not os.path.exists(directory_path):\n",
        "        try:\n",
        "            os.makedirs(directory_path)\n",
        "            print(f\"Directory '{directory_path}' created successfully by check_directory.\")\n",
        "        except OSError as e:\n",
        "            print(f\"Error creating directory '{directory_path}': {e}\")\n",
        "    else:\n",
        "        print(\"Current working directory:\", os.getcwd())\n",
        "        print(\"View under the folder icon which is followed by 2 dots..\")\n",
        "        print(f\"check_directory '{directory_path}' already exists.\")\n",
        "    return directory_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqnhVJEwWSRR"
      },
      "source": [
        "# Model functions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO DO:Following cell seems obsolete. Delete it?"
      ],
      "metadata": {
        "id": "M0onVeUgydGB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z12cWU4y09on"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import xgboost as xgb\n",
        "import time\n",
        "\n",
        "# Display model header with parameters\n",
        "def displayModelHeader(featurePath, targetPath, model):\n",
        "    \"\"\"\n",
        "    Display the header for the model report.\n",
        "\n",
        "    Args:\n",
        "        featurePath (str): The path to the features.\n",
        "        targetPath (str): The path to the targets.\n",
        "        model (str): The name of the model.\n",
        "    \"\"\"\n",
        "    print(f\"\\033[1mModel: {model}\\033[0m\")\n",
        "    print(f\"Feature path: {featurePath}\")\n",
        "    print(f\"Target path: {targetPath}\")\n",
        "    print(f\"startyear: {param.features.startyear}, endyear: {param.features.endyear}, naics: {param.features.naics}, state: {param.features.state}\")\n",
        "\n",
        "# Train the model and get the test report\n",
        "def train_model(model, X_train, y_train, X_test, y_test, over_sample):\n",
        "    \"\"\"\n",
        "    Train the model and evaluate its performance.\n",
        "\n",
        "    Args:\n",
        "        model: The machine learning model to train.\n",
        "        X_train (pd.DataFrame): Training features.\n",
        "        y_train (pd.Series): Training targets.\n",
        "        X_test (pd.DataFrame): Testing features.\n",
        "        y_test (pd.Series): Testing targets.\n",
        "        over_sample (bool): Flag to indicate if oversampling should be applied.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Contains model, predictions, accuracy number, G-mean, and classification report dictionary.\n",
        "    \"\"\"\n",
        "    if over_sample:\n",
        "        sm = SMOTE(random_state=2)\n",
        "        X_train, y_train = sm.fit_resample(X_train, y_train.ravel())\n",
        "        print(\"Oversampling done for training data.\")\n",
        "\n",
        "    start = time.time() # Tarun\n",
        "    model.fit(X_train, y_train)\n",
        "    print(\"Model fitted successfully.\")\n",
        "\n",
        "    # Calculate predictions and metrics\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_prob = model.predict_proba(X_test)\n",
        "    end = time.time() # Tarun\n",
        "    duration = end - start # Tarun\n",
        "\n",
        "\n",
        "    # ROC-AUC score\n",
        "    roc_auc = round(roc_auc_score(y_test, y_pred_prob[:, 1]), 2)\n",
        "    print(f\"\\033[1mROC-AUC Score\\033[0m: {roc_auc * 100} %\")\n",
        "\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob[:, 1], pos_label=1)\n",
        "    gmeans = np.sqrt(tpr * (1 - fpr))\n",
        "    ix = np.argmax(gmeans)\n",
        "\n",
        "    print('\\033[1mBest Threshold\\033[0m: %.3f \\n\\033[1mG-Mean\\033[0m: %.3f' % (thresholds[ix], gmeans[ix]))\n",
        "    best_threshold_num = round(thresholds[ix], 3)\n",
        "    gmeans_num = round(gmeans[ix], 3)\n",
        "\n",
        "    # Update predictions based on the best threshold\n",
        "    y_pred = (y_pred > thresholds[ix])\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_num = f\"{accuracy * 100:.1f}\"\n",
        "\n",
        "    print(\"\\033[1mModel Accuracy\\033[0m: \", round(accuracy, 2) * 100, \"%\")\n",
        "    print(\"\\033[1m\\nClassification Report:\\033[0m\")\n",
        "\n",
        "    # Generate classification report\n",
        "    cfc_report = classification_report(y_test, y_pred)\n",
        "    cfc_report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
        "    print(cfc_report)\n",
        "\n",
        "    return model, y_pred, accuracy_num, gmeans_num, roc_auc, best_threshold_num, cfc_report_dict, duration # Added duration as return Tarun\n",
        "\n",
        "# Train the specified model, impute NaN values, and save the trained model along with the feature-target report\n",
        "def train(featurePath, targetPath, model_name, target_column, dataset_name, X_train, y_train, X_test, y_test, report_gen, all_model_list, valid_report_list, over_sample=False, model_saving=True,save_pickle=False, random_state=42):\n",
        "    \"\"\"\n",
        "    Train the specified model and save it along with the reports.\n",
        "\n",
        "    Args:\n",
        "        featurePath (str): The path to the features.\n",
        "        targetPath (str): The path to the targets.\n",
        "        model_name (str): The name of the model to train.\n",
        "        target_column (str): The target column name.\n",
        "        dataset_name (str): The name of the dataset.\n",
        "        X_train (pd.DataFrame): Training features.\n",
        "        y_train (pd.Series): Training targets.\n",
        "        X_test (pd.DataFrame): Testing features.\n",
        "        y_test (pd.Series): Testing targets.\n",
        "        report_gen (bool): Flag to indicate if a report should be generated.\n",
        "        all_model_list (list): List of all available models.\n",
        "        valid_report_list (list): List of models that support report generation.\n",
        "        over_sample (bool): Flag to indicate if oversampling should be applied.\n",
        "        model_saving (bool): Flag to indicate if the model should be saved.\n",
        "        random_state (int): Random state for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Contains paths and evaluation metrics.\n",
        "    \"\"\"\n",
        "    assert model_name in all_model_list, f\"Invalid model name: {model_name}. Must be one of {all_model_list}.\"\n",
        "\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    X_train_imputed = imputer.fit_transform(X_train)\n",
        "    X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "    # model_mapping = {\n",
        "    # \"LogisticRegression\": LogisticRegression(max_iter=10000),  # from cuml.linear_model\n",
        "    # \"SVM\": SVC(probability=True),  # from cuml.svm\n",
        "    # \"MLP\": MLPClassifier(hidden_layer_sizes=(64, 32), activation='relu', solver='adam', max_iter=1000, random_state=random_state),  # CPU model\n",
        "    # \"RandomForest\": RandomForestClassifier(n_estimators=1000, criterion=\"gini\", random_state=random_state),  # from cuml.ensemble\n",
        "    # \"XGBoost\": xgb.XGBClassifier(tree_method='gpu_hist', predictor='gpu_predictor', random_state=random_state, enable_categorical=True)  # GPU-enabled XGB\n",
        "    # }\n",
        "\n",
        "\n",
        "    # model = model_mapping.get(model_name)\n",
        "    # Tarun changes and commented above model mapping code.\n",
        "    model_class = available_model_classes.get(model_name)\n",
        "\n",
        "    if not model_class:\n",
        "        raise ValueError(f\"Model class for {model_name} not found in available_model_classes.\")\n",
        "\n",
        "    # Customize default parameters\n",
        "    if model_name == \"LogisticRegression\":\n",
        "        model = model_class(max_iter=10000)\n",
        "    elif model_name == \"SVM\":\n",
        "        model = model_class(probability=True)\n",
        "    elif model_name == \"MLP\":\n",
        "        model = model_class(hidden_layer_sizes=(64, 32), activation='relu', solver='adam', max_iter=1000, random_state=random_state)\n",
        "    elif model_name == \"RandomForest\":\n",
        "        model = model_class(n_estimators=1000, criterion=\"gini\", random_state=random_state)\n",
        "    elif model_name == \"XGBoost\":\n",
        "        model = model_class(tree_method='gpu_hist', predictor='gpu_predictor', random_state=random_state, enable_categorical=True)\n",
        "    else:\n",
        "        model = model_class()\n",
        "\n",
        "    model_fullname = model_name.replace(\"RandomForest\", \"Random Forest\").replace(\"XGBoost\", \"XGBoost\")\n",
        "\n",
        "    displayModelHeader(featurePath, targetPath, model_fullname)\n",
        "\n",
        "    if model_name == \"XGBoost\":\n",
        "        model, y_pred, accuracy_num, gmeans_num, roc_auc, best_threshold_num, cfc_report_dict, runtime_seconds = train_model(model, X_train, y_train, X_test, y_test, over_sample)\n",
        "    else:\n",
        "        model, y_pred, accuracy_num, gmeans_num, roc_auc, best_threshold_num, cfc_report_dict, runtime_seconds = train_model(model, X_train_imputed, y_train, X_test_imputed, y_test, over_sample)\n",
        "\n",
        "    save_dir = f\"../output/{dataset_name}/saved\"\n",
        "    check_directory(save_dir)\n",
        "\n",
        "    if model_saving and save_pickle:  # Tarun: Added save-pickle flag\n",
        "        save_model(model, imputer if model_name != \"XGBoost\" else None, target_column, dataset_name, model_name, save_dir)\n",
        "\n",
        "    if report_gen:\n",
        "        if model_name in valid_report_list:\n",
        "            if model_name == \"RandomForest\":\n",
        "                importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': model.feature_importances_})\n",
        "            elif model_name == \"XGBoost\":\n",
        "                importance_df = pd.DataFrame(list(model.get_booster().get_score().items()), columns=[\"Feature\", \"Importance\"])\n",
        "            report = importance_df.sort_values(by='Importance', ascending=False)\n",
        "            report[\"Feature_Name\"] = report[\"Feature\"].apply(report_modify)\n",
        "            report = report.reindex(columns=[\"Feature\", \"Feature_Name\", \"Importance\"])\n",
        "            report.to_csv(os.path.join(save_dir, f\"{target_column}-{dataset_name}-report-{model_name}.csv\"), index=False)\n",
        "        else:\n",
        "            print(\"No valid report for the current model\")\n",
        "\n",
        "    return featurePath, targetPath, model, y_pred, report, model_fullname, cfc_report_dict, accuracy_num, gmeans_num, roc_auc, best_threshold_num\n",
        "\n",
        "# Save the trained model and NaN-value imputer\n",
        "def save_model(model, imputer, target_column, dataset_name, model_name, save_dir):\n",
        "    \"\"\"\n",
        "    Save the trained model and imputer to disk.\n",
        "\n",
        "    Args:\n",
        "        model: The trained model to save.\n",
        "        imputer: The imputer used for missing values, if applicable.\n",
        "        target_column (str): The target column name.\n",
        "        dataset_name (str): The name of the dataset.\n",
        "        model_name (str): The name of the model.\n",
        "        save_dir (str): The directory where the model will be saved.\n",
        "    \"\"\"\n",
        "    data = {\n",
        "        \"model\": model,\n",
        "        \"imputer\": imputer\n",
        "    }\n",
        "    with open(os.path.join(save_dir, f\"{target_column}-{dataset_name}-trained-{model_name}.pkl\"), 'wb') as file:\n",
        "        pickle.dump(data, file)\n",
        "\n",
        "# Modify the feature-importance report by adding an industry-correspondence introduction column\n",
        "def report_modify(value):\n",
        "    \"\"\"\n",
        "    Modify feature names for better readability in reports.\n",
        "\n",
        "    Args:\n",
        "        value (str): The original feature name.\n",
        "\n",
        "    Returns:\n",
        "        str: The modified feature name.\n",
        "    \"\"\"\n",
        "    splitted = value.split(\"-\")\n",
        "    if splitted[0] in [\"Emp\", \"Est\", \"Pay\"]:\n",
        "        try:\n",
        "            modified = splitted[0] + \"-\" + INDUSTRIES_DICT[splitted[1]] + \"-\" + splitted[2]\n",
        "        except KeyError:\n",
        "            modified = value  # Keep original if not found\n",
        "        return modified\n",
        "    else:\n",
        "        return value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1wLsu6Zr2oK"
      },
      "outputs": [],
      "source": [
        "# STEP: Read the single CSV file and save it as the full dataset csv\n",
        "# If save_training=True, your files will reside in the \"output\" folder.\n",
        "\n",
        "save_dir = full_save_dir  # Use the local directory if save_training is True\n",
        "\n",
        "# Check if the directory exists or create it\n",
        "check_directory(save_dir)\n",
        "\n",
        "# Since there is only one CSV file, directly read and process it\n",
        "csv_file = f\"../process/{dataset_name}/{target_column}-{dataset_name}.csv\"\n",
        "\n",
        "# Ensure csv_file is available before reading\n",
        "if save_training:\n",
        "    if os.path.exists(csv_file):  # Check if the CSV file exists\n",
        "        df = pd.read_csv(csv_file)\n",
        "        print(f\"Read file from: {csv_file}\")\n",
        "        # Save the integrated file to the desired location\n",
        "        file_path = os.path.join(save_dir, f\"{target_column}-{dataset_name}.csv\")\n",
        "        df.to_csv(file_path, index=False)\n",
        "        print(f\"Saved file at: {file_path}\")\n",
        "    else:\n",
        "        print(f\"Warning: CSV file not found at {csv_file}. Please check the path.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5xknpBMr4IV"
      },
      "outputs": [],
      "source": [
        "print(f\"target_column: {target_column}\")\n",
        "print(f\"dataset_name: {dataset_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGoijsNmr5U3"
      },
      "outputs": [],
      "source": [
        "file_path = os.path.join(full_save_dir, f\"{target_column}-{dataset_name}.csv\")\n",
        "print(f\"Reading file from: {file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqr3Jg1Tr9fa"
      },
      "outputs": [],
      "source": [
        "#TODO : Add details for the fips code; and maybe figure out why the fips from other states popup\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Check if 'target_df' exists and has 'Fips' column\n",
        "if 'target_df' in locals() and 'Fips' in target_df.columns:\n",
        "    # Convert 'Fips' to numeric if it's not already\n",
        "    target_df['Fips'] = pd.to_numeric(target_df['Fips'], errors='coerce')\n",
        "\n",
        "    # Plot histogram\n",
        "    target_df['Fips'].plot(kind='hist', bins=20, title='Fips')\n",
        "    plt.gca().spines[['top', 'right']].set_visible(False)\n",
        "    plt.xlabel('FIPS Code')  # Label for x-axis\n",
        "    plt.ylabel('Frequency')    # Label for y-axis\n",
        "    plt.show()  # Show the plot\n",
        "else:\n",
        "    # print(\"Error: target_df is not defined or 'Fips' column is missing.\")\n",
        "    # This need not be an error as in the case of Eye Blinks dataset YAML.\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdUt24w63WDa"
      },
      "outputs": [],
      "source": [
        "# STEP: Get Dictionaries for states and industries\n",
        "\n",
        "# TO DO: Try including DC and US Territories\n",
        "STATE_DICT = {\n",
        "    \"AL\": \"Alabama\", \"AK\": \"Alaska\", \"AZ\": \"Arizona\", \"AR\": \"Arkansas\", \"CA\": \"California\", \"CO\": \"Colorado\",\n",
        "    \"CT\": \"Connecticut\", \"DE\": \"Delaware\", \"FL\": \"Florida\", \"GA\": \"Georgia\", \"HI\": \"Hawaii\", \"ID\": \"Idaho\",\n",
        "    \"IL\": \"Illinois\", \"IN\": \"Indiana\", \"IA\": \"Iowa\", \"KS\": \"Kansas\", \"KY\": \"Kentucky\", \"LA\": \"Louisiana\",\n",
        "    \"ME\": \"Maine\", \"MD\": \"Maryland\", \"MA\": \"Massachusetts\", \"MI\": \"Michigan\", \"MN\": \"Minnesota\", \"MS\": \"Mississippi\",\n",
        "    \"MO\": \"Missouri\", \"MT\": \"Montana\", \"NE\": \"Nebraska\", \"NV\": \"Nevada\", \"NH\": \"New Hampshire\", \"NJ\": \"New Jersey\",\n",
        "    \"NM\": \"New Mexico\", \"NY\": \"New York\", \"NC\": \"North Carolina\", \"ND\": \"North Dakota\", \"OH\": \"Ohio\", \"OK\": \"Oklahoma\",\n",
        "    \"OR\": \"Oregon\", \"PA\": \"Pennsylvania\", \"RI\": \"Rhode Island\", \"SC\": \"South Carolina\", \"SD\": \"South Dakota\",\n",
        "    \"TN\": \"Tennessee\", \"TX\": \"Texas\", \"UT\": \"Utah\", \"VT\": \"Vermont\", \"VA\": \"Virginia\", \"WA\": \"Washington\",\n",
        "    \"WV\": \"West Virginia\", \"WI\": \"Wisconsin\", \"WY\": \"Wyoming\",\n",
        "    \"DC\": \"District of Columbia\",\n",
        "    # US Territories\n",
        "    \"AS\": \"American Samoa\", \"GU\": \"Guam\", \"MP\": \"Northern Mariana Islands\", \"PR\": \"Puerto Rico\", \"VI\": \"U.S. Virgin Islands\"\n",
        "}\n",
        "\n",
        "STATE_DICT_DELETE = {\n",
        "    \"AL\": \"ALABAMA\",\"AK\": \"ALASKA\",\"AZ\": \"ARIZONA\",\"AR\": \"ARKANSAS\",\"CA\": \"CALIFORNIA\",\"CO\": \"COLORADO\",\"CT\": \"CONNECTICUT\",\"DE\": \"DELAWARE\",\"FL\": \"FLORIDA\",\"GA\": \"GEORGIA\",\"HI\": \"HAWAII\",\"ID\": \"IDAHO\",\"IL\": \"ILLINOIS\",\"IN\": \"INDIANA\",\"IA\": \"IOWA\",\"KS\": \"KANSAS\",\"KY\": \"KENTUCKY\",\"LA\": \"LOUISIANA\",\"ME\": \"MAINE\",\"MD\": \"MARYLAND\",\"MA\": \"MASSACHUSETTS\",\"MI\": \"MICHIGAN\",\"MN\": \"MINNESOTA\",\"MS\": \"MISSISSIPPI\",\"MO\": \"MISSOURI\",\"MT\": \"MONTANA\",\"NE\": \"NEBRASKA\",\"NV\": \"NEVADA\",\"NH\": \"NEW HAMPSHIRE\",\"NJ\": \"NEW JERSEY\",\"NM\": \"NEW MEXICO\",\"NY\": \"NEW YORK\",\"NC\": \"NORTH CAROLINA\",\"ND\": \"NORTH DAKOTA\",\"OH\": \"OHIO\",\"OK\": \"OKLAHOMA\",\"OR\": \"OREGON\",\"PA\": \"PENNSYLVANIA\",\"RI\": \"RHODE ISLAND\",\"SC\": \"SOUTH CAROLINA\",\"SD\": \"SOUTH DAKOTA\",\"TN\": \"TENNESSEE\",\"TX\": \"TEXAS\",\"UT\": \"UTAH\",\"VT\": \"VERMONT\",\"VA\": \"VIRGINIA\",\"WA\": \"WASHINGTON\",\"WV\": \"WEST VIRGINIA\",\"WI\": \"WISCONSIN\",\"WY\": \"WYOMING\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxdE9ltIsBJJ"
      },
      "outputs": [],
      "source": [
        "# Define INDUSTRIES_DICT as an empty dictionary initially\n",
        "# industries_df is not currently in use - File only exists for country US and naics 2.\n",
        "# TO DO: Use to show top level industry categories in importance reports\n",
        "# Source: https://github.com/ModelEarth/community-data/blob/master/us/id_lists/naics2.csv\n",
        "INDUSTRIES_DICT = {}\n",
        "country = \"US\"\n",
        "naics_level = 2\n",
        "industries_csv_file = f\"https://raw.githubusercontent.com/ModelEarth/community-data/master/{country.lower()}/id_lists/naics{naics_level}.csv\"\n",
        "# Attempt to load the industries DataFrame from URL\n",
        "try:\n",
        "    industries_df = pd.read_csv(\n",
        "        f\"https://raw.githubusercontent.com/ModelEarth/community-data/master/{country.lower()}/id_lists/naics{naics_level}.csv\",\n",
        "        header=None\n",
        "    )\n",
        "    INDUSTRIES_DICT = industries_df.set_index(0).to_dict()[1]\n",
        "    print(\"Successfully loaded industries_df from URL.\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to load industries_df from URL due to error: {e}\")\n",
        "    # Try loading from the local file path as a fallback\n",
        "    try:\n",
        "        industries_df = pd.read_csv(industries_csv_file, header=None, names=['Industry_Code', 'Industry_Name'])\n",
        "        INDUSTRIES_DICT = industries_df.set_index('Industry_Code').to_dict()['Industry_Name']\n",
        "        print(\"Successfully loaded industries_df from local file.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file {industries_csv_file} does not exist.\")\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(\"Error: The CSV file is empty.\")\n",
        "    except pd.errors.ParserError:\n",
        "        print(\"Error: There was a parsing error while reading the CSV file.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while loading the CSV: {e}\")\n",
        "\n",
        "# Now, print the columns of industries_df if it is defined\n",
        "if 'industries_df' in locals():  # Check if industries_df is defined\n",
        "    print(\"Columns in industries_df:\")\n",
        "    print(industries_df.columns)\n",
        "else:\n",
        "    print(\"Error: industries_df is not defined. Please check the loading process.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jv_AUQwjnrkN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import cudf\n",
        "import cupy as cp\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Paths and settings\n",
        "features_template = param.features.path\n",
        "\n",
        "naics_values = getattr(param.features, \"naics\", [])\n",
        "\n",
        "startyear = getattr(param.features, \"startyear\", 1970)\n",
        "endyear = getattr(param.features, \"endyear\", 1969)\n",
        "years = range(startyear, endyear + 1)\n",
        "\n",
        "states = getattr(param.features, \"state\", \"\").split(\",\")\n",
        "\n",
        "full_save_dir = \"output/training\"\n",
        "\n",
        "os.makedirs(full_save_dir, exist_ok=True)\n",
        "\n",
        "# Build feature file paths\n",
        "feature_files = []\n",
        "for state in states:\n",
        "  for year in years:\n",
        "    for naics in naics_values:\n",
        "      feature_files.append(features_template.format(naics=naics, year=year, state=state))\n",
        "\n",
        "if not feature_files:\n",
        "  # This means param.features.path is not a template but an actual URL\n",
        "  feature_files = [features_template]\n",
        "\n",
        "print(\"Constructed Feature File Paths:\")\n",
        "for feature_file in feature_files:\n",
        "    print(feature_file)\n",
        "\n",
        "# Load feature datasets\n",
        "feature_dfs = []\n",
        "for feature_file in feature_files:\n",
        "    try:\n",
        "        feature_dfs.append(pd.read_csv(feature_file))\n",
        "        print(f\"Loaded feature file: {feature_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading feature file {feature_file}: {e}\")\n",
        "\n",
        "if not feature_dfs:\n",
        "    raise FileNotFoundError(\"No feature files could be loaded. Please check the paths and try again.\")\n",
        "\n",
        "features_df = pd.concat(feature_dfs, ignore_index=True)\n",
        "\n",
        "if target_url is None:\n",
        "  X_total_cpu = features_df.drop(columns=[target_column])\n",
        "  y_total_cpu = features_df[target_column]\n",
        "  aligned_df = features_df\n",
        "else:\n",
        "\n",
        "  # Load target dataset\n",
        "  try:\n",
        "      target_df = pd.read_csv(target_url)\n",
        "      print(\"Targets loaded successfully.\")\n",
        "  except Exception as e:\n",
        "      raise FileNotFoundError(f\"Error loading target file {target_url}: {e}\")\n",
        "\n",
        "  # Make Fips columns consistent\n",
        "  features_df[\"Fips\"] = features_df[\"Fips\"].astype(str)\n",
        "  target_df[\"Fips\"] = target_df[\"Fips\"].astype(str)\n",
        "\n",
        "  # Filter features_df to only Fips present in target_df\n",
        "  features_df = features_df[features_df[\"Fips\"].isin(target_df[\"Fips\"])]\n",
        "\n",
        "  # Sort and merge\n",
        "  features_df = features_df.sort_values(by=\"Fips\")\n",
        "  target_df = target_df.sort_values(by=\"Fips\")\n",
        "\n",
        "  aligned_df = pd.merge(features_df, target_df, on=\"Fips\", how=\"inner\")\n",
        "\n",
        "  # Verify merged data\n",
        "  print(\"\\nMerged aligned_df shape:\", aligned_df.shape)\n",
        "\n",
        "  # Separate features and target\n",
        "  X_total_cpu = aligned_df.drop(columns=[\"Target\"])\n",
        "  y_total_cpu = aligned_df[\"Target\"]\n",
        "\n",
        "print(\"X_total_cpu shape:\", X_total_cpu.shape)\n",
        "print(\"y_total_cpu shape:\", y_total_cpu.shape)\n",
        "\n",
        "# Convert to GPU\n",
        "X_total = cudf.DataFrame.from_pandas(X_total_cpu)\n",
        "y_total = cp.asarray(y_total_cpu)\n",
        "\n",
        "print(\"Data converted to GPU format successfully.\")\n",
        "print(\"X_total (GPU) rows:\", len(X_total))\n",
        "print(\"y_total (GPU) rows:\", len(y_total))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_total.describe()"
      ],
      "metadata": {
        "id": "q9rFSP9IOzQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxTWxzY5TILD"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZhG7WrwS8Fu"
      },
      "outputs": [],
      "source": [
        "def basic_info(df):\n",
        "    print(\"\\nData Overview\")\n",
        "    print(df.head())\n",
        "    print(\"\\nShape of the dataset:\", df.shape)\n",
        "    print(\"\\nColumn Information:\")\n",
        "    print(df.info())\n",
        "    print(\"\\nDescriptive Statistics:\")\n",
        "\n",
        "    if isinstance(df, cudf.DataFrame):\n",
        "        print(df.describe())  # no transpose for cudf\n",
        "    else:\n",
        "        print(df.describe().T)  # transpose for pandas\n",
        "\n",
        "    print(\"\\nNull Values:\")\n",
        "    print(df.isnull().sum())\n",
        "    print(\"\\nNumber of duplicate rows:\", df.duplicated().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AP0AI-aXSLGt"
      },
      "outputs": [],
      "source": [
        "basic_info(aligned_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCgMCSupTrI2"
      },
      "outputs": [],
      "source": [
        "basic_info(X_total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gIGX9xyxjKx"
      },
      "outputs": [],
      "source": [
        "X_total.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLCCpsHTSPKQ"
      },
      "outputs": [],
      "source": [
        "# Find duplicates\n",
        "duplicates = X_total.duplicated(keep=\"first\")\n",
        "duplicates_cpu = duplicates.to_pandas()\n",
        "\n",
        "# Filter and show\n",
        "aligned_df_duplicates = aligned_df[duplicates_cpu]\n",
        "\n",
        "print(f\"Number of duplicate rows found: {aligned_df_duplicates.shape[0]}\")\n",
        "aligned_df_duplicates.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQBCRw5lU7m9"
      },
      "outputs": [],
      "source": [
        "def missing_values_distribution(df):\n",
        "    \"\"\"\n",
        "    Plots distribution of missing values across features.\n",
        "    Works for both pandas and cuDF DataFrames.\n",
        "    \"\"\"\n",
        "    missing_ratios = df.isnull().mean() * 100\n",
        "\n",
        "    # If GPU (cuDF), convert to pandas Series\n",
        "    if str(type(missing_ratios)).startswith(\"<class 'cudf\"):\n",
        "        missing_ratios = missing_ratios.to_pandas()\n",
        "\n",
        "    # Now plotting\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    missing_ratios.hist(bins=30, color='skyblue', edgecolor='black')\n",
        "    plt.title('Distribution of Missing Value Percentages Across All Features')\n",
        "    plt.xlabel('Percentage of Missing Values')\n",
        "    plt.ylabel('Number of Features')\n",
        "    plt.grid(False)\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(10, 3))\n",
        "    plt.boxplot(missing_ratios, vert=False, patch_artist=True,\n",
        "                flierprops={'marker': 'o', 'color': 'red', 'markersize': 5})\n",
        "    plt.title('Boxplot of Missing Value Percentages')\n",
        "    plt.xlabel('Percentage of Missing Values')\n",
        "    plt.yticks([])\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7ZF7eLcVqnM"
      },
      "outputs": [],
      "source": [
        "# Missing values are okay. They indicate an industry does not exist in a county.\n",
        "missing_values_distribution(X_total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZWF8aVwy6iI"
      },
      "outputs": [],
      "source": [
        "# Fill NAs with 0\n",
        "def fill_na(dataframe):\n",
        "    dataframe = dataframe.fillna(0)\n",
        "    return dataframe\n",
        "# X_total=fill_na(X_total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NC14Jh8Akl_z"
      },
      "outputs": [],
      "source": [
        "def select_columns(dataframe, prefixes_to_exclude=None, name_to_exclude=None):\n",
        "    # Filter columns based on exclusion prefixes\n",
        "    columns_to_exclude = [col for col in dataframe.columns if any(col.startswith(prefix) for prefix in prefixes_to_exclude)]\n",
        "\n",
        "    # Remove the specific column name if provided\n",
        "    if name_to_exclude and name_to_exclude in dataframe.columns:\n",
        "        columns_to_exclude.append(name_to_exclude)\n",
        "\n",
        "    # Final columns to keep\n",
        "    columns_to_keep = [col for col in dataframe.columns if col not in columns_to_exclude]\n",
        "\n",
        "    return dataframe[columns_to_keep]\n",
        "\n",
        "\n",
        "X_total = select_columns(X_total, prefixes_to_exclude=['Est', 'Pay'], name_to_exclude='Name')\n",
        "###Xucen Liao, due to the high correlation between PercentUrban and Population, exclude PercentUrban\n",
        "X_total = select_columns(X_total, prefixes_to_exclude=['Est', 'Pay'], name_to_exclude='PercentUrban')\n",
        "X_total.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81mniYtpzqua"
      },
      "outputs": [],
      "source": [
        "X_total.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8vlkUawtVnX"
      },
      "outputs": [],
      "source": [
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "def plot_histograms_and_test_normality(df, column_indices):\n",
        "    results = pd.DataFrame(columns=['Column', 'Shapiro_Statistic', 'Shapiro_p-value'])\n",
        "\n",
        "    for column in df.columns[column_indices]:\n",
        "        data = df[column].dropna()\n",
        "\n",
        "        # If cuDF, convert to pandas\n",
        "        if str(type(data)).startswith(\"<class 'cudf\"):\n",
        "            data = data.to_pandas()\n",
        "\n",
        "        # Force conversion to numeric (important)\n",
        "        data = pd.to_numeric(data, errors='coerce')\n",
        "        data = data.dropna()  # Final cleaning\n",
        "\n",
        "        if len(data) < 3:\n",
        "            print(f\"Skipping column {column} due to insufficient valid data.\")\n",
        "            continue\n",
        "\n",
        "        # Create histogram plot\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.hist(data, bins=30, alpha=0.75, color='blue')\n",
        "        plt.title(f'Histogram of {column}')\n",
        "        plt.xlabel('Data Points')\n",
        "        plt.ylabel('Frequency')\n",
        "\n",
        "        # Perform Shapiro-Wilk test\n",
        "        shapiro_stat, shapiro_p = stats.shapiro(data)\n",
        "\n",
        "        # QQ plot\n",
        "        plt.subplot(1, 2, 2)\n",
        "        stats.probplot(data, dist=\"norm\", plot=plt)\n",
        "        plt.title(f'QQ Plot of {column}')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        results = pd.concat([results, pd.DataFrame({\n",
        "            'Column': [column],\n",
        "            'Shapiro_Statistic': [shapiro_stat],\n",
        "            'Shapiro_p-value': [shapiro_p]\n",
        "        })], ignore_index=True)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Example usage\n",
        "column_indices = slice(0, 20)\n",
        "results = plot_histograms_and_test_normality(X_total, column_indices)\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZcET7hMv1v-"
      },
      "outputs": [],
      "source": [
        "def apply_log_transform(df, exclude_columns=None):\n",
        "    transformed_df = df.copy()\n",
        "    if exclude_columns is None:\n",
        "        exclude_columns = []\n",
        "\n",
        "    for column in transformed_df.columns:\n",
        "        if pd.api.types.is_numeric_dtype(transformed_df[column]) and column not in exclude_columns:\n",
        "            transformed_df[column] = np.log1p(transformed_df[column])\n",
        "    return transformed_df\n",
        "\n",
        "\n",
        "# 'latitude', 'longitude' represent the location and we do not need to assume it is normally distributed\n",
        "exclude_columns = ['Latitude', 'Longitude', 'Fips']\n",
        "X_total = apply_log_transform(X_total, exclude_columns=exclude_columns)\n",
        "X_total.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEXLYbluERka"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "def preprocess_data(dataframe, scale_type='standardize', include_target=False, target=None):\n",
        "    if scale_type == 'standardize':\n",
        "        scaler = StandardScaler()\n",
        "    elif scale_type == 'normalize':\n",
        "        scaler = MinMaxScaler()\n",
        "    else:\n",
        "        raise ValueError(\"Invalid scaling type. Choose 'standardize' or 'normalize'.\")\n",
        "\n",
        "    # Convert to pandas for sklearn scalers\n",
        "    if isinstance(dataframe, cudf.DataFrame):\n",
        "        dataframe_pd = dataframe.to_pandas()\n",
        "    else:\n",
        "        dataframe_pd = dataframe\n",
        "\n",
        "    if include_target and target in dataframe_pd.columns:\n",
        "        features = dataframe_pd.drop(columns=[target])\n",
        "        scaled_features = scaler.fit_transform(features)\n",
        "        scaled_df = pd.DataFrame(scaled_features, columns=features.columns)\n",
        "        scaled_df[target] = dataframe_pd[target].values\n",
        "    else:\n",
        "        scaled_features = scaler.fit_transform(dataframe_pd)\n",
        "        scaled_df = pd.DataFrame(scaled_features, columns=dataframe_pd.columns)\n",
        "\n",
        "    # Convert back to cuDF\n",
        "    return cudf.DataFrame.from_pandas(scaled_df)\n",
        "\n",
        "X_total = preprocess_data(X_total, scale_type='standardize', include_target=False)\n",
        "X_total.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4NWQQ8N0NxU"
      },
      "outputs": [],
      "source": [
        "X_total.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJWrF5IDnoQt"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_correlation_heatmap(dataframe, column_prefix):\n",
        "    columns_to_analyze = [col for col in dataframe.columns if not col.startswith(column_prefix)]\n",
        "\n",
        "    # Ensure the correlation matrix is computed using pandas\n",
        "    corr_matrix = dataframe[columns_to_analyze].to_pandas().corr()\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True, square=True)\n",
        "    plt.title('Correlation Heatmap')\n",
        "    plt.show()\n",
        "\n",
        "plot_correlation_heatmap(X_total, 'Emp')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjaXGi4_2Zkp"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_correlation_heatmap(dataframe, column_prefix, target_series=None, target_name='target'):\n",
        "    columns_to_analyze = [col for col in dataframe.columns if not col.startswith(column_prefix)]\n",
        "\n",
        "    if target_series is not None:\n",
        "        if len(target_series) == len(dataframe):\n",
        "            dataframe = dataframe.copy()\n",
        "            dataframe[target_name] = target_series\n",
        "            columns_to_analyze.append(target_name)\n",
        "        else:\n",
        "            raise ValueError(\"The length of target_series and dataframe must match.\")\n",
        "\n",
        "    corr_matrix = dataframe[columns_to_analyze].to_pandas().corr()\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True, square=True)\n",
        "    plt.title('Correlation Heatmap')\n",
        "    plt.show()\n",
        "\n",
        "plot_correlation_heatmap(X_total, 'Emp', y_total, 'y_total')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_98zcueHejZA"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "def target_variable_analysis(df):\n",
        "    if isinstance(df, cp.ndarray):\n",
        "        df = pd.Series(cp.asnumpy(df))\n",
        "\n",
        "    print(\"\\nTarget Variable Analysis\")\n",
        "    print(\"Data Type:\", df.dtype)\n",
        "    print(\"Unique Values:\", df.nunique())\n",
        "    print(\"Value Counts:\")\n",
        "    print(df.value_counts())\n",
        "\n",
        "    if df.nunique() < 20:\n",
        "        df.value_counts().plot(kind='bar', color='orange', figsize=(10, 6))\n",
        "        plt.title('Target Variable Distribution (Categorical)')\n",
        "        plt.xlabel('Classes')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yfsogoqdev6q"
      },
      "outputs": [],
      "source": [
        "target_variable_analysis(y_total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6YlbNDcVKzy"
      },
      "outputs": [],
      "source": [
        "!pip install imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_total"
      ],
      "metadata": {
        "id": "-xWcI-2tPGf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAg8Qy7xSHwZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "# Perform train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_total,\n",
        "    y_total,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Save the train-test split datasets if required\n",
        "save_training = True\n",
        "if save_training:\n",
        "    X_train.to_pandas().to_csv(os.path.join(full_save_dir, \"X_train.csv\"), index=False)\n",
        "    X_test.to_pandas().to_csv(os.path.join(full_save_dir, \"X_test.csv\"), index=False)\n",
        "    pd.Series(cp.asnumpy(y_train)).to_csv(os.path.join(full_save_dir, \"y_train.csv\"), index=False)\n",
        "    pd.Series(cp.asnumpy(y_test)).to_csv(os.path.join(full_save_dir, \"y_test.csv\"), index=False)\n",
        "    print(\"Train-test split files saved successfully.\")\n",
        "\n",
        "print(\"Processing completed successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDegRCiCVTHz"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Fill NaNs in X_train\n",
        "X_train_filled = X_train.fillna(0)\n",
        "\n",
        "# Convert to pandas and numpy before SMOTE\n",
        "X_train_filled_pd = X_train_filled.to_pandas()\n",
        "y_train_np = cp.asnumpy(y_train)\n",
        "\n",
        "# Apply SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_smote_pd, y_train_smote_np = smote.fit_resample(X_train_filled_pd, y_train_np)\n",
        "\n",
        "# Select only numeric columns from the SMOTE output\n",
        "X_train_smote_pd = X_train_smote_pd.select_dtypes(include=np.number)\n",
        "\n",
        "# Convert back to GPU\n",
        "X_train_smote = cudf.DataFrame.from_pandas(X_train_smote_pd)\n",
        "y_train_smote = cp.asarray(y_train_smote_np)\n",
        "\n",
        "print(\"SMOTE applied successfully. Shapes after resampling:\")\n",
        "print(X_train_smote.shape, y_train_smote.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUYu5ZV8_t5k"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Count before and after SMOTE\n",
        "before_counts = pd.Series(cp.asnumpy(y_train)).value_counts().sort_index()\n",
        "after_counts = pd.Series(cp.asnumpy(y_train_smote)).value_counts().sort_index()\n",
        "\n",
        "# Plot\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "# Before SMOTE\n",
        "axes[0].bar(before_counts.index.astype(str), before_counts.values, color='salmon')\n",
        "axes[0].set_title(\"Class Distribution Before SMOTE\")\n",
        "axes[0].set_xlabel(\"Class\")\n",
        "axes[0].set_ylabel(\"Count\")\n",
        "for i, v in enumerate(before_counts.values):\n",
        "    axes[0].text(i, v + 2, str(v), ha='center')\n",
        "\n",
        "# After SMOTE\n",
        "axes[1].bar(after_counts.index.astype(str), after_counts.values, color='seagreen')\n",
        "axes[1].set_title(\"Class Distribution After SMOTE\")\n",
        "axes[1].set_xlabel(\"Class\")\n",
        "axes[1].set_ylabel(\"Count\")\n",
        "for i, v in enumerate(after_counts.values):\n",
        "    axes[1].text(i, v + 2, str(v), ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbtyrJQfsPGz"
      },
      "source": [
        "# Model training, testing and results saving:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_98WkjcsMca"
      },
      "source": [
        "Below code block can train multiple models at the same time due to use of a function and loop. This is the second version of printing results in the colab file manually using print statements and no report generator function."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cupy as cp\n",
        "import cudf\n",
        "\n",
        "# ------------------ Helper Functions ------------------ #\n",
        "def safe_to_cpu(arr):\n",
        "    \"\"\"Safely convert any GPU array (cuDF, CuPy) to CPU numpy.\"\"\"\n",
        "    if isinstance(arr, cp.ndarray):\n",
        "        return cp.asnumpy(arr)\n",
        "    elif isinstance(arr, (cudf.Series, cudf.DataFrame)):\n",
        "        return arr.to_numpy()\n",
        "    else:\n",
        "        return arr"
      ],
      "metadata": {
        "id": "CniD3TAxGS6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Python 3 scikit-learn-style wrapper for the Random Bits Forest (RBF) binary.\n",
        "\n",
        "Features\n",
        "- Auto-downloads the RBF binary from SourceForge if it's missing\n",
        "  (override URL with env RBF_BINARY_URL).\n",
        "- Writes both CSV and space-delimited inputs to maximize compatibility.\n",
        "- Reads common output filenames: testYhat / testy / testyhat (with/without extension).\n",
        "- predict_proba(X) -> (n_samples, 2) as [P0, P1]; predict(X) returns labels.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import uuid\n",
        "import glob\n",
        "import shutil\n",
        "import warnings\n",
        "import tempfile\n",
        "import subprocess\n",
        "import zipfile\n",
        "from urllib.request import urlopen, Request\n",
        "from typing import Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "DEFAULT_RBF_URL = \"https://downloads.sourceforge.net/project/random-bits-forest/rbf.zip\"\n",
        "\n",
        "\n",
        "def _to_2d(X) -> np.ndarray:\n",
        "    if hasattr(X, \"to_numpy\"):\n",
        "        X = X.to_numpy()\n",
        "    X = np.asarray(X)\n",
        "    if X.ndim == 1:\n",
        "        X = X.reshape(-1, 1)\n",
        "    return X.astype(float, copy=False)\n",
        "\n",
        "\n",
        "def _to_1d(y) -> np.ndarray:\n",
        "    if hasattr(y, \"to_numpy\"):\n",
        "        y = y.to_numpy()\n",
        "    y = np.asarray(y)\n",
        "    if y.ndim > 1:\n",
        "        y = y.ravel()\n",
        "    return y\n",
        "\n",
        "\n",
        "class RandomBitsForest(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(\n",
        "        self,\n",
        "        number_of_trees: int = 200,\n",
        "        bin_path: Optional[str] = None,     # defaults to \"<this_dir>/rbf/rbf\"\n",
        "        temp_extension: str = \".csv\",       # also writes no-extension copies\n",
        "        verbose: bool = False,\n",
        "        auto_download: bool = True,         # download binary if missing\n",
        "        download_url: Optional[str] = None, # override URL if needed\n",
        "    ):\n",
        "        self.number_of_trees = number_of_trees\n",
        "        self.bin_path = bin_path\n",
        "        self.temp_extension = temp_extension\n",
        "        self.verbose = verbose\n",
        "        self.auto_download = auto_download\n",
        "        self.download_url = download_url\n",
        "\n",
        "        # fitted artifacts\n",
        "        self._le: Optional[LabelEncoder] = None\n",
        "        self._X_train: Optional[np.ndarray] = None\n",
        "        self._y_train: Optional[np.ndarray] = None\n",
        "        self.n_features_in_: Optional[int] = None\n",
        "\n",
        "        # runtime logs\n",
        "        self.last_stdout: str = \"\"\n",
        "        self.last_stderr: str = \"\"\n",
        "        self.last_cwd: Optional[str] = None\n",
        "\n",
        "    # ------------------ sklearn API ------------------ #\n",
        "    def fit(self, X, y):\n",
        "        y = safe_to_cpu(y)\n",
        "        X = _to_2d(X)\n",
        "        y = _to_1d(y)\n",
        "\n",
        "        self._le = LabelEncoder()\n",
        "        y_enc = self._le.fit_transform(y)\n",
        "        classes = np.unique(y_enc)\n",
        "        if classes.size != 2:\n",
        "            raise ValueError(\n",
        "                f\"RandomBitsForest currently supports binary classification only; \"\n",
        "                f\"got classes={list(self._le.classes_)}\"\n",
        "            )\n",
        "\n",
        "        self._X_train = X\n",
        "        self._y_train = y_enc.astype(float)\n",
        "        self.n_features_in_ = X.shape[1]\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X) -> np.ndarray:\n",
        "        if self._X_train is None or self._y_train is None:\n",
        "            raise RuntimeError(\"Call fit(X, y) before predict_proba.\")\n",
        "\n",
        "        X = _to_2d(X)\n",
        "        if X.shape[1] != self.n_features_in_:\n",
        "            raise ValueError(f\"Incompatible n_features: got {X.shape[1]} but fitted with {self.n_features_in_}\")\n",
        "\n",
        "        with self._temp_workspace() as workdir:\n",
        "            self._ensure_binary(workdir)\n",
        "            io_paths = self._write_io_files(workdir, self._X_train, self._y_train, X)\n",
        "            self._run_binary(workdir, io_paths)\n",
        "            proba_1 = self._read_output(workdir, io_paths).astype(float).ravel()\n",
        "\n",
        "        proba_1 = np.clip(proba_1, 0.0, 1.0)\n",
        "        proba_0 = 1.0 - proba_1\n",
        "        return np.vstack([proba_0, proba_1]).T\n",
        "\n",
        "\n",
        "    def predict(self, X) -> np.ndarray:\n",
        "        P1 = self.predict_proba(X)[:, 1]\n",
        "        y_bin = (P1 >= 0.5).astype(int)\n",
        "        return self._le.inverse_transform(y_bin)\n",
        "\n",
        "    # ------------------ binary handling ------------------ #\n",
        "    def _default_bin_path(self) -> str:\n",
        "        # Notebooks don't have __file__\n",
        "        here = os.path.dirname(os.path.abspath(__file__)) if \"__file__\" in globals() else os.getcwd()\n",
        "        return os.path.join(here, \"rbf\", \"rbf\")\n",
        "\n",
        "\n",
        "    def _ensure_binary(self, workdir: str):\n",
        "        bin_path = self.bin_path or self._default_bin_path()\n",
        "        if os.path.exists(bin_path) and os.access(bin_path, os.X_OK):\n",
        "            return\n",
        "\n",
        "        if not self.auto_download:\n",
        "            raise FileNotFoundError(\n",
        "                f\"RBF binary not found at {bin_path} and auto_download=False\"\n",
        "            )\n",
        "\n",
        "        target_dir = os.path.dirname(bin_path)\n",
        "        os.makedirs(target_dir, exist_ok=True)\n",
        "        url = self.download_url or os.environ.get(\"RBF_BINARY_URL\", DEFAULT_RBF_URL)\n",
        "        if self.verbose:\n",
        "            print(f\"[RBF] downloading binary from: {url}\")\n",
        "\n",
        "        tmp_zip = os.path.join(workdir, f\"rbf_{uuid.uuid4().hex}.zip\")\n",
        "        self._download_file(url, tmp_zip)\n",
        "        print(f\"tmp_zip: {tmp_zip}\")\n",
        "        with zipfile.ZipFile(tmp_zip) as zf:\n",
        "            zf.extractall(target_dir)\n",
        "\n",
        "        # try to locate 'rbf' inside target_dir (sometimes nested)\n",
        "        cand = None\n",
        "        for root, _, files in os.walk(target_dir):\n",
        "            if \"rbf\" in files:\n",
        "                cand = os.path.join(root, \"rbf\")\n",
        "                break\n",
        "        if cand is None:\n",
        "            raise FileNotFoundError(\n",
        "                f\"Downloaded zip did not contain an 'rbf' executable in {target_dir}\"\n",
        "            )\n",
        "\n",
        "        # put/copy it at the canonical location if different\n",
        "        if os.path.abspath(cand) != os.path.abspath(bin_path):\n",
        "            os.makedirs(os.path.dirname(bin_path), exist_ok=True)\n",
        "            shutil.copy2(cand, bin_path)\n",
        "\n",
        "        # ensure executable\n",
        "        try:\n",
        "            os.chmod(bin_path, 0o755)\n",
        "        except Exception as e:\n",
        "            warnings.warn(f\"Could not chmod +x {bin_path}: {e}\")\n",
        "\n",
        "\n",
        "    def _download_file(self, url: str, dst_path: str) -> bool:\n",
        "        cmd = [\"wget\", \"-q\", \"--content-disposition\", \"-O\", dst_path, url]\n",
        "        # Add retries to be safe:\n",
        "        # cmd = [\"wget\", \"-q\", \"--tries=3\", \"--timeout=30\", \"--content-disposition\", \"-O\", dst_path, url]\n",
        "        try:\n",
        "            subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "        except subprocess.CalledProcessError:\n",
        "            return False\n",
        "        return zipfile.is_zipfile(dst_path)\n",
        "\n",
        "\n",
        "    # ------------------ IO helpers ------------------ #\n",
        "    def _write_io_files(self, workdir: str, Xtr: np.ndarray, ytr: np.ndarray, Xte: np.ndarray):\n",
        "      ext = self.temp_extension if self.temp_extension else \".csv\"\n",
        "      paths = {\n",
        "          \"trainx\": os.path.join(workdir, f\"trainx{ext}\"),\n",
        "          \"trainy\": os.path.join(workdir, f\"trainy{ext}\"),\n",
        "          \"testx\":  os.path.join(workdir, f\"testx{ext}\"),\n",
        "          \"out\":    os.path.join(workdir, f\"testYhat{ext}\"),\n",
        "          # keep raw (space-delimited) as a backup if you like\n",
        "          \"trainx_raw\": os.path.join(workdir, \"trainx\"),\n",
        "          \"trainy_raw\": os.path.join(workdir, \"trainy\"),\n",
        "          \"testx_raw\":  os.path.join(workdir, \"testx\"),\n",
        "      }\n",
        "\n",
        "      # CSV (no header)\n",
        "      pd.DataFrame(Xtr).to_csv(paths[\"trainx\"], header=False, index=False)\n",
        "      pd.DataFrame(ytr.reshape(-1, 1)).to_csv(paths[\"trainy\"], header=False, index=False)\n",
        "      pd.DataFrame(Xte).to_csv(paths[\"testx\"], header=False, index=False)\n",
        "\n",
        "      # Optional raw (space-delimited) fallback\n",
        "      np.savetxt(paths[\"trainx_raw\"], Xtr, fmt=\"%.10g\")\n",
        "      np.savetxt(paths[\"trainy_raw\"], ytr.reshape(-1, 1), fmt=\"%.10g\")\n",
        "      np.savetxt(paths[\"testx_raw\"],  Xte, fmt=\"%.10g\")\n",
        "\n",
        "      return paths\n",
        "\n",
        "\n",
        "    def _run_binary(self, workdir: str, io_paths: dict):\n",
        "        bin_path = self.bin_path or self._default_bin_path()\n",
        "        if self.verbose:\n",
        "            print(f\"[RBF] running: {bin_path}\\n  cwd: {workdir}\")\n",
        "\n",
        "        self.last_cwd = workdir\n",
        "        cmd = [\n",
        "            bin_path,\n",
        "            \"-n\", str(self.number_of_trees),  # keep your API param\n",
        "            io_paths[\"trainx\"],\n",
        "            io_paths[\"trainy\"],\n",
        "            io_paths[\"testx\"],\n",
        "            io_paths[\"out\"],\n",
        "        ]\n",
        "        proc = subprocess.run(\n",
        "            cmd, cwd=workdir,\n",
        "            stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n",
        "            text=True, check=False,\n",
        "        )\n",
        "        self.last_stdout = proc.stdout or \"\"\n",
        "        self.last_stderr = proc.stderr or \"\"\n",
        "\n",
        "        if self.verbose and self.last_stdout.strip():\n",
        "            print(\"[RBF stdout]\\n\" + self.last_stdout)\n",
        "        if self.verbose and self.last_stderr.strip():\n",
        "            print(\"[RBF stderr]\\n\" + self.last_stderr)\n",
        "\n",
        "        if proc.returncode != 0:\n",
        "            raise RuntimeError(\n",
        "                f\"RBF process failed (code {proc.returncode}).\\ncmd: {' '.join(cmd)}\\n\"\n",
        "                f\"stdout:\\n{self.last_stdout}\\n\\nstderr:\\n{self.last_stderr}\"\n",
        "            )\n",
        "\n",
        "    def _read_output(self, workdir: str, io_paths: Optional[dict] = None) -> np.ndarray:\n",
        "        if io_paths and os.path.exists(io_paths[\"out\"]):\n",
        "            df = pd.read_csv(io_paths[\"out\"], header=None)\n",
        "            return df.iloc[:, 0].to_numpy()\n",
        "\n",
        "        # fallback search (old behavior)\n",
        "        ext = self.temp_extension if self.temp_extension else \"\"\n",
        "        base_names = [\"testYhat\", \"testy\", \"testyhat\"]\n",
        "        candidates = [os.path.join(workdir, b) for b in base_names] + \\\n",
        "                    [os.path.join(workdir, b + ext) for b in base_names]\n",
        "        for p in candidates:\n",
        "            if os.path.exists(p):\n",
        "                df = pd.read_csv(p, header=None)\n",
        "                return df.iloc[:, 0].to_numpy()\n",
        "\n",
        "        raise FileNotFoundError(\n",
        "            \"RBF did not produce a recognizable output file.\\n\"\n",
        "            f\"stdout:\\n{self.last_stdout}\\n\\nstderr:\\n{self.last_stderr}\"\n",
        "        )\n",
        "\n",
        "\n",
        "    # ------------------ temp workspace ------------------ #\n",
        "    def _temp_workspace(self):\n",
        "        class _WS:\n",
        "            def __init__(self, verbose=False):\n",
        "                self._dir = None\n",
        "                self._verbose = verbose\n",
        "            def __enter__(self):\n",
        "                self._dir = tempfile.mkdtemp(prefix=\"rbf_\", suffix=\"_\" + uuid.uuid4().hex)\n",
        "                if self._verbose:\n",
        "                    print(f\"[RBF] temp dir: {self._dir}\")\n",
        "                return self._dir\n",
        "            def __exit__(self, exc_type, exc, tb):\n",
        "                try:\n",
        "                    shutil.rmtree(self._dir, ignore_errors=True)\n",
        "                finally:\n",
        "                    self._dir = None\n",
        "        return _WS(self.verbose)\n"
      ],
      "metadata": {
        "id": "8jbdg_MYCrv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoPB-9qx1n7O"
      },
      "outputs": [],
      "source": [
        "# ------------------ Imports ------------------ #\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cupy as cp\n",
        "import cudf\n",
        "from cuml.ensemble import RandomForestClassifier as cuRF\n",
        "from cuml.linear_model import LogisticRegression as cuLR\n",
        "from cuml.svm import SVC as cuSVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, train_test_split\n",
        "import time\n",
        "\n",
        "\n",
        "# ------------------ Training Function (Before SMOTE) ------------------ #\n",
        "def train_multiple_models(X_train, y_train, X_test, y_test, model_types, random_state=None, n_iter=20):\n",
        "    # Ensure types are GPU-ready\n",
        "    if isinstance(X_train, pd.DataFrame):\n",
        "        X_train = cudf.DataFrame.from_pandas(X_train)\n",
        "    if isinstance(X_test, pd.DataFrame):\n",
        "        X_test = cudf.DataFrame.from_pandas(X_test)\n",
        "    if isinstance(y_train, (np.ndarray, pd.Series)):\n",
        "        y_train = cp.asarray(y_train)\n",
        "    if isinstance(y_test, (np.ndarray, pd.Series)):\n",
        "        y_test = cp.asarray(y_test)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Fill missing values\n",
        "    X_train = X_train.fillna(0)\n",
        "    X_test = X_test.fillna(0)\n",
        "\n",
        "    # Hyperparameters for tuning\n",
        "    param_grids = {\n",
        "        \"xgboost\": {\n",
        "            \"n_estimators\": np.random.randint(50, 150, n_iter).tolist(),\n",
        "            \"learning_rate\": np.random.uniform(0.01, 0.2, n_iter).tolist(),\n",
        "            \"max_depth\": np.random.randint(3, 8, n_iter).tolist(),\n",
        "            \"subsample\": np.random.uniform(0.6, 1.0, n_iter).tolist(),\n",
        "            \"colsample_bytree\": np.random.uniform(0.6, 1.0, n_iter).tolist(),\n",
        "        },\n",
        "        \"mlp\": {\n",
        "            \"hidden_layer_sizes\": [(50,), (100,), (50, 50)],\n",
        "            \"activation\": [\"relu\", \"tanh\"],\n",
        "            \"solver\": [\"adam\", \"sgd\"],\n",
        "            \"alpha\": np.logspace(-4, -2, n_iter).tolist(),\n",
        "            \"learning_rate_init\": np.random.uniform(0.0005, 0.01, n_iter).tolist(),\n",
        "            \"max_iter\": [300, 500]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    for model_type in model_types:\n",
        "        # Initialize models\n",
        "        if model_type == \"rfc\":\n",
        "            model = cuRF(n_estimators=100, max_depth=8, random_state=random_state, n_streams=1)\n",
        "        elif model_type == \"xgboost\":\n",
        "            model = XGBClassifier(\n",
        "                tree_method=\"gpu_hist\",\n",
        "                device=\"cuda\",\n",
        "                predictor=\"gpu_predictor\",\n",
        "                use_label_encoder=False,\n",
        "                eval_metric=\"logloss\",\n",
        "                random_state=random_state\n",
        "            )\n",
        "        elif model_type == \"lr\":\n",
        "            model = cuLR(max_iter=1000, penalty='l2')\n",
        "        elif model_type == \"svm\":\n",
        "            model = cuSVC(probability=True, kernel=\"rbf\", C=1.0)\n",
        "        elif model_type == \"mlp\":\n",
        "            model = MLPClassifier(random_state=random_state)\n",
        "        elif model_type == \"rbf\":\n",
        "            model = RandomBitsForest()\n",
        "        else:\n",
        "            print(f\"Skipping unsupported model type: {model_type}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\nTraining model: {model_type.upper()}...\")\n",
        "        start = time.time()\n",
        "\n",
        "        if model_type in [\"xgboost\", \"mlp\"]:\n",
        "            # Only for MLP (CPU) or XGBoost search\n",
        "            X_train_cpu = X_train.to_pandas()\n",
        "            X_test_cpu = X_test.to_pandas()\n",
        "            y_train_cpu = cp.asnumpy(y_train)\n",
        "\n",
        "            rand_search = RandomizedSearchCV(\n",
        "                model,\n",
        "                param_distributions=param_grids[model_type],\n",
        "                n_iter=n_iter,\n",
        "                cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state),\n",
        "                scoring=\"accuracy\",\n",
        "                n_jobs=-1,\n",
        "                verbose=1,\n",
        "                random_state=random_state\n",
        "            )\n",
        "            rand_search.fit(X_train_cpu, y_train_cpu)\n",
        "            best_model = rand_search.best_estimator_\n",
        "\n",
        "            y_pred = best_model.predict(X_test_cpu)\n",
        "            y_pred_prob = best_model.predict_proba(X_test_cpu)\n",
        "\n",
        "        else:\n",
        "            model.fit(X_train, y_train)\n",
        "            best_model = model\n",
        "            y_pred = model.predict(X_test)\n",
        "            if hasattr(best_model, \"predict_proba\"):\n",
        "                y_pred_prob = model.predict_proba(X_test)\n",
        "            else:\n",
        "                y_pred_prob = None\n",
        "\n",
        "        end = time.time()\n",
        "\n",
        "        # Safe conversion to CPU numpy\n",
        "        y_test_cpu = safe_to_cpu(y_test)\n",
        "        y_pred_cpu = safe_to_cpu(y_pred)\n",
        "        y_pred_prob_cpu = safe_to_cpu(y_pred_prob) if y_pred_prob is not None else None\n",
        "\n",
        "        # Metrics\n",
        "        report = classification_report(y_test_cpu, y_pred_cpu, output_dict=True)\n",
        "        accuracy_num = accuracy_score(y_test_cpu, y_pred_cpu)\n",
        "        roc_auc = roc_auc_score(y_test_cpu, y_pred_prob_cpu[:, 1]) if y_pred_prob_cpu is not None else 0.0\n",
        "        gmean_num = (report[\"0\"][\"recall\"] * report[\"1\"][\"recall\"])**0.5 if \"0\" in report and \"1\" in report else 0.0\n",
        "\n",
        "        precision = report[\"1\"][\"precision\"] if \"1\" in report else 0.0\n",
        "        recall = report[\"1\"][\"recall\"] if \"1\" in report else 0.0\n",
        "        f1 = report[\"1\"][\"f1-score\"] if \"1\" in report else 0.0\n",
        "\n",
        "        results.append({\n",
        "            \"model_type\": model_type,\n",
        "            \"best_model\": best_model,\n",
        "            \"accuracy\": round(accuracy_num, 4),\n",
        "            \"roc_auc\": round(roc_auc, 4),\n",
        "            \"gmean\": round(gmean_num, 4),\n",
        "            \"precision\": round(precision, 4),\n",
        "            \"recall\": round(recall, 4),\n",
        "            \"f1_score\": round(f1, 4),\n",
        "            \"time\": round(end - start, 2),\n",
        "            \"classification_report\": report,\n",
        "        })\n",
        "\n",
        "        print(f\"Accuracy: {accuracy_num:.4f}, ROC-AUC: {roc_auc:.4f}, F1: {f1:.4f}, G-Mean: {gmean_num:.4f}\")\n",
        "        print(f\"Training Time: {end - start:.2f} seconds\")\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kkk9oVRttn_"
      },
      "outputs": [],
      "source": [
        "# Usage example:\n",
        "# DONE: Change RandomForest to rfc -Yash\n",
        "# DONE: Add rbf for Random Bits Forest -Yash\n",
        "# Our rbf page: https://model.earth/realitystream/models/random-bits-forest\n",
        "# Loop through models from the param and train the models\n",
        "\n",
        "model_types_lower = [model.lower() for model in param.models]\n",
        "\n",
        "# Call the training function\n",
        "results_no_smote = train_multiple_models(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    X_test,\n",
        "    y_test,\n",
        "    model_types_lower,\n",
        "    random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "mjDN7r4gPsff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDR3q2Ant7cA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert results into a DataFrame\n",
        "results_no_smote_df = pd.DataFrame([{\n",
        "    \"Model\": r[\"model_type\"],\n",
        "    \"Accuracy\": r[\"accuracy\"],\n",
        "    \"ROC_AUC\": r[\"roc_auc\"],\n",
        "    \"F1_Score\": r[\"f1_score\"],\n",
        "    \"Precision\": r[\"precision\"],\n",
        "    \"Recall\": r[\"recall\"],\n",
        "    \"GMean\": r[\"gmean\"],\n",
        "    \"Training_Time_Seconds\": r[\"time\"]\n",
        "} for r in results_no_smote])\n",
        "\n",
        "# Save as CSV (CPU-side)\n",
        "results_no_smote_df.to_csv(os.path.join(REPORT_FOLDER, \"model_performance_report_no_smote.csv\"), index=False)\n",
        "\n",
        "print(\" Model performance report saved to model_performance_report_no_smote.csv\")\n",
        "print(results_no_smote_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CF6W1dz-xqcd"
      },
      "outputs": [],
      "source": [
        "# ------------------ Imports ------------------ #\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cudf\n",
        "import cupy as cp\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from cuml.ensemble import RandomForestClassifier as cuRF\n",
        "from cuml.linear_model import LogisticRegression as cuLR\n",
        "from cuml.svm import SVC as cuSVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
        "from xgboost import XGBClassifier\n",
        "import time\n",
        "\n",
        "# ------------------ Helper Functions ------------------ #\n",
        "def safe_to_cpu(arr):\n",
        "    if isinstance(arr, cp.ndarray):\n",
        "        return cp.asnumpy(arr)\n",
        "    elif isinstance(arr, (cudf.Series, cudf.DataFrame)):\n",
        "        return arr.to_numpy()\n",
        "    else:\n",
        "        return arr\n",
        "\n",
        "# ------------------ Training Function ------------------ #\n",
        "def train_multiple_models(X_train, y_train, X_test, y_test, model_types, random_state=None, n_iter=20):\n",
        "    if isinstance(X_train, pd.DataFrame):\n",
        "        X_train = cudf.DataFrame.from_pandas(X_train)\n",
        "    if isinstance(X_test, pd.DataFrame):\n",
        "        X_test = cudf.DataFrame.from_pandas(X_test)\n",
        "    if isinstance(y_train, (np.ndarray, pd.Series)):\n",
        "        y_train = cp.asarray(y_train)\n",
        "    if isinstance(y_test, (np.ndarray, pd.Series)):\n",
        "        y_test = cp.asarray(y_test)\n",
        "\n",
        "    results = []\n",
        "    X_train = X_train.fillna(0)\n",
        "    X_test = X_test.fillna(0)\n",
        "\n",
        "    param_grids = {\n",
        "        \"xgboost\": {\n",
        "            \"n_estimators\": np.random.randint(20, 50, n_iter).tolist(),\n",
        "            \"learning_rate\": np.random.uniform(0.01, 0.1, n_iter).tolist(),\n",
        "            \"max_depth\": np.random.randint(2, 4, n_iter).tolist(),\n",
        "            \"min_child_weight\": np.random.randint(5, 10, n_iter).tolist(),\n",
        "            \"subsample\": np.random.uniform(0.5, 0.7, n_iter).tolist(),\n",
        "            \"colsample_bytree\": np.random.uniform(0.5, 0.7, n_iter).tolist(),\n",
        "            \"gamma\": np.random.uniform(0.1, 0.5, n_iter).tolist(),\n",
        "            \"reg_alpha\": np.random.uniform(0.5, 1.5, n_iter).tolist(),\n",
        "            \"reg_lambda\": np.random.uniform(1.0, 3.0, n_iter).tolist(),\n",
        "        },\n",
        "        \"mlp\": {\n",
        "            \"hidden_layer_sizes\": [(50,), (100,), (50, 50)],\n",
        "            \"activation\": [\"relu\", \"tanh\"],\n",
        "            \"solver\": [\"adam\", \"sgd\"],\n",
        "            \"alpha\": np.logspace(-4, -1, n_iter).tolist(),\n",
        "            \"learning_rate_init\": np.random.uniform(0.0001, 0.01, n_iter).tolist(),\n",
        "            \"max_iter\": [300, 500]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    for model_type in model_types:\n",
        "        if model_type == \"rfc\":\n",
        "            model = cuRF(\n",
        "                n_estimators=100,\n",
        "                max_depth=7,\n",
        "                max_features=0.7,\n",
        "                random_state=random_state,\n",
        "                n_streams=1\n",
        "            )\n",
        "        elif model_type == \"xgboost\":\n",
        "            model = XGBClassifier(\n",
        "                tree_method=\"gpu_hist\",\n",
        "                predictor=\"gpu_predictor\",\n",
        "                use_label_encoder=False,\n",
        "                eval_metric=\"logloss\",\n",
        "                random_state=random_state\n",
        "            )\n",
        "        elif model_type == \"lr\":\n",
        "            model = cuLR(max_iter=1000)\n",
        "        elif model_type == \"svm\":\n",
        "            model = cuSVC(probability=True, kernel='rbf', C=10.0, gamma='auto')\n",
        "        elif model_type == \"mlp\":\n",
        "            model = MLPClassifier(random_state=random_state)\n",
        "        else:\n",
        "            print(f\"Skipping unsupported model type: {model_type}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\nTraining model: {model_type.upper()}...\")\n",
        "        start = time.time()\n",
        "\n",
        "        if model_type in [\"xgboost\", \"mlp\"]:\n",
        "            X_train_pd = X_train.to_pandas()\n",
        "            X_test_pd = X_test.to_pandas()\n",
        "            y_train_np = cp.asnumpy(y_train)\n",
        "\n",
        "            rand_search = RandomizedSearchCV(\n",
        "                model,\n",
        "                param_distributions=param_grids[model_type],\n",
        "                n_iter=n_iter,\n",
        "                cv=3,\n",
        "                scoring=\"accuracy\",\n",
        "                n_jobs=-1,\n",
        "                verbose=1,\n",
        "                random_state=random_state\n",
        "            )\n",
        "            rand_search.fit(X_train_pd, y_train_np)\n",
        "            best_model = rand_search.best_estimator_\n",
        "\n",
        "            y_pred = best_model.predict(X_test_pd)\n",
        "            y_pred_prob = best_model.predict_proba(X_test_pd)\n",
        "        else:\n",
        "            model.fit(X_train, y_train)\n",
        "            best_model = model\n",
        "            y_pred = model.predict(X_test)\n",
        "            y_pred_prob = model.predict_proba(X_test) if hasattr(model, \"predict_proba\") else None\n",
        "\n",
        "        end = time.time()\n",
        "\n",
        "        y_test_cpu = safe_to_cpu(y_test)\n",
        "        y_pred_cpu = safe_to_cpu(y_pred)\n",
        "        y_pred_prob_cpu = safe_to_cpu(y_pred_prob) if y_pred_prob is not None else None\n",
        "\n",
        "        report = classification_report(y_test_cpu, y_pred_cpu, output_dict=True, zero_division=0)\n",
        "        accuracy_num = accuracy_score(y_test_cpu, y_pred_cpu)\n",
        "        roc_auc = roc_auc_score(y_test_cpu, y_pred_prob_cpu[:, 1]) if y_pred_prob_cpu is not None else 0.0\n",
        "        gmeans_num = (report[\"0\"][\"recall\"] * report[\"1\"][\"recall\"]) ** 0.5 if \"0\" in report and \"1\" in report else 0.0\n",
        "\n",
        "        precision = report[\"1\"][\"precision\"] if \"1\" in report else 0.0\n",
        "        recall = report[\"1\"][\"recall\"] if \"1\" in report else 0.0\n",
        "        f1 = report[\"1\"][\"f1-score\"] if \"1\" in report else 0.0\n",
        "\n",
        "        results.append({\n",
        "            \"model_type\": model_type,\n",
        "            \"best_model\": best_model,\n",
        "            \"accuracy\": round(accuracy_num, 4),\n",
        "            \"roc_auc\": round(roc_auc, 4),\n",
        "            \"gmean\": round(gmeans_num, 4),\n",
        "            \"precision\": round(precision, 4),\n",
        "            \"recall\": round(recall, 4),\n",
        "            \"f1_score\": round(f1, 4),\n",
        "            \"time\": round(end - start, 2),\n",
        "            \"classification_report\": report,\n",
        "        })\n",
        "\n",
        "        print(f\"Accuracy: {accuracy_num:.4f}, ROC-AUC: {roc_auc:.4f}, F1-Score: {f1:.4f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# ------------------ Main Execution ------------------ #\n",
        "\n",
        "# Step 1: Split CPU Data\n",
        "X_train_pd, X_val_pd, y_train_np, y_val_np = train_test_split(\n",
        "    X_total_cpu.fillna(0),\n",
        "    y_total_cpu,\n",
        "    test_size=0.2,\n",
        "    stratify=y_total_cpu,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Keep only numeric columns\n",
        "X_train_pd = X_train_pd.select_dtypes(include=[np.number])\n",
        "X_val_pd = X_val_pd.select_dtypes(include=[np.number])\n",
        "\n",
        "# Step 3: Apply SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_smote_pd, y_train_smote_np = smote.fit_resample(X_train_pd, y_train_np)\n",
        "\n",
        "# Step 4: Convert to GPU\n",
        "X_train_smote = cudf.DataFrame.from_pandas(X_train_smote_pd)\n",
        "y_train_smote = cp.asarray(y_train_smote_np)\n",
        "X_val = cudf.DataFrame.from_pandas(X_val_pd)\n",
        "y_val = cp.asarray(y_val_np)\n",
        "\n",
        "print(f\"After SMOTE: X_train_smote {X_train_smote.shape}, X_val {X_val.shape}\")\n",
        "\n",
        "# Step 5: Train Models\n",
        "results_smote = train_multiple_models(\n",
        "    X_train=X_train_smote,\n",
        "    y_train=y_train_smote,\n",
        "    X_test=X_val,\n",
        "    y_test=y_val,\n",
        "    model_types=['rfc', 'xgboost', 'lr', 'mlp', 'svm'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 6: Save report\n",
        "results_smote_df = pd.DataFrame([{\n",
        "    \"Model\": r[\"model_type\"],\n",
        "    \"Accuracy\": r[\"accuracy\"],\n",
        "    \"ROC_AUC\": r[\"roc_auc\"],\n",
        "    \"F1_Score\": r[\"f1_score\"],\n",
        "    \"Precision\": r[\"precision\"],\n",
        "    \"Recall\": r[\"recall\"],\n",
        "    \"GMean\": r[\"gmean\"],\n",
        "    \"Training_Time_Seconds\": r[\"time\"]\n",
        "} for r in results_smote])\n",
        "\n",
        "results_smote_df.to_csv(os.path.join(REPORT_FOLDER, \"model_performance_report_smote.csv\"), index=False)\n",
        "print(\"Report saved to model_performance_report_smote.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otyID14G1CPw"
      },
      "source": [
        "# Extracting Feature Importance\n",
        "\n",
        "Below code extracts feature importance from trained models (RandomForest, XGBoost), sorts the values, and stores them in a dictionary for further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzWwiwBAZ4mR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cupy as cp\n",
        "import cudf\n",
        "\n",
        "# Helper function to safely move data to CPU\n",
        "def safe_to_cpu(arr):\n",
        "    if isinstance(arr, cp.ndarray):\n",
        "        return cp.asnumpy(arr)\n",
        "    elif isinstance(arr, (cudf.Series, cudf.DataFrame)):\n",
        "        return arr.to_pandas()\n",
        "    else:\n",
        "        return arr\n",
        "\n",
        "# Get feature names (ensure it's from the same source as training)\n",
        "feature_names = safe_to_cpu(X_train_smote).columns.tolist()\n",
        "\n",
        "# Loop through trained models and extract importance for XGBoost\n",
        "for result in results_smote:\n",
        "    model_type = result[\"model_type\"]\n",
        "    model = result[\"best_model\"]\n",
        "\n",
        "    if model_type == \"xgboost\":\n",
        "        print(\"Extracting feature importance for XGBoost...\")\n",
        "\n",
        "        # Get importance scores from booster\n",
        "        booster = model.get_booster()\n",
        "        importance_dict = booster.get_score(importance_type=\"weight\")\n",
        "\n",
        "        # Map importance to all features (0 if not present)\n",
        "        importance_values = np.array([importance_dict.get(f, 0) for f in feature_names])\n",
        "\n",
        "        # Create DataFrame\n",
        "        feature_importance_df = pd.DataFrame({\n",
        "            \"Feature\": feature_names,\n",
        "            \"Importance\": importance_values\n",
        "        }).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "        # Save to CSV\n",
        "        feature_importance_df.to_csv(os.path.join(REPORT_FOLDER, \"feature_importance_xgboost.csv\"), index=False)\n",
        "        print(\"Saved: feature_importance_xgboost.csv\")\n",
        "\n",
        "        # Optional: Plot top 20 features (future-proofed)\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.barplot(\n",
        "            data=feature_importance_df.head(20),\n",
        "            x=\"Importance\",\n",
        "            y=\"Feature\",\n",
        "            hue=\"Feature\",         # explicitly assign hue\n",
        "            dodge=False,           # avoid bar separation\n",
        "            legend=False,          # no redundant legend\n",
        "            palette=\"viridis\"\n",
        "        )\n",
        "        plt.title(\"Top 20 Feature Importances (XGBoost)\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKPhev8mZ5l1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cupy as cp\n",
        "import cudf\n",
        "\n",
        "# ------------------ Helper Function ------------------ #\n",
        "def safe_to_cpu(arr):\n",
        "    \"\"\"Safely move GPU data (CuPy/cuDF) to CPU (NumPy/Pandas).\"\"\"\n",
        "    if isinstance(arr, cp.ndarray):\n",
        "        return cp.asnumpy(arr)\n",
        "    elif isinstance(arr, (cudf.Series, cudf.DataFrame)):\n",
        "        return arr.to_pandas()\n",
        "    else:\n",
        "        return arr\n",
        "\n",
        "# ------------------ Feature Importance Plot and Save ------------------ #\n",
        "def plot_and_save_feature_importance(feature_importance_df, model_name, top_n=20, save_dir=\"/content/feature_importance\"):\n",
        "    \"\"\"\n",
        "    Plot and save top N feature importances as PNG.\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Sort and select top N\n",
        "    top_features = feature_importance_df.sort_values(by=\"Importance\", ascending=False).head(top_n)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(\n",
        "        data=top_features,\n",
        "        x=\"Importance\",\n",
        "        y=\"Feature\",\n",
        "        hue=\"Feature\",     # needed to suppress seaborn warnings\n",
        "        dodge=False,\n",
        "        legend=False,\n",
        "        palette=\"viridis\"\n",
        "    )\n",
        "    plt.title(f\"Top {top_n} Feature Importances - {model_name.upper()}\")\n",
        "    plt.xlabel(\"Importance\")\n",
        "    plt.ylabel(\"Feature\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save\n",
        "    plot_path = os.path.join(save_dir, f\"feature_importance_{model_name.lower()}.png\")\n",
        "    plt.savefig(plot_path)\n",
        "    plt.show()\n",
        "    print(f\"Plot saved to: {plot_path}\")\n",
        "\n",
        "# ------------------ Feature Importance Extraction ------------------ #\n",
        "# Get feature names from training data\n",
        "feature_names = safe_to_cpu(X_train_smote).columns.tolist()\n",
        "\n",
        "# Loop through results and process only XGBoost\n",
        "for result in results_smote:\n",
        "    model_type = result[\"model_type\"]\n",
        "    model = result[\"best_model\"]\n",
        "\n",
        "    if model_type == \"xgboost\":\n",
        "        print(\"Extracting feature importance for XGBoost...\")\n",
        "\n",
        "        # Get booster importance dictionary\n",
        "        booster = model.get_booster()\n",
        "        importance_dict = booster.get_score(importance_type=\"weight\")\n",
        "\n",
        "        # Map importance to all features (0 if not present)\n",
        "        importance_values = np.array([importance_dict.get(f, 0) for f in feature_names])\n",
        "\n",
        "        # Create DataFrame\n",
        "        feature_importance_df = pd.DataFrame({\n",
        "            \"Feature\": feature_names,\n",
        "            \"Importance\": importance_values\n",
        "        })\n",
        "\n",
        "        # Save CSV\n",
        "        feature_importance_df.to_csv(os.path.join(REPORT_FOLDER, \"feature_importance_xgboost.csv\"), index=False)\n",
        "        print(\"Saved: feature_importance_xgboost.csv\")\n",
        "\n",
        "        # Plot and save PNG\n",
        "        plot_and_save_feature_importance(\n",
        "            feature_importance_df,\n",
        "            model_name=\"xgboost\",\n",
        "            top_n=20\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgRKHELLZ7z0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# Ensure output directory exists\n",
        "def ensure_dir(path):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    return path\n",
        "\n",
        "# Safely move GPU data to CPU\n",
        "def safe_to_cpu(arr):\n",
        "    import cupy as cp\n",
        "    import cudf\n",
        "    if isinstance(arr, cp.ndarray):\n",
        "        return cp.asnumpy(arr)\n",
        "    elif isinstance(arr, (cudf.DataFrame, cudf.Series)):\n",
        "        return arr.to_pandas()\n",
        "    return arr\n",
        "\n",
        "# Plot and save top N feature importances\n",
        "def plot_and_save_feature_importance(df, model_name, top_n=20, save_dir=REPORT_FOLDER):\n",
        "    ensure_dir(save_dir)\n",
        "    df_top = df.head(top_n)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(data=df_top, x=\"Importance\", y=\"Feature\", palette=\"viridis\")\n",
        "    plt.title(f\"Top {top_n} Feature Importances ({model_name.upper()})\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plot_path = os.path.join(save_dir, f\"permutation_importance_{model_name}.png\")\n",
        "    plt.savefig(plot_path)\n",
        "    plt.show()\n",
        "\n",
        "# Compute permutation importance\n",
        "def compute_permutation_importance(model, X_val, y_val, model_name, top_n=20, save_dir=REPORT_FOLDER):\n",
        "    print(f\"\\nComputing permutation importance for: {model_name.upper()}\")\n",
        "\n",
        "    X_val_cpu = safe_to_cpu(X_val)\n",
        "    y_val_cpu = safe_to_cpu(y_val)\n",
        "\n",
        "    # Validate feature consistency\n",
        "    if hasattr(model, \"feature_names_in_\"):\n",
        "        expected_features = model.feature_names_in_\n",
        "        X_val_cpu = X_val_cpu[expected_features]\n",
        "\n",
        "    result = permutation_importance(model, X_val_cpu, y_val_cpu, scoring=\"accuracy\", n_repeats=10, random_state=42)\n",
        "\n",
        "    feature_names = X_val_cpu.columns.tolist()\n",
        "    importance_df = pd.DataFrame({\n",
        "        \"Feature\": feature_names,\n",
        "        \"Importance\": result.importances_mean\n",
        "    }).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "    # Save CSV\n",
        "    ensure_dir(save_dir)\n",
        "    csv_path = os.path.join(save_dir, f\"permutation_importance_{model_name}.csv\")\n",
        "    importance_df.to_csv(csv_path, index=False)\n",
        "    print(f\"Saved: {csv_path}\")\n",
        "\n",
        "    # Plot and save\n",
        "    plot_and_save_feature_importance(importance_df, model_name=model_name, top_n=top_n, save_dir=save_dir)\n",
        "\n",
        "# Only runs if mlp is in param['models']\n",
        "cpu_safe_models = [\"mlp\"]  # expand this if you trained others with sklearn\n",
        "\n",
        "for result in results_smote:\n",
        "    #model_type = result[\"model_type\"] # TODO - These might be all the models. Switch to just param['models']\n",
        "    model_type = param.models\n",
        "    model = result[\"best_model\"]\n",
        "\n",
        "    if model_type in cpu_safe_models:\n",
        "        compute_permutation_importance(\n",
        "            model=model,\n",
        "            X_val=X_val,  # full column set\n",
        "            y_val=y_val,\n",
        "            model_name=model_type,\n",
        "            top_n=20\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ukfmcqf1QgC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Initialize a dictionary to store Feature Importance for different models\n",
        "feature_importance_dict = {}\n",
        "\n",
        "# Iterate through trained models\n",
        "for result in results_no_smote:\n",
        "    model_type = result[\"model_type\"]\n",
        "    model = result[\"best_model\"]\n",
        "\n",
        "    # Ensure the model supports Feature Importance\n",
        "    # TO DO: Should feature_importance be calculated for RBF? An API does not exist.\n",
        "    if model_type in [\"rfc\"]:\n",
        "        feature_importance = model.feature_importances_\n",
        "\n",
        "    elif model_type == \"xgboost\":\n",
        "        importance_dict = model.get_booster().get_score(importance_type=\"weight\")\n",
        "        feature_importance = np.array([importance_dict.get(f, 0) for f in X_train.columns])\n",
        "\n",
        "    elif model_type == \"lr\":\n",
        "        feature_importance = np.abs(model.coef_[0])\n",
        "\n",
        "    elif model_type in [\"svm\", \"mlp\"]:\n",
        "        print(f\"Feature importance not directly supported for {model_type}. Consider using permutation importance or SHAP.\")\n",
        "        continue\n",
        "\n",
        "    else:\n",
        "        print(f\"Skipping unsupported model type: {model_type}\")\n",
        "        continue\n",
        "\n",
        "    # Store Feature Importance in a DataFrame\n",
        "    feature_importance_df = pd.DataFrame({\n",
        "        \"Feature\": X_train.columns,\n",
        "        \"Importance\": feature_importance\n",
        "    }).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "    # Save the Feature Importance DataFrame in the dictionary\n",
        "    feature_importance_dict[model_type] = feature_importance_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuzGygg4aseX"
      },
      "outputs": [],
      "source": [
        "###Xucen Liao 04/20 - retraining Random Forest, XGboost, and LR based on top 10 important features.\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "def retrain_top_10_models(X_train, y_train, X_test, y_test, feature_importance_dict):\n",
        "    models = {\n",
        "        'rfc': RandomForestClassifier(random_state=42),\n",
        "        'xgboost': XGBClassifier(eval_metric='logloss', tree_method='hist', enable_categorical=False, random_state=42),\n",
        "        'lr': LogisticRegression(max_iter=200, solver='liblinear', random_state=42)\n",
        "    }\n",
        "\n",
        "    retrained_results = {}\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        if model_name not in feature_importance_dict:\n",
        "            print(f\"Skipping {model_name} as it's not in the feature importance dictionary.\")\n",
        "            continue\n",
        "        print(f\"\\n--- Retraining {model_name} with Top 10 Features ---\")\n",
        "        # Get top 10 features\n",
        "        top_features = feature_importance_dict[model_name].sort_values(by='Importance', ascending=False)['Feature'].head(10).tolist()\n",
        "\n",
        "        # Subset data\n",
        "        X_train_subset = X_train[top_features]\n",
        "        X_test_subset = X_test[top_features]\n",
        "\n",
        "        # Fit and evaluate\n",
        "        model.fit(X_train_subset, y_train)\n",
        "        y_pred = model.predict(X_test_subset)\n",
        "        y_proba = model.predict_proba(X_test_subset)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
        "\n",
        "        y_test_numpy_ndarray = y_test.get() if isinstance(y_test, cp.ndarray) else y_test\n",
        "        accuracy = accuracy_score(y_test_numpy_ndarray, y_pred)\n",
        "        roc = roc_auc_score(y_test_numpy_ndarray, y_proba) if y_proba is not None else 0.0\n",
        "        report = classification_report(y_test_numpy_ndarray, y_pred, output_dict=True)\n",
        "        f1 = report['1']['f1-score'] if '1' in report else 0.0\n",
        "\n",
        "        print(f\"Top 10 Features: {top_features}\")\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"ROC-AUC: {roc:.4f}\")\n",
        "        print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "        retrained_results[model_name] = {\n",
        "            'model': model,\n",
        "            'top_features': top_features,\n",
        "            'accuracy': accuracy,\n",
        "            'roc_auc': roc,\n",
        "            'f1_score': f1,\n",
        "            'classification_report': report\n",
        "        }\n",
        "\n",
        "    return retrained_results\n",
        "results_top_10 = retrain_top_10_models(X_train, y_train, X_test, y_test, feature_importance_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iximWkt1r-W"
      },
      "source": [
        "**Plot and Save Feature Importance**\n",
        "\n",
        "This function plots and saves the top N most important features from trained models (RandomForest, XGBoost).\n",
        "\n",
        "**Key Steps:**\n",
        "- Ensure the save directory exists (/content/feature_importance).\n",
        "- Sort features by importance in descending order.\n",
        "- Plot feature importance using a bar chart.\n",
        "- Save the plot as a PNG file in the specified directory.\n",
        "- Display the plot after saving.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCTOw2Az2BYt"
      },
      "outputs": [],
      "source": [
        "def plot_feature_importance(feature_importance_df, model_name, top_n=10, save_dir=\"/content/feature_importance\"):\n",
        "    \"\"\"\n",
        "    Plot and save the top `top_n` most important features.\n",
        "\n",
        "    Args:\n",
        "        feature_importance_df (pd.DataFrame): DataFrame containing `Feature` and `Importance` columns.\n",
        "        model_name (str): Name of the model (used in the title and filename).\n",
        "        top_n (int): Number of top features to display.\n",
        "        save_dir (str): Directory where the figure should be saved.\n",
        "    \"\"\"\n",
        "    # Ensure directory exists\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Sort the features by importance in descending order\n",
        "    feature_importance_df = feature_importance_df.sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "    # Create the bar plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x=\"Importance\", y=\"Feature\", data=feature_importance_df[:top_n], palette=\"Blues_r\", errorbar=None)\n",
        "\n",
        "    # Set labels and title\n",
        "    plt.xlabel(\"Importance Score\")\n",
        "    plt.ylabel(\"Feature\")\n",
        "    plt.title(f\"Top {top_n} Feature Importances ({model_type})\")\n",
        "\n",
        "    # Save the figure to the specified directory\n",
        "    file_path = os.path.join(save_dir, f\"feature_importance_{model_type}.png\")\n",
        "    plt.savefig(file_path, bbox_inches=\"tight\", dpi=300)\n",
        "    print(f\"Saved feature importance plot for {model_type} at: {file_path}\")\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgUt-wHvPmUW"
      },
      "outputs": [],
      "source": [
        "# Display feature importance\n",
        "# TODO(Done): get the feature importance of the parameters from the models specified in parameters.yaml file\n",
        "for result in results_no_smote:\n",
        "    model_type = result[\"model_type\"]\n",
        "    if model_type in feature_importance_dict:\n",
        "      model = result[\"best_model\"]\n",
        "      plot_feature_importance(feature_importance_dict[model_type], model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7D2sKux-2bGa"
      },
      "source": [
        "**Mapping NAICS6 Codes to Industry Names & Updating Feature Importance**\n",
        "\n",
        "This section retrieves NAICS6 industry classifications, maps feature names (Emp-XXXXXX) to their corresponding industry names, and updates the feature importance reports accordingly.\n",
        "\n",
        "**Key Steps:**\n",
        "\n",
        "1. Load NAICS6 Data\n",
        "   - Reads the 2017 NAICS6 codes from an Excel file.\n",
        "   - Converts them into a dictionary for fast lookups.\n",
        "\n",
        "2. Map Features to Industry Names\n",
        "   - Extracts NAICS6 codes from feature names (Emp-XXXXXX).\n",
        "   - Replaces them with formatted \"NAICS6Code-IndustryName\" strings.\n",
        "\n",
        "3. Update Feature Importance Reports\n",
        "   - Applies mapping only if the features path contains \"naics\".\n",
        "   - Updates the feature names in feature_importance_dict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MeMmdwH2b00"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the URL to the NAICS6 classification Excel file\n",
        "naics6_url = \"https://github.com/ModelEarth/concordance/raw/master/data-raw/6-digit_2017_Codes.xlsx\"\n",
        "\n",
        "# Read the Excel file, skipping the first row, and selecting only the relevant columns\n",
        "naics6_df = pd.read_excel(naics6_url, dtype=str, skiprows=1, usecols=[0, 1])\n",
        "\n",
        "# Rename columns for clarity\n",
        "naics6_df.columns = [\"NAICS6_Code\", \"Industry_Name\"]\n",
        "\n",
        "# Convert the DataFrame into a dictionary for quick lookups\n",
        "naics6_mapping = naics6_df.set_index(\"NAICS6_Code\")[\"Industry_Name\"].to_dict()\n",
        "\n",
        "# Print the first few rows to verify the cleanup\n",
        "print(naics6_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yP50FcIh2fxD"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def map_emp_to_sector(feature_name):\n",
        "    \"\"\"\n",
        "    Replace `Emp-XXXXXX` with the corresponding NAICS6 industry name.\n",
        "\n",
        "    Example:\n",
        "        Emp-454310 -> 454310-Retail Trade\n",
        "        Emp-221310 -> 221310-Water Supply and Irrigation Systems\n",
        "        Latitude   -> Latitude (unchanged)\n",
        "\n",
        "    Args:\n",
        "        feature_name (str): The original feature name.\n",
        "\n",
        "    Returns:\n",
        "        str: The formatted \"NAICS6Code-IndustryName\" if found, otherwise the original feature name.\n",
        "    \"\"\"\n",
        "    match = re.match(r\"Emp-(\\d{6})\", feature_name)  # Match pattern 'Emp-XXXXXX'\n",
        "    if match:\n",
        "        naics_code = match.group(1)  # Extract full NAICS6 code\n",
        "        industry_name = naics6_mapping.get(naics_code, \"Unknown\")  # Look up NAICS6 industry name\n",
        "        return f\"{naics_code}-{industry_name}\"  # Return \"NAICS6Code-IndustryName\"\n",
        "\n",
        "    return feature_name  # Return the original name if no match\n",
        "\n",
        "# **Test cases**\n",
        "print(map_emp_to_sector(\"Emp-454310\"))  # Expected: \"454310-Fuel Dealers\"\n",
        "print(map_emp_to_sector(\"Emp-221310\"))  # Expected: \"221310-Water Supply and Irrigation Systems\"\n",
        "print(map_emp_to_sector(\"Latitude\"))    # Expected: \"Latitude\" (unchanged)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdUbd10E2jvV"
      },
      "outputs": [],
      "source": [
        "# Ensure mapping only happens if features.path contains \"naics2\"\n",
        "if \"naics\" in param.features.path:\n",
        "    for model_name in feature_importance_dict:\n",
        "        feature_importance_dict[model_name] = feature_importance_dict[model_name].copy()\n",
        "        feature_importance_dict[model_name][\"Feature\"] = feature_importance_dict[model_name][\"Feature\"].apply(map_emp_to_sector)\n",
        "\n",
        "# Display the first few rows of the updated feature importance for each model\n",
        "for model_name, importance_df in feature_importance_dict.items():\n",
        "    print(f\"\\nFeature Importance for {model_name}:\")\n",
        "    print(importance_df.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBf_XUzv6QPl"
      },
      "outputs": [],
      "source": [
        "feature_importance_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMTPlt6v2mXo"
      },
      "outputs": [],
      "source": [
        "# TODO - Send to repo in last step\n",
        "for result in results_no_smote:\n",
        "    model_type = result[\"model_type\"]\n",
        "    if model_type in feature_importance_dict:\n",
        "      model = result[\"best_model\"]\n",
        "      plot_feature_importance(feature_importance_dict[model_type], model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHJPag4N6h39"
      },
      "source": [
        "#Unified Aggregation Results & Helper Functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwjxEOya6wQi"
      },
      "source": [
        "Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9ocvA6E6j-o"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import cudf\n",
        "\n",
        "def get_original_column(mapped_name):\n",
        "    '''\n",
        "    Given a mapped feature name (e.g., \"562111-Solid Waste Collection\"),\n",
        "    extract the first six digits and prepend 'Emp-' to form the original column name.\n",
        "    If no six-digit code is found, return the mapped name.\n",
        "    '''\n",
        "    match = re.match(r\"(\\d{6})\", mapped_name)\n",
        "    if match:\n",
        "        return f\"Emp-{match.group(1)}\"\n",
        "    else:\n",
        "        return mapped_name\n",
        "\n",
        "\n",
        "def aggregate_model_results(results, feature_importance_dict=None, show_best_threshold=True):\n",
        "    \"\"\"\n",
        "    Aggregate and display model results with optional feature importances.\n",
        "\n",
        "    This function supports both full names (e.g. \"RandomForest\", \"XGBoost\")\n",
        "    and abbreviated model types (e.g. \"rfc\", \"xgboost\", \"rbf\", etc.).\n",
        "\n",
        "    Args:\n",
        "        results (list): List of model result dictionaries from training runs.\n",
        "        feature_importance_dict (dict): Dictionary of model_type -> feature importance DataFrames.\n",
        "        show_best_threshold (bool): Whether to include best threshold in the aggregated results.\n",
        "\n",
        "    Returns:\n",
        "        dict: A unified dictionary of aggregated results.\n",
        "    \"\"\"\n",
        "    modelResults = {}\n",
        "\n",
        "    # Use explicit mapping for both full and abbreviated names\n",
        "    for result in results:\n",
        "        # Get raw model type and convert to lower-case for comparisons\n",
        "        raw_model_type = result[\"model_type\"].strip()\n",
        "        model_type_lower = raw_model_type.lower()\n",
        "\n",
        "        if model_type_lower in [\"randomforest\", \"rfc\"]:\n",
        "            key = \"rfc\"\n",
        "            model_title = \"Random Forest Classifier\"\n",
        "        elif model_type_lower in [\"xgboost\"]:\n",
        "            key = \"xgboost\"\n",
        "            model_title = \"XGBoost\"\n",
        "        elif model_type_lower in [\"rbf\"]:\n",
        "            key = \"rbf\"\n",
        "            model_title = \"Random Bits Forest\"\n",
        "        elif model_type_lower in [\"lr\"]:\n",
        "            key = \"lr\"\n",
        "            model_title = \"Logistic Regression\"\n",
        "        elif model_type_lower in [\"svm\"]:\n",
        "            key = \"svm\"\n",
        "            model_title = \"Support Vector Machine\"\n",
        "        elif model_type_lower in [\"mlp\"]:\n",
        "            key = \"mlp\"\n",
        "            model_title = \"Multi-Layer Perceptron\"\n",
        "        else:\n",
        "            key = model_type_lower\n",
        "            model_title = raw_model_type.title()\n",
        "\n",
        "        # Gather the metrics from the result\n",
        "        accuracy = result.get(\"accuracy\")\n",
        "        roc_auc = result.get(\"roc_auc\")\n",
        "        gmean = result.get(\"gmean\")\n",
        "        classification_report = result.get(\"classification_report\")\n",
        "\n",
        "        runtime_seconds = result.get(\"runtime_seconds\",None) # Tarun , to pull runtime seconds for each dict\n",
        "\n",
        "        entry = {\n",
        "            \"title\": model_title,\n",
        "            \"accuracy\": accuracy,\n",
        "            \"roc_auc\": roc_auc,\n",
        "            \"gmean\": gmean,\n",
        "            \"classification_report\": classification_report,\n",
        "            \"runtime_seconds\": runtime_seconds,\n",
        "        }\n",
        "        if show_best_threshold:\n",
        "            entry[\"best_threshold\"] = result.get(\"best_threshold\")\n",
        "        if feature_importance_dict and key in feature_importance_dict:\n",
        "            # Get the top 10 feature importances (as list of records)\n",
        "            entry[\"top_importances\"] = feature_importance_dict[key].head(10).to_dict(orient=\"records\")\n",
        "        else:\n",
        "            entry[\"top_importances\"] = None\n",
        "\n",
        "        modelResults[key] = entry\n",
        "\n",
        "    # Create a summary table for the main evaluation metrics\n",
        "    summary_rows = []\n",
        "    for key, result in modelResults.items():\n",
        "        row = {\n",
        "            \"Model Key\": key,\n",
        "            \"Title\": result[\"title\"],\n",
        "            \"Accuracy\": result[\"accuracy\"],\n",
        "            \"ROC-AUC\": result[\"roc_auc\"],\n",
        "            \"G-Mean\": result[\"gmean\"],\n",
        "            \"Runtime (s)\": result.get(\"runtime_seconds\") # Tarun , to display runtime in summary table.\n",
        "        }\n",
        "        if show_best_threshold:\n",
        "            row[\"Best Threshold\"] = result.get(\"best_threshold\")\n",
        "        summary_rows.append(row)\n",
        "    summary_df = pd.DataFrame(summary_rows)\n",
        "    print(\"Unified Model Results Summary:\")\n",
        "    print(tabulate(summary_df, headers=\"keys\", tablefmt=\"pipe\", showindex=False))\n",
        "\n",
        "    # For each model, display an enhanced table for the top 10 feature importances.\n",
        "    # This section augments the stored top importances with correlation information and prefix labels.\n",
        "    for key, result in modelResults.items():\n",
        "        top_importances = result.get(\"top_importances\")\n",
        "        if top_importances:\n",
        "            fi_df = pd.DataFrame(top_importances)\n",
        "\n",
        "            # Prepare lists to store prefix, correlation values, and correlation sign.\n",
        "            prefixes = []\n",
        "            correlations = []\n",
        "            signs = []\n",
        "            for mapped_feature in fi_df[\"Feature\"]:\n",
        "                # Use your already working helper function to get the original feature name.\n",
        "                original_feature = get_original_column(mapped_feature)\n",
        "                if original_feature in X_train.columns:\n",
        "                    prefix = original_feature.split(\"-\")[0]  # e.g., 'Emp', 'Pay', or 'Est'\n",
        "                    corr = X_train[original_feature].corr(cudf.Series(y_train))\n",
        "                    correlations.append(round(corr, 3))\n",
        "                    if corr > 0:\n",
        "                        signs.append(\"Positive\")\n",
        "                    elif corr < 0:\n",
        "                        signs.append(\"Negative\")\n",
        "                    else:\n",
        "                        signs.append(\"Zero\")\n",
        "                else:\n",
        "                    prefix = \"N/A\"\n",
        "                    correlations.append(\"N/A\")\n",
        "                    signs.append(\"N/A\")\n",
        "                prefixes.append(prefix)\n",
        "\n",
        "            # Append the new information to the DataFrame.\n",
        "            fi_df[\"Prefix\"] = prefixes\n",
        "            fi_df[\"Correlation\"] = correlations\n",
        "            fi_df[\"Correlation Sign\"] = signs\n",
        "\n",
        "            print(f\"\\nTop 10 Feature Importances for {result['title']} ({key}):\")\n",
        "            print(tabulate(fi_df, headers=\"keys\", tablefmt=\"pipe\", showindex=False))\n",
        "    return modelResults\n",
        "def plot_correlation_charts(modelResults, X_train, y_train):\n",
        "    \"\"\"\n",
        "    For each model in the aggregated results (modelResults), this function plots\n",
        "    a horizontal bar chart showing the Pearson correlations of the top features\n",
        "    with the target. Bars are colored green for positive correlations and salmon\n",
        "    for negative correlations.\n",
        "\n",
        "    Args:\n",
        "        modelResults (dict): Aggregated model results containing a key \"top_importances\"\n",
        "                              for each model (list of dictionaries).\n",
        "        X_train (pd.DataFrame): The training features.\n",
        "        y_train (pd.Series): The target values corresponding to the training features.\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import pandas as pd\n",
        "\n",
        "    for model_key, result in modelResults.items():\n",
        "        top_importances = result.get(\"top_importances\")\n",
        "        if top_importances:\n",
        "            # Convert the stored list of top importances into a DataFrame.\n",
        "            fi_df = pd.DataFrame(top_importances)\n",
        "\n",
        "            # If the correlation info isn't present, compute and add it.\n",
        "            if (\"Correlation\" not in fi_df.columns or\n",
        "                \"Prefix\" not in fi_df.columns or\n",
        "                \"Correlation Sign\" not in fi_df.columns):\n",
        "\n",
        "                prefixes = []\n",
        "                correlations = []\n",
        "                signs = []\n",
        "                for mapped_feature in fi_df[\"Feature\"]:\n",
        "                    # Use your helper function to get the original feature name.\n",
        "                    original_feature = get_original_column(mapped_feature)\n",
        "                    if original_feature in X_train.columns:\n",
        "                        # Extract prefix (e.g., \"Emp\", \"Pay\", \"Est\")\n",
        "                        prefix = original_feature.split(\"-\")[0]\n",
        "                        corr = X_train[original_feature].corr(cudf.Series(y_train))\n",
        "                        correlations.append(round(corr, 3))\n",
        "                        if corr > 0:\n",
        "                            signs.append(\"Positive\")\n",
        "                        elif corr < 0:\n",
        "                            signs.append(\"Negative\")\n",
        "                        else:\n",
        "                            signs.append(\"Zero\")\n",
        "                    else:\n",
        "                        prefix = \"N/A\"\n",
        "                        correlations.append(\"N/A\")\n",
        "                        signs.append(\"N/A\")\n",
        "                    prefixes.append(prefix)\n",
        "                # Append computed columns.\n",
        "                fi_df[\"Prefix\"] = prefixes\n",
        "                fi_df[\"Correlation\"] = correlations\n",
        "                fi_df[\"Correlation Sign\"] = signs\n",
        "\n",
        "            # Filter out rows with non-numeric correlation values.\n",
        "            fi_numeric = fi_df[fi_df[\"Correlation\"] != \"N/A\"].copy()\n",
        "            fi_numeric[\"Correlation\"] = pd.to_numeric(fi_numeric[\"Correlation\"])\n",
        "\n",
        "            # Create a label for each feature by combining its name and prefix.\n",
        "            fi_numeric[\"Feature_Label\"] = fi_numeric[\"Feature\"] + \" (\" + fi_numeric[\"Prefix\"] + \")\"\n",
        "\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            # Color bars: green for positive values, salmon for negatives.\n",
        "            colors = fi_numeric[\"Correlation\"].apply(lambda x: \"green\" if x > 0 else \"salmon\")\n",
        "            plt.barh(fi_numeric[\"Feature_Label\"], fi_numeric[\"Correlation\"], color=colors)\n",
        "            plt.xlabel(\"Pearson Correlation\")\n",
        "            plt.title(f\"Correlation of Top Features with Target for {result['title']}\")\n",
        "            plt.axvline(0, color=\"black\", linewidth=0.8)\n",
        "\n",
        "            # Move y-axis tick labels to the right.\n",
        "            ax = plt.gca()\n",
        "            ax.yaxis.tick_right()\n",
        "            ax.yaxis.set_label_position(\"right\")\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOj4ThlV62uL"
      },
      "outputs": [],
      "source": [
        "modelResults = aggregate_model_results(results_no_smote, feature_importance_dict, show_best_threshold=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDjUeglA69bg"
      },
      "outputs": [],
      "source": [
        "plot_correlation_charts(modelResults, X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "73SWD6cmU5tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUpEuxERyBjH"
      },
      "source": [
        "# Upload to Github"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLlz3Ts7brgf"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "# NOTE: Github tokens have been expiring monthly, even when set to never expire.\n",
        "# Using 90-day instead of never expires (which was expiring monthly)\n",
        "# Expires on Mon, Sep 15 2025.\n",
        "DEFAULT_REPO = \"modelearth/reports\"\n",
        "DEFAULT_TOKEN = \"\"\n",
        "# Comment above and uncomment below before committing this to Github as Github won't allow the token value to be pushed.\n",
        "# DEFAULT_TOKEN = \"[GITHUB_TOKEN]\"\n",
        "\n",
        "# The following chunk is an effort to run only this last step.\n",
        "# Also edit these lines in prior step. Maybe move settings here.\n",
        "# TO DO: Avoid saving custom folder name in left side reports.\n",
        "# TO DO: Send a test file if left side reports are not there.\n",
        "GITHUB_YEAR = \"2025\"\n",
        "\n",
        "GITHUB_SUBFOLDER = datetime.now().strftime(\"run-%Y-%m-%dT%H-%M-%S\")\n",
        "FULL_REPORT_PATH = os.path.join(GITHUB_YEAR, GITHUB_SUBFOLDER)\n",
        "\n",
        "\n",
        "def get_file_sha(remote_path, repo, token, branch='main'):\n",
        "    \"\"\"\n",
        "    Retrieve the SHA of an existing file in the GitHub repository.\n",
        "    \"\"\"\n",
        "    url = f'https://api.github.com/repos/{repo}/contents/{remote_path}?ref={branch}'\n",
        "    headers = {\n",
        "        \"Authorization\": f\"token {token}\",\n",
        "        \"Accept\": \"application/vnd.github.v3+json\"\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        return response.json().get('sha')\n",
        "    return None\n",
        "\n",
        "def remove_sensitive_info(obj):\n",
        "    \"\"\"\n",
        "    Recursively process the object. For any string, obfuscate token patterns\n",
        "    by inserting a zero-width space after the first underscore. This ensures\n",
        "    that tokens (even in commented-out code) do not trigger GitHub's secret scanning.\n",
        "    \"\"\"\n",
        "    if isinstance(obj, dict):\n",
        "        new_obj = {}\n",
        "        for key, value in obj.items():\n",
        "            new_obj[key] = remove_sensitive_info(value)\n",
        "        return new_obj\n",
        "    elif isinstance(obj, list):\n",
        "        return [remove_sensitive_info(item) for item in obj]\n",
        "    elif isinstance(obj, str):\n",
        "        # Pattern matches both ghp_ tokens and github_pat_ tokens.\n",
        "        pattern = r\"(ghp_[A-Za-z0-9]{36}|github_pat_[A-Za-z0-9_]+)\"\n",
        "        def obfuscate_token(match):\n",
        "            token = match.group(0)\n",
        "            parts = token.split('_', 1)\n",
        "            if len(parts) == 2:\n",
        "                # Insert a zero-width space after the first underscore.\n",
        "                return parts[0] + '_\\u200b' + parts[1]\n",
        "            return token\n",
        "        return re.sub(pattern, obfuscate_token, obj)\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "def setup_report_folder(report_folder=REPORT_FOLDER):\n",
        "    \"\"\"\n",
        "    Create the report folder if it doesn't exist and download the report.html template and save as index.html.\n",
        "    Returns the number of files in the folder.\n",
        "    \"\"\"\n",
        "    # Create the report folder if it doesn't exist\n",
        "    if not os.path.exists(report_folder):\n",
        "        os.makedirs(report_folder)\n",
        "        print(f\"Created new directory: {report_folder}\")\n",
        "\n",
        "    # Check if index.html exists, if not download it\n",
        "    index_file_path = os.path.join(report_folder, \"index.html\")\n",
        "    if not os.path.exists(index_file_path):\n",
        "        template_url = \"https://raw.githubusercontent.com/ModelEarth/localsite/refs/heads/main/start/template/report.html\"\n",
        "        try:\n",
        "            response = requests.get(template_url)\n",
        "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "\n",
        "            with open(index_file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(response.text)\n",
        "            print(f\"Downloaded index.html template to {index_file_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading template: {e}\")\n",
        "\n",
        "    add_readme_to_report_folder(report_folder)\n",
        "\n",
        "    # Count the number of files in the report folder\n",
        "    file_count = len([f for f in os.listdir(report_folder) if os.path.isfile(os.path.join(report_folder, f))])\n",
        "    print(f\"Report folder contains {file_count} files\")\n",
        "    return file_count\n",
        "\n",
        "def add_readme_to_report_folder(report_folder=REPORT_FOLDER):\n",
        "    \"\"\"\n",
        "    Create a README.md file in the report folder if it doesn't exist yet.\n",
        "    \"\"\"\n",
        "    readme_path = os.path.join(report_folder, \"README.md\")\n",
        "\n",
        "    if not os.path.exists(readme_path):\n",
        "        readme_content = \"# Run Models Report\\n\\nThis folder contains generated reports from model executions.\"\n",
        "\n",
        "        with open(readme_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(readme_content)\n",
        "        print(f\"Created README.md in {report_folder}\")\n",
        "\n",
        "    return readme_path\n",
        "\n",
        "def upload_reports_to_github(repo, token, branch='main', commit_message='Reports from Run Models colab'):\n",
        "    \"\"\"\n",
        "    Upload all files from the report folder to GitHub repository.\n",
        "\n",
        "    Args:\n",
        "        repo (str): GitHub repository in the format 'username/repo'\n",
        "        token (str): GitHub personal access token\n",
        "        branch (str): Branch to push to (default: 'main')\n",
        "        commit_message (str): Commit message (can include {report_file_count} placeholder)\n",
        "    \"\"\"\n",
        "    # First, set up the report folder and get file count\n",
        "    report_file_count = len([f for f in os.listdir(REPORT_FOLDER) if os.path.isfile(os.path.join(REPORT_FOLDER, f))])\n",
        "\n",
        "    # Format the commit message with the file count if needed\n",
        "    if \"{report_file_count}\" in commit_message:\n",
        "        commit_message = commit_message.format(report_file_count=report_file_count)\n",
        "\n",
        "    print(f\"Preparing to push {report_file_count} reports to: {repo}\")\n",
        "\n",
        "    # GitHub API endpoint for getting the reference\n",
        "    api_url = f\"https://api.github.com/repos/{repo}\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"token {token}\",\n",
        "        \"Accept\": \"application/vnd.github.v3+json\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Get the current reference (SHA) of the branch\n",
        "        ref_response = requests.get(f\"{api_url}/git/refs/heads/{branch}\", headers=headers)\n",
        "        ref_response.raise_for_status()\n",
        "        ref_sha = ref_response.json()[\"object\"][\"sha\"]\n",
        "\n",
        "        # Get the current commit to which the branch points\n",
        "        commit_response = requests.get(f\"{api_url}/git/commits/{ref_sha}\", headers=headers)\n",
        "        commit_response.raise_for_status()\n",
        "        base_tree_sha = commit_response.json()[\"tree\"][\"sha\"]\n",
        "\n",
        "        # Create a new tree with all the files in the report folder\n",
        "        new_tree = []\n",
        "\n",
        "        report_path = Path(REPORT_FOLDER)\n",
        "        for file_path in report_path.glob(\"**/*\"):\n",
        "            if file_path.is_file():\n",
        "                # Calculate the path relative to the report folder\n",
        "                relative_path = file_path.relative_to(report_path)\n",
        "                #github_path = f\"reports/{relative_path}\"\n",
        "                thefile = file_path.name\n",
        "                github_path = f\"{FULL_REPORT_PATH}/{thefile}\"\n",
        "                print(f\"github_path: {github_path}\")\n",
        "\n",
        "                # Read file content and encode as base64\n",
        "                with open(file_path, \"rb\") as f:\n",
        "                    content = f.read()\n",
        "\n",
        "                # Add the file to the new tree\n",
        "                new_tree.append({\n",
        "                    \"path\": github_path,\n",
        "                    \"mode\": \"100644\",  # File mode (100644 for regular file)\n",
        "                    \"type\": \"blob\",\n",
        "                    \"content\": content.decode('utf-8', errors='replace')\n",
        "                })\n",
        "\n",
        "        # Create a new tree with the new files\n",
        "        new_tree_response = requests.post(\n",
        "            f\"{api_url}/git/trees\",\n",
        "            headers=headers,\n",
        "            json={\n",
        "                \"base_tree\": base_tree_sha,\n",
        "                \"tree\": new_tree\n",
        "            }\n",
        "        )\n",
        "        new_tree_response.raise_for_status()\n",
        "        new_tree_sha = new_tree_response.json()[\"sha\"]\n",
        "\n",
        "        # Create a new commit\n",
        "        new_commit_response = requests.post(\n",
        "            f\"{api_url}/git/commits\",\n",
        "            headers=headers,\n",
        "            json={\n",
        "                \"message\": commit_message,\n",
        "                \"tree\": new_tree_sha,\n",
        "                \"parents\": [ref_sha]\n",
        "            }\n",
        "        )\n",
        "        new_commit_response.raise_for_status()\n",
        "        new_commit_sha = new_commit_response.json()[\"sha\"]\n",
        "\n",
        "        # Update the reference to point to the new commit\n",
        "        update_ref_response = requests.patch(\n",
        "            f\"{api_url}/git/refs/heads/{branch}\",\n",
        "            headers=headers,\n",
        "            json={\"sha\": new_commit_sha}\n",
        "        )\n",
        "        update_ref_response.raise_for_status()\n",
        "\n",
        "        print(f\"Pushed {report_file_count} files to GitHub repository: {repo}\")\n",
        "        print(f\"Branch: {branch}\")\n",
        "        print(f\"Commit message: {commit_message}\")\n",
        "        print(f\"Repo: {DEFAULT_REPO}\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error uploading files to GitHub: {e}\")\n",
        "        return False\n",
        "\n",
        "upload_reports_to_github(DEFAULT_REPO, DEFAULT_TOKEN, branch='main', commit_message='Pushed report files to GitHub')\n",
        "\n",
        "#upload_notebook_to_github(\"Run-Models-bkup.ipynb\", DEFAULT_REPO, DEFAULT_TOKEN, branch='main', commit_message='Update notebook')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pulling Data from Google Data Commons - to be deleted later"
      ],
      "metadata": {
        "id": "kpjiu4YiD0Lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datacommons_pandas --upgrade --quiet\n",
        "\n",
        "import datacommons_pandas as dc"
      ],
      "metadata": {
        "id": "tEBnfCw-D8JS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30f9aa02"
      },
      "source": [
        "def getGoogleData(stat_vars, places):\n",
        "    \"\"\"\n",
        "    Fetch full time series data for multiple (place, stat_var) pairs from Data Commons.\n",
        "\n",
        "    Parameters:\n",
        "        stat_vars (str or list): One or more statistical variable DCIDs\n",
        "        places (str or list): One or more place DCIDs\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Long-format DataFrame with columns: date, value, place, stat_var\n",
        "    \"\"\"\n",
        "    if isinstance(stat_vars, str):\n",
        "        stat_vars = [stat_vars]\n",
        "    if isinstance(places, str):\n",
        "        places = [places]\n",
        "\n",
        "    all_data = []\n",
        "\n",
        "    for place in places:\n",
        "        for stat_var in stat_vars:\n",
        "            try:\n",
        "                ts = dc.build_time_series(place=place, stat_var=stat_var)\n",
        "                if isinstance(ts, pd.Series):\n",
        "                    ts = ts.to_frame(name=\"value\")\n",
        "                    ts[\"place\"] = place\n",
        "                    ts[\"stat_var\"] = stat_var\n",
        "                    ts = ts.reset_index(names=\"date\")\n",
        "                    all_data.append(ts)\n",
        "            except Exception as e:\n",
        "                print(f\"Error fetching {stat_var} for {place}: {e}\")\n",
        "\n",
        "    if all_data:\n",
        "        return pd.concat(all_data, ignore_index=True)\n",
        "    else:\n",
        "        return pd.DataFrame()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fea73c54"
      },
      "source": [
        "#Get CO2 emissions and population over time for USA and China\n",
        "df = getGoogleData(\n",
        "    stat_vars=[\"Count_Person\", 'Annual_Amount_Emissions_CarbonDioxide'],\n",
        "    places=[\"country/USA\", \"country/CHN\"]\n",
        ")\n",
        "\n",
        "print(df.head())\n",
        "print(df.tail())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get population across US states over time\n",
        "us_counties = dc.get_places_in([\"country/USA\"], \"County\")\n",
        "print(us_counties)"
      ],
      "metadata": {
        "id": "ALV9PfHcQ69r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(us_counties[\"country/USA\"])"
      ],
      "metadata": {
        "id": "-F9j_hetddTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Commenting this as it is very time consuming. Takes around 5 mins.\n",
        "# df_counties = getGoogleData(\n",
        "#     stat_vars=\"Count_Person\",\n",
        "#     places=us_counties['country/USA'] #using the list from above\n",
        "# )"
      ],
      "metadata": {
        "id": "XH3MqBEwSyiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(df_counties.head())"
      ],
      "metadata": {
        "id": "i9kEYMmGTWo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring v2 of datacommons Python API - to be deleted?"
      ],
      "metadata": {
        "id": "SFLk-jXIc-jP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"datacommons-client[Pandas]\" --upgrade --quiet\n",
        "\n",
        "from datacommons_client import DataCommonsClient"
      ],
      "metadata": {
        "id": "J8yX2h4qYjmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = DataCommonsClient(api_key=\"AIzaSyCTI4Xz-UW_G2Q2RfknhcfdAnTHq5X5XuI\")"
      ],
      "metadata": {
        "id": "trsjSsShYkaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "usa_name = 'United States'\n",
        "usa = client.resolve.fetch_dcids_by_name(usa_name).to_flat_dict()[usa_name]\n",
        "usa"
      ],
      "metadata": {
        "id": "MJTrPYfCjo1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counties = client.node.fetch_place_children(usa, children_type='County')[usa]\n",
        "counties[:5]"
      ],
      "metadata": {
        "id": "4dhaKUEzfxL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counties = [county['dcid'] for county in counties]\n",
        "counties[:5]"
      ],
      "metadata": {
        "id": "16nsif2CkgLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = client.observations_dataframe(\n",
        "    variable_dcids=[\"Count_Person\"],\n",
        "    date=\"all\",\n",
        "    entity_dcids=counties\n",
        ")"
      ],
      "metadata": {
        "id": "-89KfzP8YoI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "3P0XgaYcY1Tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "qCQ74tJQY3PK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"entity_name\"].nunique()"
      ],
      "metadata": {
        "id": "MB7CxiTda3BO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"entity\"] = df[\"entity\"].str[6:]\n",
        "df_counties = df[[\"entity\", \"entity_name\", \"date\", \"value\"]].copy()\n",
        "df_counties = df_counties.rename(columns={\"entity\": \"Fips\", \"entity_name\": \"county_name\", \"value\": \"Population\"})\n",
        "df_counties.head()"
      ],
      "metadata": {
        "id": "XfEij9jqm6IS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Land Cover Fraction Forest Data/ Tree data - Thanmayi\n"
      ],
      "metadata": {
        "id": "JHNZTLUnjCf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"datacommons-client[Pandas]\" --upgrade --quiet"
      ],
      "metadata": {
        "id": "oyQ9rze7j7Lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datacommons_client import DataCommonsClient\n",
        "import time\n",
        "\n",
        "client = DataCommonsClient(api_key=\"AIzaSyCTI4Xz-UW_G2Q2RfknhcfdAnTHq5X5XuI\")"
      ],
      "metadata": {
        "id": "DE-wNlf8j-an"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_forest_cover_dataframe(dcid: str, level: str = \"County\") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Pulls LandCoverFraction_Forest data for all child counties under the given dcid.\n",
        "\n",
        "    Parameters:\n",
        "    - dcid: The DCID of the parent region (e.g., \"geoId/13\" for Georgia)\n",
        "    - level: Child place type to pull data for (default: \"County\")\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame with forest cover data\n",
        "    \"\"\"\n",
        "    from datacommons_client import DataCommonsClient\n",
        "    import pandas as pd\n",
        "\n",
        "    client = DataCommonsClient(api_key=\"AIzaSyCTI4Xz-UW_G2Q2RfknhcfdAnTHq5X5XuI\")\n",
        "\n",
        "    print(f\"Getting child {level}s for {dcid}...\")\n",
        "    try:\n",
        "        child_places = client.node.fetch_place_children(dcid, children_type=level)[dcid]\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching children for {dcid}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    if not child_places:\n",
        "        print(\"No child places found.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Extract DCIDs only\n",
        "    child_dcids = [place[\"dcid\"] for place in child_places]\n",
        "    print(f\"Found {len(child_dcids)} {level}s\")\n",
        "\n",
        "    print(\"Fetching forest cover data...\")\n",
        "    df = client.observations_dataframe(\n",
        "        variable_dcids=[\"LandCoverFraction_Forest\"],\n",
        "        date=\"all\",\n",
        "        entity_dcids=child_dcids\n",
        "    )\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"No forest cover data found.\")\n",
        "    else:\n",
        "        print(f\"Retrieved {len(df)} rows of forest data.\")\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "3_45HHyloO7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Get full forest data for Georgia\n",
        "df = get_forest_cover_dataframe(\"geoId/13\")\n",
        "\n",
        "\"\"\"\n",
        "Below is the complete list of U.S. state geoId codes you can use as input:\n",
        "\n",
        "    \"Alabama\": \"geoId/01\",\n",
        "    \"Alaska\": \"geoId/02\",\n",
        "    \"Arizona\": \"geoId/04\",\n",
        "    \"Arkansas\": \"geoId/05\",\n",
        "    \"California\": \"geoId/06\",\n",
        "    \"Colorado\": \"geoId/08\",\n",
        "    \"Connecticut\": \"geoId/09\",\n",
        "    \"Delaware\": \"geoId/10\",\n",
        "    \"Florida\": \"geoId/12\",\n",
        "    \"Georgia\": \"geoId/13\",\n",
        "    \"Hawaii\": \"geoId/15\",\n",
        "    \"Idaho\": \"geoId/16\",\n",
        "    \"Illinois\": \"geoId/17\",\n",
        "    \"Indiana\": \"geoId/18\",\n",
        "    \"Iowa\": \"geoId/19\",\n",
        "    \"Kansas\": \"geoId/20\",\n",
        "    \"Kentucky\": \"geoId/21\",\n",
        "    \"Louisiana\": \"geoId/22\",\n",
        "    \"Maine\": \"geoId/23\",\n",
        "    \"Maryland\": \"geoId/24\",\n",
        "    \"Massachusetts\": \"geoId/25\",\n",
        "    \"Michigan\": \"geoId/26\",\n",
        "    \"Minnesota\": \"geoId/27\",\n",
        "    \"Mississippi\": \"geoId/28\",\n",
        "    \"Missouri\": \"geoId/29\",\n",
        "    \"Montana\": \"geoId/30\",\n",
        "    \"Nebraska\": \"geoId/31\",\n",
        "    \"Nevada\": \"geoId/32\",\n",
        "    \"New Hampshire\": \"geoId/33\",\n",
        "    \"New Jersey\": \"geoId/34\",\n",
        "    \"New Mexico\": \"geoId/35\",\n",
        "    \"New York\": \"geoId/36\",\n",
        "    \"North Carolina\": \"geoId/37\",\n",
        "    \"North Dakota\": \"geoId/38\",\n",
        "    \"Ohio\": \"geoId/39\",\n",
        "    \"Oklahoma\": \"geoId/40\",\n",
        "    \"Oregon\": \"geoId/41\",\n",
        "    \"Pennsylvania\": \"geoId/42\",\n",
        "    \"Rhode Island\": \"geoId/44\",\n",
        "    \"South Carolina\": \"geoId/45\",\n",
        "    \"South Dakota\": \"geoId/46\",\n",
        "    \"Tennessee\": \"geoId/47\",\n",
        "    \"Texas\": \"geoId/48\",\n",
        "    \"Utah\": \"geoId/49\",\n",
        "    \"Vermont\": \"geoId/50\",\n",
        "    \"Virginia\": \"geoId/51\",\n",
        "    \"Washington\": \"geoId/53\",\n",
        "    \"West Virginia\": \"geoId/54\",\n",
        "    \"Wisconsin\": \"geoId/55\",\n",
        "    \"Wyoming\": \"geoId/56\"\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Step 2: Clean and rename\n",
        "df[\"entity\"] = df[\"entity\"].str[6:]\n",
        "df_forest = df[[\"entity\", \"entity_name\", \"date\", \"value\"]].copy()\n",
        "df_forest = df_forest.rename(columns={\n",
        "    \"entity\": \"Fips\",\n",
        "    \"entity_name\": \"county_name\",\n",
        "    \"date\": \"year\",\n",
        "    \"value\": \"Forest_Cover_Percent\"\n",
        "})\n",
        "\n",
        "# Step 3: Filter for a specific year\n",
        "\"\"\"\n",
        "    Forest Cover Data Years:\n",
        "\n",
        "    Available years: 2015, 2016, 2017, 2018, 2019\n",
        "\n",
        "    Forest cover data is not available before 2015 or after 2019.\n",
        "    If filtering by year, please choose a value from this range.\n",
        "\n",
        "    Example:\n",
        "    year_to_filter = \"2019\"\n",
        "\"\"\"\n",
        "year_to_filter = \"2019\"  # <-- user can input this\n",
        "df_filtered = df_forest[df_forest[\"year\"] == year_to_filter]\n",
        "\n",
        "# Step 4: Preview\n",
        "df_filtered.head()"
      ],
      "metadata": {
        "id": "ThytFRavkQo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pull from Google Data Commons from yaml files - Prathyusha\n"
      ],
      "metadata": {
        "id": "jgqucY_A_rYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"datacommons-client[Pandas]\" --upgrade --quiet\n",
        "\n",
        "import pandas as pd\n",
        "from datacommons_client import DataCommonsClient"
      ],
      "metadata": {
        "id": "eMzCTYs4EfPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GDC Data Pull Function if dcids are present in param object\n",
        "def load_gdc_data_if_present(param):\n",
        "    \"\"\"\n",
        "    Load data from GDC if dcid fields are present.\n",
        "    Returns (features_df, targets_df) if dcid found, otherwise (None, None)\n",
        "    \"\"\"\n",
        "\n",
        "    # Check if dcid fields exist\n",
        "    features_has_dcid = (hasattr(param, 'features') and\n",
        "                        hasattr(param.features, 'dcid'))\n",
        "\n",
        "    targets_has_dcid = (hasattr(param, 'targets') and\n",
        "                       hasattr(param.targets, 'dcid'))\n",
        "\n",
        "    if not features_has_dcid and not targets_has_dcid:\n",
        "        print(\"No dcid fields found in parameters\")\n",
        "        return None, None\n",
        "\n",
        "    print(\"Found dcid fields - loading from Google Data Commons...\")\n",
        "\n",
        "    # Initialize GDC client\n",
        "    try:\n",
        "        client = DataCommonsClient(api_key=\"AIzaSyCTI4Xz-UW_G2Q2RfknhcfdAnTHq5X5XuI\")\n",
        "        print(\"GDC client initialized\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to initialize GDC client: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    features_df = None\n",
        "    targets_df = None\n",
        "\n",
        "    # Load features from GDC\n",
        "    if features_has_dcid:\n",
        "        try:\n",
        "            print(\"Loading features from GDC...\")\n",
        "\n",
        "            dcids = param.features.dcid\n",
        "            if isinstance(dcids, str):\n",
        "                dcids = [dcids]\n",
        "\n",
        "            variables = getattr(param.features, 'variables', ['Count_Person', 'Median_Income_Person'])\n",
        "            if isinstance(variables, str):\n",
        "                variables = [variables]\n",
        "\n",
        "            year = getattr(param.features, 'year', 'LATEST')\n",
        "\n",
        "            print(f\"Entities: {len(dcids)}\")\n",
        "            print(f\"Variables: {variables}\")\n",
        "            print(f\"Year: {year}\")\n",
        "\n",
        "            features_df = client.observations_dataframe(\n",
        "                variable_dcids=variables,\n",
        "                date=str(year) if year != 'LATEST' else 'LATEST',\n",
        "                entity_dcids=dcids\n",
        "            )\n",
        "\n",
        "            if not features_df.empty:\n",
        "                # Clean entity column\n",
        "                features_df[\"entity\"] = features_df[\"entity\"].str.replace(\"geoId/\", \"\", regex=False)\n",
        "                features_df = features_df.rename(columns={\"entity\": \"Fips\"})\n",
        "                features_df[\"Fips\"] = features_df[\"Fips\"].astype(str)\n",
        "\n",
        "                print(f\"Features loaded: {features_df.shape}\")\n",
        "            else:\n",
        "                print(\"No features data returned from GDC\")\n",
        "                features_df = None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Features loading failed: {e}\")\n",
        "            features_df = None\n",
        "\n",
        "    # Load targets from GDC\n",
        "    if targets_has_dcid:\n",
        "        try:\n",
        "            print(\"Loading targets from GDC...\")\n",
        "\n",
        "            dcids = param.targets.dcid\n",
        "            if isinstance(dcids, str):\n",
        "                dcids = [dcids]\n",
        "\n",
        "            variables = getattr(param.targets, 'variables', ['Count_Person'])\n",
        "            if isinstance(variables, str):\n",
        "                variables = [variables]\n",
        "\n",
        "            year = getattr(param.targets, 'year', 'LATEST')\n",
        "\n",
        "            print(f\"Entities: {len(dcids)}\")\n",
        "            print(f\"Variables: {variables}\")\n",
        "            print(f\"Year: {year}\")\n",
        "\n",
        "            targets_df = client.observations_dataframe(\n",
        "                variable_dcids=variables,\n",
        "                date=str(year) if year != 'LATEST' else 'LATEST',\n",
        "                entity_dcids=dcids\n",
        "            )\n",
        "\n",
        "            if not targets_df.empty:\n",
        "                # Clean entity column\n",
        "                targets_df[\"entity\"] = targets_df[\"entity\"].str.replace(\"geoId/\", \"\", regex=False)\n",
        "                targets_df = targets_df.rename(columns={\"entity\": \"Fips\"})\n",
        "                targets_df[\"Fips\"] = targets_df[\"Fips\"].astype(str)\n",
        "\n",
        "                print(f\"Targets loaded: {targets_df.shape}\")\n",
        "            else:\n",
        "                print(\"No targets data returned from GDC\")\n",
        "                targets_df = None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Targets loading failed: {e}\")\n",
        "            targets_df = None\n",
        "\n",
        "    return features_df, targets_df"
      ],
      "metadata": {
        "id": "a-rdH5vwP6aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GDC data pull\n",
        "from IPython.display import display\n",
        "\n",
        "if 'param' not in globals():\n",
        "    print(\"No param object found. Run your parameter widget first.\")\n",
        "else:\n",
        "    print(\"Attempting to load data from Google Data Commons...\")\n",
        "\n",
        "    features_df, targets_df = load_gdc_data_if_present(param)\n",
        "\n",
        "    # Show results\n",
        "    if features_df is not None or targets_df is not None:\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"GDC DATA SUMMARY\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        if features_df is not None:\n",
        "            print(f\"\\nFEATURES: {features_df.shape}\")\n",
        "            print(f\"Columns: {list(features_df.columns)}\")\n",
        "            print(\"\\nSample data:\")\n",
        "            display(features_df.head(3))\n",
        "\n",
        "        if targets_df is not None:\n",
        "            print(f\"\\nTARGETS: {targets_df.shape}\")\n",
        "            print(f\"Columns: {list(targets_df.columns)}\")\n",
        "            print(\"\\nSample data:\")\n",
        "            display(targets_df.head(3))\n",
        "\n",
        "        print(\"=\"*50)\n",
        "    else:\n",
        "        print(\"\\nNo GDC data loaded - use existing data loading methods\")"
      ],
      "metadata": {
        "id": "-b792UtcQ5Ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Land Cover Fraction Forest Data/ Tree data Prep - Savar\n"
      ],
      "metadata": {
        "id": "Ah08HJxpVjqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"datacommons-client[Pandas]\" --upgrade --quiet"
      ],
      "metadata": {
        "id": "pBkwOnJbVjqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datacommons_client import DataCommonsClient\n",
        "import time\n",
        "\n",
        "client = DataCommonsClient(api_key=\"AIzaSyCTI4Xz-UW_G2Q2RfknhcfdAnTHq5X5XuI\")"
      ],
      "metadata": {
        "id": "bxcF3U4uVjqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_forest_cover_dataframe(dcid: str, level: str = \"County\") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Pulls LandCoverFraction_Forest data for all child counties under the given dcid.\n",
        "\n",
        "    Parameters:\n",
        "    - dcid: The DCID of the parent region (e.g., \"geoId/13\" for Georgia)\n",
        "    - level: Child place type to pull data for (default: \"County\")\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame with forest cover data\n",
        "    \"\"\"\n",
        "    from datacommons_client import DataCommonsClient\n",
        "    import pandas as pd\n",
        "\n",
        "    client = DataCommonsClient(api_key=\"AIzaSyCTI4Xz-UW_G2Q2RfknhcfdAnTHq5X5XuI\")\n",
        "\n",
        "    print(f\"Getting child {level}s for {dcid}...\")\n",
        "    try:\n",
        "        child_places = client.node.fetch_place_children(dcid, children_type=level)[dcid]\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching children for {dcid}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    if not child_places:\n",
        "        print(\"No child places found.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Extract DCIDs only\n",
        "    child_dcids = [place[\"dcid\"] for place in child_places]\n",
        "    print(f\"Found {len(child_dcids)} {level}s\")\n",
        "\n",
        "    print(\"Fetching forest cover data...\")\n",
        "    df = client.observations_dataframe(\n",
        "        variable_dcids=[\"LandCoverFraction_Forest\"],\n",
        "        date=\"all\",\n",
        "        entity_dcids=child_dcids\n",
        "    )\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"No forest cover data found.\")\n",
        "    else:\n",
        "        print(f\"Retrieved {len(df)} rows of forest data.\")\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "1bRYOkoSVjqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Get full forest data for Georgia\n",
        "df = get_forest_cover_dataframe(\"geoId/13\")\n",
        "\n",
        "\"\"\"\n",
        "Below is the complete list of U.S. state geoId codes you can use as input:\n",
        "\n",
        "    \"Alabama\": \"geoId/01\",\n",
        "    \"Alaska\": \"geoId/02\",\n",
        "    \"Arizona\": \"geoId/04\",\n",
        "    \"Arkansas\": \"geoId/05\",\n",
        "    \"California\": \"geoId/06\",\n",
        "    \"Colorado\": \"geoId/08\",\n",
        "    \"Connecticut\": \"geoId/09\",\n",
        "    \"Delaware\": \"geoId/10\",\n",
        "    \"Florida\": \"geoId/12\",\n",
        "    \"Georgia\": \"geoId/13\",\n",
        "    \"Hawaii\": \"geoId/15\",\n",
        "    \"Idaho\": \"geoId/16\",\n",
        "    \"Illinois\": \"geoId/17\",\n",
        "    \"Indiana\": \"geoId/18\",\n",
        "    \"Iowa\": \"geoId/19\",\n",
        "    \"Kansas\": \"geoId/20\",\n",
        "    \"Kentucky\": \"geoId/21\",\n",
        "    \"Louisiana\": \"geoId/22\",\n",
        "    \"Maine\": \"geoId/23\",\n",
        "    \"Maryland\": \"geoId/24\",\n",
        "    \"Massachusetts\": \"geoId/25\",\n",
        "    \"Michigan\": \"geoId/26\",\n",
        "    \"Minnesota\": \"geoId/27\",\n",
        "    \"Mississippi\": \"geoId/28\",\n",
        "    \"Missouri\": \"geoId/29\",\n",
        "    \"Montana\": \"geoId/30\",\n",
        "    \"Nebraska\": \"geoId/31\",\n",
        "    \"Nevada\": \"geoId/32\",\n",
        "    \"New Hampshire\": \"geoId/33\",\n",
        "    \"New Jersey\": \"geoId/34\",\n",
        "    \"New Mexico\": \"geoId/35\",\n",
        "    \"New York\": \"geoId/36\",\n",
        "    \"North Carolina\": \"geoId/37\",\n",
        "    \"North Dakota\": \"geoId/38\",\n",
        "    \"Ohio\": \"geoId/39\",\n",
        "    \"Oklahoma\": \"geoId/40\",\n",
        "    \"Oregon\": \"geoId/41\",\n",
        "    \"Pennsylvania\": \"geoId/42\",\n",
        "    \"Rhode Island\": \"geoId/44\",\n",
        "    \"South Carolina\": \"geoId/45\",\n",
        "    \"South Dakota\": \"geoId/46\",\n",
        "    \"Tennessee\": \"geoId/47\",\n",
        "    \"Texas\": \"geoId/48\",\n",
        "    \"Utah\": \"geoId/49\",\n",
        "    \"Vermont\": \"geoId/50\",\n",
        "    \"Virginia\": \"geoId/51\",\n",
        "    \"Washington\": \"geoId/53\",\n",
        "    \"West Virginia\": \"geoId/54\",\n",
        "    \"Wisconsin\": \"geoId/55\",\n",
        "    \"Wyoming\": \"geoId/56\"\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Step 2: Clean and rename\n",
        "df[\"entity\"] = df[\"entity\"].str[6:]\n",
        "df_forest = df[[\"entity\", \"entity_name\", \"date\", \"value\"]].copy()\n",
        "df_forest = df_forest.rename(columns={\n",
        "    \"entity\": \"Fips\",\n",
        "    \"entity_name\": \"county_name\",\n",
        "    \"date\": \"year\",\n",
        "    \"value\": \"Forest_Cover_Percent\"\n",
        "})\n",
        "\n",
        "\n",
        "# Step 3: Filter for a specific year\n",
        "\"\"\"\n",
        "    Forest Cover Data Years:\n",
        "\n",
        "    Available years: 2015, 2016, 2017, 2018, 2019\n",
        "\n",
        "    Forest cover data is not available before 2015 or after 2019.\n",
        "    If filtering by year, please choose a value from this range.\n",
        "\n",
        "    Example:\n",
        "    year_to_filter = \"2019\"\n",
        "\"\"\"\n",
        "year_to_filter = \"2019\"  # <-- user can input this\n",
        "df_filtered = df_forest[df_forest[\"year\"] == year_to_filter]\n",
        "\n",
        "\n",
        "\n",
        "# Preview the filtered DataFrame\n",
        "display(df_filtered.head())\n",
        "\n",
        "\n",
        "# Step 4: Preview\n",
        "\n",
        "print(df_filtered.shape)"
      ],
      "metadata": {
        "id": "u_SP0oX5VjqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3de733d4"
      },
      "source": [
        "## Task - Savar\n",
        "Identify the top ten counties in each state likely to have declining tree canopy."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "years_to_study = range(2015, 2020)\n",
        "yearly_forest_data = {}\n",
        "\n",
        "for year in years_to_study:\n",
        "    df_year = df_forest[df_forest[\"year\"].astype(int) == year].copy()\n",
        "    yearly_forest_data[year] = df_year\n",
        "    print(f\"Created dataframe for year {year} with shape: {df_year.shape}\")\n",
        "\n",
        "# You can access the dataframes using the dictionary, e.g., yearly_forest_data[2015]"
      ],
      "metadata": {
        "id": "u4E_9f3pttPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logically calculated decline in forest cover averaged over 4 years"
      ],
      "metadata": {
        "id": "tvwB3kcHuUiV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d6655d6"
      },
      "source": [
        "# Get data for 2015 and 2019\n",
        "df_2015 = yearly_forest_data[2015]\n",
        "df_2019 = yearly_forest_data[2019]\n",
        "\n",
        "# Merge the dataframes on Fips\n",
        "df_merged = pd.merge(df_2015[['Fips', 'county_name', 'Forest_Cover_Percent']],\n",
        "                     df_2019[['Fips', 'Forest_Cover_Percent']],\n",
        "                     on='Fips',\n",
        "                     suffixes=('_2015', '_2019'))\n",
        "\n",
        "# Calculate the forest cover change and take the absolute value\n",
        "df_merged['Forest_Cover_Drop'] = abs(df_merged['Forest_Cover_Percent_2019'] - df_merged['Forest_Cover_Percent_2015'])\n",
        "\n",
        "# Sort by change to find the largest declines\n",
        "df_declining = df_merged.sort_values(by='Forest_Cover_Drop', ascending=False).head(10)\n",
        "\n",
        "print(\"Top 10 counties with the largest forest cover decline (2015-2019):\")\n",
        "display(df_declining)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing process to calculate top 10 counties methematically"
      ],
      "metadata": {
        "id": "CmonM6-m4liZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d40e5c2"
      },
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "def process_state_forest_decline(state_geoId: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Identifies the top 10 counties with the largest forest cover decline\n",
        "    for a given state (geoId) between 2015 and 2019.\n",
        "\n",
        "    Parameters:\n",
        "    - state_geoId: The geoId of the state (e.g., \"geoId/13\" for Georgia)\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame with the top 10 declining counties for the state,\n",
        "      including state geoId. Returns an empty DataFrame\n",
        "      if data fetching fails or no counties are found.\n",
        "    \"\"\"\n",
        "    print(f\"\\nProcessing forest decline for state: {state_geoId}\")\n",
        "\n",
        "    # Get forest cover data for the state's counties\n",
        "    df_state_forest = get_forest_cover_dataframe(state_geoId, level=\"County\")\n",
        "\n",
        "    if df_state_forest.empty:\n",
        "        print(f\"No forest data found for {state_geoId}. Skipping.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Filter for years 2015 and 2019 using the 'date' column\n",
        "    df_2015 = df_state_forest[df_state_forest[\"date\"].astype(int) == 2015].copy()\n",
        "    df_2019 = df_state_forest[df_state_forest[\"date\"].astype(int) == 2019].copy()\n",
        "\n",
        "    if df_2015.empty or df_2019.empty:\n",
        "        print(f\"Data for 2015 or 2019 is missing for {state_geoId}. Skipping.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Merge the dataframes on Fips (entity column after renaming)\n",
        "    df_merged = pd.merge(df_2015[['entity', 'entity_name', 'value']],\n",
        "                         df_2019[['entity', 'value']],\n",
        "                         on='entity',\n",
        "                         suffixes=('_2015', '_2019'))\n",
        "\n",
        "    # Rename columns for clarity after merging\n",
        "    df_merged = df_merged.rename(columns={\n",
        "        'entity': 'Fips',\n",
        "        'entity_name': 'county_name',\n",
        "        'value_2015': 'Forest_Cover_Percent_2015',\n",
        "        'value_2019': 'Forest_Cover_Percent_2019'\n",
        "    })\n",
        "\n",
        "\n",
        "    # Calculate the absolute forest cover drop\n",
        "    df_merged['Forest_Cover_Drop'] = abs(df_merged['Forest_Cover_Percent_2019'] - df_merged['Forest_Cover_Percent_2015'])\n",
        "\n",
        "    # Sort by drop to find the largest declines and get top 10\n",
        "    df_declining = df_merged.sort_values(by='Forest_Cover_Drop', ascending=False).head(10).copy()\n",
        "\n",
        "    # Add state geoId\n",
        "    df_declining['state_geoId'] = state_geoId\n",
        "\n",
        "\n",
        "    print(f\"Found top 10 declining counties for {state_geoId}.\")\n",
        "    return df_declining[['state_geoId', 'Fips', 'county_name', 'Forest_Cover_Percent_2015', 'Forest_Cover_Percent_2019', 'Forest_Cover_Drop']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing statewise average drops"
      ],
      "metadata": {
        "id": "DyZdcjMg4sRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "def calculate_average_state_drop(state_geoId: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calculates the average forest cover drop for a state between 2015 and 2019.\n",
        "\n",
        "    Parameters:\n",
        "    - state_geoId: The geoId of the state (e.g., \"geoId/13\" for Georgia)\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame with columns ['geoId', 'average_drop']\n",
        "      containing the calculated values for the input state. Returns an empty\n",
        "      DataFrame if data fetching fails or no counties are found.\n",
        "    \"\"\"\n",
        "    print(f\"\\nCalculating average forest drop for state: {state_geoId}\")\n",
        "\n",
        "    # Get forest cover data for the state's counties\n",
        "    df_state_forest = get_forest_cover_dataframe(state_geoId, level=\"County\")\n",
        "\n",
        "    if df_state_forest.empty:\n",
        "        print(f\"No forest data found for {state_geoId}. Skipping.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Filter for years 2015 and 2019 using the 'date' column\n",
        "    df_2015 = df_state_forest[df_state_forest[\"date\"].astype(int) == 2015].copy()\n",
        "    df_2019 = df_state_forest[df_state_forest[\"date\"].astype(int) == 2019].copy()\n",
        "\n",
        "    if df_2015.empty or df_2019.empty:\n",
        "        print(f\"Data for 2015 or 2019 is missing for {state_geoId}. Skipping.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Rename columns before merging for consistency\n",
        "    df_2015 = df_2015.rename(columns={\n",
        "        'entity': 'Fips',\n",
        "        'value': 'Forest_Cover_Percent'\n",
        "    })\n",
        "    df_2019 = df_2019.rename(columns={\n",
        "        'entity': 'Fips',\n",
        "        'value': 'Forest_Cover_Percent'\n",
        "    })\n",
        "\n",
        "    # Merge the dataframes on Fips\n",
        "    df_merged = pd.merge(df_2015[['Fips', 'Forest_Cover_Percent']],\n",
        "                         df_2019[['Fips', 'Forest_Cover_Percent']],\n",
        "                         on='Fips',\n",
        "                         suffixes=('_2015', '_2019'))\n",
        "\n",
        "\n",
        "    # Calculate the absolute forest cover drop\n",
        "    df_merged['Forest_Cover_Drop'] = abs(df_merged['Forest_Cover_Percent_2019'] - df_merged['Forest_Cover_Percent_2015'])\n",
        "\n",
        "    # Calculate the average drop for the state\n",
        "    average_drop = df_merged['Forest_Cover_Drop'].mean()\n",
        "\n",
        "    print(f\"Average drop for {state_geoId}: {average_drop:.2f}%\")\n",
        "\n",
        "    # Return as a DataFrame\n",
        "    return pd.DataFrame({\n",
        "        'state_geoId': [state_geoId],\n",
        "        'average_drop': [average_drop]\n",
        "    })"
      ],
      "metadata": {
        "id": "xlN2V2N72w2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process test with 4 states"
      ],
      "metadata": {
        "id": "ndQ0XSnV4yHo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07e7f0e0"
      },
      "source": [
        "# List of state geoIds to process (replace with your desired list of states)\n",
        "# Example includes a few states:\n",
        "state_geoIds = [\n",
        "    \"geoId/13\", # Georgia (already used in testing)\n",
        "    \"geoId/36\", # New York\n",
        "    \"geoId/06\", # California\n",
        "    \"geoId/48\"  # Texas\n",
        "]\n",
        "\n",
        "all_states_top_10_declining = pd.DataFrame()\n",
        "average_state_drops = pd.DataFrame()\n",
        "\n",
        "for state_geoId in state_geoIds:\n",
        "    # Process state forest decline and append to the combined DataFrame\n",
        "    top_10_declining_state = process_state_forest_decline(state_geoId)\n",
        "    if not top_10_declining_state.empty:\n",
        "        all_states_top_10_declining = pd.concat([all_states_top_10_declining, top_10_declining_state], ignore_index=True)\n",
        "\n",
        "    # Calculate average state drop and append to the combined DataFrame\n",
        "    average_drop_state = calculate_average_state_drop(state_geoId)\n",
        "    if not average_drop_state.empty:\n",
        "        average_state_drops = pd.concat([average_state_drops, average_drop_state], ignore_index=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming STATE_DICT is already defined in a previous cell\n",
        "# Example STATE_DICT (should match the one in your notebook):\n",
        "STATE_DICT = {\n",
        "    \"AL\": \"Alabama\", \"AK\": \"Alaska\", \"AZ\": \"Arizona\", \"AR\": \"Arkansas\", \"CA\": \"California\", \"CO\": \"Colorado\",\n",
        "    \"CT\": \"Connecticut\", \"DE\": \"Delaware\", \"FL\": \"Florida\", \"GA\": \"Georgia\", \"HI\": \"Hawaii\", \"ID\": \"Idaho\",\n",
        "    \"IL\": \"Illinois\", \"IN\": \"Indiana\", \"IA\": \"Iowa\", \"KS\": \"Kansas\", \"KY\": \"Kentucky\", \"LA\": \"Louisiana\",\n",
        "    \"ME\": \"Maine\", \"MD\": \"Maryland\", \"MA\": \"Massachusetts\", \"MI\": \"Michigan\", \"MN\": \"Minnesota\", \"MS\": \"Mississippi\",\n",
        "    \"MO\": \"Missouri\", \"MT\": \"Montana\", \"NE\": \"Nebraska\", \"NV\": \"Nevada\", \"NH\": \"New Hampshire\", \"NJ\": \"New Jersey\",\n",
        "    \"NM\": \"New Mexico\", \"NY\": \"New York\", \"NC\": \"North Carolina\", \"ND\": \"North Dakota\", \"OH\": \"Ohio\", \"OK\": \"Oklahoma\",\n",
        "    \"OR\": \"Oregon\", \"PA\": \"Pennsylvania\", \"RI\": \"Rhode Island\", \"SC\": \"South Carolina\", \"SD\": \"South Dakota\",\n",
        "    \"TN\": \"Tennessee\", \"TX\": \"Texas\", \"UT\": \"Utah\", \"VT\": \"Vermont\", \"VA\": \"Virginia\", \"WA\": \"Washington\",\n",
        "    \"WV\": \"West Virginia\", \"WI\": \"Wisconsin\", \"WY\": \"Wyoming\",\n",
        "    \"DC\": \"District of Columbia\",\n",
        "    # US Territories\n",
        "    \"AS\": \"American Samoa\", \"GU\": \"Guam\", \"MP\": \"Northern Mariana Islands\", \"PR\": \"Puerto Rico\", \"VI\": \"U.S. Virgin Islands\"\n",
        "}\n",
        "\n",
        "\n",
        "def map_geoId_to_state_name(df, geoid_column='geoId'):\n",
        "    \"\"\"\n",
        "    Maps geoId values in a specified column of a DataFrame to their\n",
        "    corresponding state names using the STATE_DICT.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame.\n",
        "        geoid_column (str): The name of the column containing geoId values.\n",
        "                             Defaults to 'geoId'.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame with geoId values replaced by state names,\n",
        "                      or the original DataFrame if the column does not exist.\n",
        "    \"\"\"\n",
        "    if geoid_column not in df.columns:\n",
        "        print(f\"Warning: Column '{geoid_column}' not found in the DataFrame.\")\n",
        "        return df\n",
        "\n",
        "    # Extract state abbreviation from geoId (e.g., 'geoId/13' -> '13')\n",
        "    # Then map the state code (e.g., '13') to the state abbreviation (e.g., 'GA')\n",
        "    # This requires a mapping from state code to abbreviation or updating STATE_DICT keys.\n",
        "    # Assuming STATE_DICT keys are already abbreviations like 'GA', 'NY', etc.\n",
        "    # If geoId format is 'geoId/XX', we need to map XX to the abbreviation first.\n",
        "    # Let's assume for now geoId column contains abbreviations or full geoId strings like 'geoId/13'\n",
        "\n",
        "    # If geoId column contains strings like 'geoId/13', extract the state code '13'\n",
        "    # and then map it to the abbreviation using a reverse lookup or another dictionary.\n",
        "    # For simplicity, let's assume the column contains abbreviations (e.g., 'GA', 'NY')\n",
        "    # or that STATE_DICT keys are designed to handle the format in the DataFrame.\n",
        "\n",
        "    # A robust approach would involve mapping geoId/XX to abbreviation first.\n",
        "    # Let's create a simple mapping for common geoIds to abbreviations for this function's use case.\n",
        "    # This is a simplified example, a more complete mapping might be needed.\n",
        "    geoid_to_abbr = {\n",
        "        \"geoId/01\": \"AL\", \"geoId/02\": \"AK\", \"geoId/04\": \"AZ\", \"geoId/05\": \"AR\", \"geoId/06\": \"CA\",\n",
        "        \"geoId/08\": \"CO\", \"geoId/09\": \"CT\", \"geoId/10\": \"DE\", \"geoId/12\": \"FL\", \"geoId/13\": \"GA\",\n",
        "        \"geoId/15\": \"HI\", \"geoId/16\": \"ID\", \"geoId/17\": \"IL\", \"geoId/18\": \"IN\", \"geoId/19\": \"IA\",\n",
        "        \"geoId/20\": \"KS\", \"geoId/21\": \"KY\", \"geoId/22\": \"LA\", \"geoId/23\": \"ME\", \"geoId/24\": \"MD\",\n",
        "        \"geoId/25\": \"MA\", \"geoId/26\": \"MI\", \"geoId/27\": \"MN\", \"geoId/28\": \"MS\", \"geoId/29\": \"MO\",\n",
        "        \"geoId/30\": \"MT\", \"geoId/31\": \"NE\", \"geoId/32\": \"NV\", \"geoId/33\": \"NH\", \"geoId/34\": \"NJ\",\n",
        "        \"geoId/35\": \"NM\", \"geoId/36\": \"NY\", \"geoId/37\": \"NC\", \"geoId/38\": \"ND\", \"geoId/39\": \"OH\",\n",
        "        \"geoId/40\": \"OK\", \"geoId/41\": \"OR\", \"geoId/42\": \"PA\", \"geoId/44\": \"RI\", \"geoId/45\": \"SC\",\n",
        "        \"geoId/46\": \"SD\", \"geoId/47\": \"TN\", \"geoId/48\": \"TX\", \"geoId/49\": \"UT\", \"geoId/50\": \"VT\",\n",
        "        \"geoId/51\": \"VA\", \"geoId/53\": \"WA\", \"geoId/54\": \"WV\", \"geoId/55\": \"WI\", \"geoId/56\": \"WY\",\n",
        "        \"geoId/11\": \"DC\" # District of Columbia\n",
        "    }\n",
        "\n",
        "\n",
        "    # Apply the mapping\n",
        "    # First map geoId string to abbreviation\n",
        "    df[geoid_column] = df[geoid_column].map(geoid_to_abbr).fillna(df[geoid_column]) # Fallback to original if no mapping\n",
        "\n",
        "    # Then map abbreviation to full state name\n",
        "    df[geoid_column] = df[geoid_column].map(STATE_DICT).fillna(df[geoid_column]) # Fallback to abbreviation if no mapping\n",
        "\n",
        "    return df\n",
        "\n",
        "# Example usage (assuming average_state_drops DataFrame exists):\n",
        "# average_state_drops_with_names = map_geoId_to_state_name(average_state_drops.copy(), geoid_column='geoId')\n",
        "# print(\"\\nAverage State Forest Drops with State Names:\")\n",
        "# display(average_state_drops_with_names)\n",
        "\n",
        "# Example usage with all_states_top_10_declining (assuming it exists):\n",
        "# all_states_top_10_declining_with_names = map_geoId_to_state_name(all_states_top_10_declining.copy(), geoid_column='state_geoId')\n",
        "# print(\"\\nCombined Top 10 Declining Counties with State Names:\")\n",
        "# display(all_states_top_10_declining_with_names.head())"
      ],
      "metadata": {
        "id": "JYWoaLhBnHFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Processing Complete ---\")\n",
        "print(\"\\nCombined Top 10 Declining Counties (all states):\")\n",
        "all_states_top_10_declining=map_geoId_to_state_name(all_states_top_10_declining.copy(), geoid_column='state_geoId')\n",
        "display(all_states_top_10_declining.head())\n",
        "print(f\"\\nShape of combined top 10 declining counties: {all_states_top_10_declining.shape}\")\n",
        "\n",
        "print(\"\\nAverage State Forest Drops:\")\n",
        "average_state_drops=map_geoId_to_state_name(average_state_drops.copy(), geoid_column='state_geoId')\n",
        "display(average_state_drops.head())\n",
        "print(f\"\\nShape of average state forest drops: {average_state_drops.shape}\")"
      ],
      "metadata": {
        "id": "40mnS8FhnpWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## County Wise Visualizations"
      ],
      "metadata": {
        "id": "HWYaowHa4-Tj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49886c42"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Ensure the dataframe is sorted for better visualization\n",
        "average_state_drops = average_state_drops.sort_values(by='average_drop', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='state_geoId', y='average_drop', data=average_state_drops, palette='viridis')\n",
        "plt.title('Average Forest Canopy Drop by State (2015-2019)')\n",
        "plt.xlabel('State geoId')\n",
        "plt.ylabel('Average Forest Canopy Drop (%)')\n",
        "plt.xticks(rotation=45, ha='right') # Rotate labels for better readability\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.scatterplot(\n",
        "    data=all_states_top_10_declining,\n",
        "    x='Forest_Cover_Percent_2015',\n",
        "    y='Forest_Cover_Percent_2019',\n",
        "    size='Forest_Cover_Drop',\n",
        "    hue='state_geoId',\n",
        "    sizes=(100, 1000),  # Adjust bubble size range as needed\n",
        "    alpha=0.7\n",
        ")\n",
        "\n",
        "plt.title('County Deforestation (2015-2019) by State')\n",
        "plt.xlabel('Forest Cover Percent (2015)')\n",
        "plt.ylabel('Forest Cover Percent (2019)')\n",
        "plt.grid(True)\n",
        "plt.legend(title='State geoId', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4rn-08Nk3N_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af1ba112"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(data=all_states_top_10_declining, x='state_geoId', y='Forest_Cover_Drop', palette='viridis', hue='state_geoId', legend=False)\n",
        "plt.title('Distribution of Forest Cover Drop by State (2015-2019)')\n",
        "plt.xlabel('State geoId')\n",
        "plt.ylabel('Forest Cover Drop (%)')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Assuming get_forest_cover_dataframe is defined in a previous cell\n",
        "\n",
        "def get_all_years_forest_data_for_state(state_geoId: str, start_year: int = 2015, end_year: int = 2019) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Fetches and combines forest cover data for all years in a specified range\n",
        "    for a single state.\n",
        "\n",
        "    Parameters:\n",
        "    - state_geoId: The geoId of the state (e.g., \"geoId/13\" for Georgia)\n",
        "    - start_year: The starting year (inclusive)\n",
        "    - end_year: The ending year (inclusive)\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame with forest cover data for all years and counties in the state,\n",
        "      or an empty DataFrame if data fetching fails or no data is found.\n",
        "    \"\"\"\n",
        "    print(f\"\\nFetching and combining forest data for state: {state_geoId} from {start_year} to {end_year}\")\n",
        "\n",
        "    # Get forest cover data for the state's counties\n",
        "    df_state_forest = get_forest_cover_dataframe(state_geoId, level=\"County\")\n",
        "\n",
        "    if df_state_forest.empty:\n",
        "        print(f\"No forest data found for {state_geoId}. Skipping.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    years_to_include = range(start_year, end_year + 1)\n",
        "    all_years_data = pd.DataFrame()\n",
        "\n",
        "    # Filter for the desired years and concatenate\n",
        "    for year in years_to_include:\n",
        "        df_year = df_state_forest[df_state_forest[\"date\"].astype(int) == year].copy()\n",
        "\n",
        "        if not df_year.empty:\n",
        "            all_years_data = pd.concat([all_years_data, df_year], ignore_index=True)\n",
        "            print(f\"Added data for year {year}. Current shape: {all_years_data.shape}\")\n",
        "        else:\n",
        "            print(f\"No data found for year {year} for state {state_geoId}.\")\n",
        "\n",
        "    # Rename columns for clarity after combining\n",
        "    all_years_data = all_years_data.rename(columns={\n",
        "        \"entity\": \"Fips\",\n",
        "        \"entity_name\": \"county_name\",\n",
        "        \"date\": \"year\",\n",
        "        \"value\": \"Forest_Cover_Percent\"\n",
        "    })\n",
        "\n",
        "    # Add state geoId column\n",
        "    if not all_years_data.empty:\n",
        "         all_years_data['state_geoId'] = state_geoId\n",
        "\n",
        "\n",
        "    print(f\"\\nCombined forest cover data for {state_geoId} ({start_year}-{end_year}):\")\n",
        "    display(all_years_data.head())\n",
        "    display(all_years_data.tail())\n",
        "\n",
        "    return all_years_data\n",
        "\n",
        "# Example Usage:\n",
        "# Replace \"geoId/13\" with the geoId of the state you want to process\n",
        "state_to_process = \"geoId/36\" # Example: New York\n",
        "\n",
        "all_years_state_forest_data = get_all_years_forest_data_for_state(state_to_process, start_year=2015, end_year=2019)"
      ],
      "metadata": {
        "id": "J3zNn8InBBoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "from functools import reduce # Import reduce\n",
        "\n",
        "years_to_study = range(2015, 2020)\n",
        "yearly_forest_data = {}\n",
        "\n",
        "for year in years_to_study:\n",
        "    # Ensure 'year' is treated as integer for filtering\n",
        "    df_year = df_forest[df_forest[\"year\"].astype(int) == year].copy()\n",
        "    yearly_forest_data[year] = df_year\n",
        "    print(f\"Created dataframe for year {year} with shape: {df_year.shape}\")\n",
        "\n",
        "# You can access the dataframes using the dictionary, e.g., yearly_forest_data[2015]\n",
        "\n",
        "df_2015 = yearly_forest_data[2015]\n",
        "df_2016 = yearly_forest_data[2016]\n",
        "df_2017 = yearly_forest_data[2017]\n",
        "df_2018 = yearly_forest_data[2018]\n",
        "df_2019 = yearly_forest_data[2019]\n",
        "\n",
        "# List of dataframes to merge\n",
        "dfs_to_merge = [df_2015, df_2016, df_2017, df_2018, df_2019]\n",
        "\n",
        "# Define a function to merge two dataframes\n",
        "def merge_dfs(left, right):\n",
        "    # Extract year from the right dataframe's columns (assuming 'year' is still a column before renaming)\n",
        "    # Or, more reliably, use the year from the yearly_forest_data dictionary keys if available\n",
        "    # Let's assume we rename columns before merging to avoid conflicts and keep track of the year\n",
        "    year_right = right['year'].iloc[0] # Get the year from the 'year' column\n",
        "    right_renamed = right[['Fips', 'Forest_Cover_Percent']].rename(columns={'Forest_Cover_Percent': f'Forest_Cover_{year_right}'})\n",
        "\n",
        "    # Merge on 'Fips'\n",
        "    return pd.merge(left, right_renamed, on='Fips', how='outer')\n",
        "\n",
        "# Rename columns of the first dataframe before starting the merge\n",
        "df_2015_renamed = df_2015[['Fips', 'county_name', 'Forest_Cover_Percent']].rename(columns={'Forest_Cover_Percent': 'Forest_Cover_2015'})\n",
        "\n",
        "\n",
        "# Use reduce to apply the merge function sequentially\n",
        "# Start with the first renamed dataframe\n",
        "df_merged_all_years = reduce(merge_dfs, dfs_to_merge[1:], df_2015_renamed)\n",
        "\n",
        "print(\"\\nMerged DataFrame with forest cover for all years:\")\n",
        "display(df_merged_all_years.head())"
      ],
      "metadata": {
        "id": "oPzaEF0wC1Zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation for Analysis"
      ],
      "metadata": {
        "id": "2-1HkjOREDXn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dRS1lttZEZT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48eb0873"
      },
      "source": [
        "# Task\n",
        "Prepare the data from `df_merged_all_years` for machine learning analysis using EDA, feature sensitivity, and other related techniques, and provide required visualizations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2f6b92a"
      },
      "source": [
        "## Perform basic eda\n",
        "\n",
        "### Subtask:\n",
        "Display the head, info, and descriptive statistics of `df_merged_all_years` to understand its structure and content.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bda7c70d"
      },
      "source": [
        "**Reasoning**:\n",
        "Display the head, info, and descriptive statistics of the `df_merged_all_years` DataFrame to understand its structure and content as requested by the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e02f52d0"
      },
      "source": [
        "display(df_merged_all_years.head())\n",
        "df_merged_all_years.info()\n",
        "display(df_merged_all_years.describe().T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f245d2e"
      },
      "source": [
        "## Analyze missing values\n",
        "\n",
        "### Subtask:\n",
        "Check for and visualize the distribution of missing values in the DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56eb8ab7"
      },
      "source": [
        "**Reasoning**:\n",
        "Calculate and display the number of missing values per column in the DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f89d0800"
      },
      "source": [
        "missing_values = df_merged_all_years.isnull().sum()\n",
        "print(\"Missing values per column:\")\n",
        "print(missing_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2bb2e75"
      },
      "source": [
        "## Handle missing values\n",
        "\n",
        "### Subtask:\n",
        "Implement a strategy to handle missing values, such as imputation or removal.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91d27823"
      },
      "source": [
        "print(\"Based on the previous analysis, no missing values were found in the df_merged_all_years DataFrame.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcd79ada"
      },
      "source": [
        "## Feature engineering (if necessary)\n",
        "\n",
        "### Subtask:\n",
        "Create new features from existing ones in `df_merged_all_years` that might be relevant for the analysis of declining tree canopy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c904fd9"
      },
      "source": [
        "**Reasoning**:\n",
        "Calculate new features based on the forest cover percentages across the years and display the updated DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27e57698"
      },
      "source": [
        "# 1. Calculate total forest cover change from 2015 to 2019\n",
        "df_merged_all_years['Forest_Cover_Change_2015_2019'] = df_merged_all_years['Forest_Cover_2019'] - df_merged_all_years['Forest_Cover_2015']\n",
        "\n",
        "# 2. Calculate the average forest cover across all years (2015-2019)\n",
        "forest_cover_columns = ['Forest_Cover_2015', 'Forest_Cover_2016', 'Forest_Cover_2017', 'Forest_Cover_2018', 'Forest_Cover_2019']\n",
        "df_merged_all_years['Average_Forest_Cover'] = df_merged_all_years[forest_cover_columns].mean(axis=1)\n",
        "\n",
        "# 3. Calculate the standard deviation of forest cover across all years (2015-2019)\n",
        "df_merged_all_years['Forest_Cover_Std_Dev'] = df_merged_all_years[forest_cover_columns].std(axis=1)\n",
        "\n",
        "# 4. Display the head of the updated DataFrame\n",
        "display(df_merged_all_years.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2830cc79"
      },
      "source": [
        "## Target variable analysis\n",
        "\n",
        "### Subtask:\n",
        "Analyze the distribution and characteristics of the target variable (`Forest_Cover_Change_2015_2019`) in `df_merged_all_years`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c7ec6ed"
      },
      "source": [
        "**Reasoning**:\n",
        "Display the data type and basic descriptive statistics of the target variable, then plot its distribution and calculate skewness and kurtosis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2644ec48"
      },
      "source": [
        "# 1. Display data type and basic descriptive statistics\n",
        "print(\"Data type of 'Forest_Cover_Change_2015_2019':\", df_merged_all_years['Forest_Cover_Change_2015_2019'].dtype)\n",
        "print(\"\\nDescriptive Statistics of 'Forest_Cover_Change_2015_2019':\")\n",
        "display(df_merged_all_years['Forest_Cover_Change_2015_2019'].describe())\n",
        "\n",
        "# 2. Plot a histogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(df_merged_all_years['Forest_Cover_Change_2015_2019'], bins=30, color='skyblue', edgecolor='black')\n",
        "plt.title('Distribution of Forest Cover Change (2015-2019)')\n",
        "plt.xlabel('Forest Cover Change (%)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.show()\n",
        "\n",
        "# 3. Calculate and print skewness and kurtosis\n",
        "skewness = df_merged_all_years['Forest_Cover_Change_2015_2019'].skew()\n",
        "kurtosis = df_merged_all_years['Forest_Cover_Change_2015_2019'].kurtosis()\n",
        "print(f\"\\nSkewness of 'Forest_Cover_Change_2015_2019': {skewness:.4f}\")\n",
        "print(f\"Kurtosis of 'Forest_Cover_Change_2015_2019': {kurtosis:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65c610ae"
      },
      "source": [
        "## Perform feature sensitivity/correlation analysis\n",
        "\n",
        "### Subtask:\n",
        "Analyze the relationships between features and the target variable, and among features themselves, using correlation matrices. Visualize these relationships.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "557bd00a"
      },
      "source": [
        "**Reasoning**:\n",
        "Select numerical columns, calculate the correlation matrix, and visualize it using a heatmap to understand relationships among features and with the target variable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c6c8b37"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the `numpy` library was not imported. Import `numpy` and rerun the code to select numerical columns, calculate and visualize the correlation matrix, and analyze correlations with the target variable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySVU6eWlGBUp"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Select numerical columns, excluding 'Fips' and 'county_name'\n",
        "numerical_df = df_merged_all_years.select_dtypes(include=np.number)\n",
        "\n",
        "# 2. Calculate the Pearson correlation matrix\n",
        "correlation_matrix = numerical_df.corr()\n",
        "\n",
        "# 3. Generate a heatmap of the correlation matrix\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True)\n",
        "plt.title('Correlation Matrix of Numerical Features')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 4. Calculate the correlation between each numerical feature and the target variable\n",
        "target_correlation = correlation_matrix['Forest_Cover_Change_2015_2019'].sort_values(ascending=False)\n",
        "\n",
        "# 5. Display the correlation coefficients with the target\n",
        "print(\"\\nCorrelation with 'Forest_Cover_Change_2015_2019':\")\n",
        "display(target_correlation)\n",
        "\n",
        "# 6. Generate a bar plot for top N features' correlation with the target\n",
        "# Excluding the target variable itself from the plot\n",
        "target_correlation_for_plot = target_correlation.drop('Forest_Cover_Change_2015_2019')\n",
        "top_n = 10  # Choose N\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=target_correlation_for_plot.head(top_n).values,\n",
        "            y=target_correlation_for_plot.head(top_n).index,\n",
        "            palette='viridis',\n",
        "            hue=target_correlation_for_plot.head(top_n).index,\n",
        "            legend=False)\n",
        "plt.title(f'Top {top_n} Features Correlated with Forest Cover Change (2015-2019)')\n",
        "plt.xlabel('Pearson Correlation Coefficient')\n",
        "plt.ylabel('Feature')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=target_correlation_for_plot.tail(top_n).values,\n",
        "            y=target_correlation_for_plot.tail(top_n).index,\n",
        "            palette='magma',\n",
        "            hue=target_correlation_for_plot.tail(top_n).index,\n",
        "            legend=False)\n",
        "plt.title(f'Bottom {top_n} Features Correlated with Forest Cover Change (2015-2019)')\n",
        "plt.xlabel('Pearson Correlation Coefficient')\n",
        "plt.ylabel('Feature')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37f99ae2"
      },
      "source": [
        "## Visualize feature distributions\n",
        "\n",
        "### Subtask:\n",
        "Plot histograms for a selection of numerical features in `df_merged_all_years` to visualize their distributions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d7cf6ca"
      },
      "source": [
        "**Reasoning**:\n",
        "Select the numerical columns for plotting histograms and then iterate through them to create and display each histogram.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0f8d73f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Select numerical columns, excluding identifier columns\n",
        "numerical_columns_for_hist = [col for col in df_merged_all_years.select_dtypes(include=np.number).columns if col not in ['Fips', 'county_name', 'Forest_Cover_Change_2015_2019', 'Average_Forest_Cover', 'Forest_Cover_Std_Dev']]\n",
        "\n",
        "# Iterate through selected columns and plot histograms\n",
        "for col in numerical_columns_for_hist:\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.hist(df_merged_all_years[col], bins=30, color='teal', edgecolor='black')\n",
        "    plt.title(f'Distribution of {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(axis='y', alpha=0.75)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b33d7e3"
      },
      "source": [
        "## Split data\n",
        "\n",
        "### Subtask:\n",
        "Split the data into training and testing sets for model building and evaluation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89a6848a"
      },
      "source": [
        "**Reasoning**:\n",
        "Define features and target, then split the data into training and testing sets and print the shapes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a60499a"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because 'Fips' was not in the numerical columns after selecting only numeric types. Need to drop 'Fips' and 'county_name' from the original dataframe before selecting numerical columns.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp3zQtELGW5P"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Drop non-numerical and target columns before selecting numerical types\n",
        "features_df = df_merged_all_years.drop(columns=['Fips', 'county_name', 'Forest_Cover_Change_2015_2019'])\n",
        "\n",
        "# Define features (X) by selecting numerical columns\n",
        "X = features_df.select_dtypes(include=np.number)\n",
        "\n",
        "# Define target (y)\n",
        "y = df_merged_all_years['Forest_Cover_Change_2015_2019']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.2,  # 20% for testing\n",
        "    random_state=42 # for reproducibility\n",
        ")\n",
        "\n",
        "# Print the shapes of the resulting splits\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26f8613d"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The initial EDA revealed that the `df_merged_all_years` DataFrame contains 159 entries and no missing values across all columns.\n",
        "*   On average, there was a decrease in forest cover between 2015 and 2019, with the `Forest_Cover_Change_2015_2019` having a mean of -2.78. The change ranged from a decrease of approximately 7.83% to an increase of approximately 1.53%.\n",
        "*   The distribution of `Forest_Cover_Change_2015_2019` is slightly left-skewed (skewness: -0.2639) and slightly platykurtic (kurtosis: -0.4016).\n",
        "*   Features like `Forest_Cover_2015`, `Forest_Cover_Std_Dev`, and forest cover in other years (`2016` to `2019`) show strong negative correlations with the `Forest_Cover_Change_2015_2019`.\n",
        "*   The data was successfully split into training (80%) and testing (20%) sets, with the shapes of the resulting sets printed for verification.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The strong negative correlation between initial forest cover (`Forest_Cover_2015`) and the change in forest cover suggests that areas with higher initial forest cover might have experienced larger absolute declines. This could be a key factor to explore further in modeling.\n",
        "*   The absence of missing values simplifies the initial data preparation. The engineered features (`Forest_Cover_Change_2015_2019`, `Average_Forest_Cover`, `Forest_Cover_Std_Dev`) capture different aspects of forest cover dynamics and should be valuable for predictive modeling.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine Learning on Data"
      ],
      "metadata": {
        "id": "W6CY12aWNHji"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7IHOH9VfNR1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5de8656"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df_merged_all_years is available from previous steps\n",
        "\n",
        "# Define thresholds for categorization (these can be adjusted)\n",
        "# For example, classify based on quantiles or domain knowledge\n",
        "change_column = df_merged_all_years['Forest_Cover_Change_2015_2019']\n",
        "\n",
        "# Example categorization based on ranges:\n",
        "# Significant Loss: e.g., change < -4%\n",
        "# Slight Loss/Stable: e.g., -4% <= change <= 1%\n",
        "# Gain: e.g., change > 1%\n",
        "\n",
        "# Let's determine some example thresholds based on the distribution\n",
        "# Using percentiles can be a robust way if specific domain thresholds aren't defined\n",
        "loss_threshold = change_column.quantile(0.25) # e.g., counties in the bottom 25% of change\n",
        "gain_threshold = change_column.quantile(0.75) # e.g., counties in the top 25% of change\n",
        "\n",
        "print(f\"Using thresholds based on quantiles: Loss < {loss_threshold:.2f}%, Gain > {gain_threshold:.2f}%\")\n",
        "\n",
        "\n",
        "def categorize_change(change, loss_thresh, gain_thresh):\n",
        "    if change < loss_thresh:\n",
        "        return 'Significant Loss'\n",
        "    elif change > gain_thresh:\n",
        "        return 'Gain'\n",
        "    else:\n",
        "        return 'Slight Loss/Stable'\n",
        "\n",
        "# Apply the categorization function\n",
        "df_merged_all_years['Canopy_Change_Category'] = df_merged_all_years['Forest_Cover_Change_2015_2019'].apply(\n",
        "    lambda x: categorize_change(x, loss_threshold, gain_threshold)\n",
        ")\n",
        "\n",
        "# Display the counts for each category\n",
        "print(\"\\nCounts of counties in each canopy change category:\")\n",
        "display(df_merged_all_years['Canopy_Change_Category'].value_counts())\n",
        "\n",
        "# Display the DataFrame with the new category column\n",
        "print(\"\\nDataFrame head with Canopy_Change_Category:\")\n",
        "display(df_merged_all_years.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming df_merged_all_years DataFrame with 'Forest_Cover_Change_2015_2019' and 'county_name' exists\n",
        "\n",
        "# Sort the DataFrame by the forest cover change to highlight largest drops/gains\n",
        "df_sorted_change = df_merged_all_years.sort_values(by='Forest_Cover_Change_2015_2019', ascending=False)\n",
        "\n",
        "# Select a reasonable number of top and bottom counties to visualize if there are many\n",
        "# For simplicity, let's visualize all if the number of counties is manageable, or top/bottom N otherwise\n",
        "# Let's set a threshold, e.g., if more than 50 counties, show top/bottom 25\n",
        "num_counties = len(df_sorted_change)\n",
        "if num_counties > 50:\n",
        "    top_n = 25\n",
        "    bottom_n = 25\n",
        "    df_viz = pd.concat([df_sorted_change.head(top_n), df_sorted_change.tail(bottom_n)])\n",
        "else:\n",
        "    df_viz = df_sorted_change\n",
        "\n",
        "# Create the bar chart\n",
        "plt.figure(figsize=(12, max(6, len(df_viz) * 0.3))) # Adjust figure size based on number of counties\n",
        "sns.barplot(data=df_viz, x='Forest_Cover_Change_2015_2019', y='county_name', palette='coolwarm', hue='county_name', legend=False)\n",
        "\n",
        "plt.title('Forest Cover Change (2015-2019) by County')\n",
        "plt.xlabel('Forest Cover Change (%)')\n",
        "plt.ylabel('County Name')\n",
        "plt.grid(axis='x', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cdPFDEC2WrO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attempts at Logistic Regression Based on Available Data (Canopy) - Savar"
      ],
      "metadata": {
        "id": "wXzH7LhDYnfS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad135f00"
      },
      "source": [
        "## Reshape data\n",
        "\n",
        "### Subtask:\n",
        "Melt the `df_merged_all_years` DataFrame to transform the yearly forest cover columns into a single 'Year' column and a 'Forest_Cover' column.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71566835"
      },
      "source": [
        "**Reasoning**:\n",
        "Melt the DataFrame to transform the yearly forest cover columns into a long format as requested by the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43d53730"
      },
      "source": [
        "# Melt the DataFrame to long format\n",
        "df_long = pd.melt(\n",
        "    df_merged_all_years,\n",
        "    id_vars=['Fips', 'county_name', 'Forest_Cover_Change_2015_2019', 'Average_Forest_Cover', 'Forest_Cover_Std_Dev', 'Canopy_Change_Category'],\n",
        "    var_name='Year',\n",
        "    value_name='Forest_Cover'\n",
        ")\n",
        "\n",
        "# Display the head of the long-format DataFrame to verify\n",
        "display(df_long.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "159ed326"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because `df_merged_all_years` was not defined. I need to re-run the code that creates `df_merged_all_years` before melting it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7d740d3"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from statsmodels.tsa.holtwinters import Holt\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Ensure 'Year' is treated as a numerical feature for modeling\n",
        "# Extract the year number from the string and convert to integer\n",
        "df_long['Year'] = df_long['Year'].str.extract(r'_(\\d{4})').astype(int)\n",
        "\n",
        "# For time series modeling, it's often useful to have a single time series.\n",
        "# Let's aggregate the data to get the average forest cover per year across all counties.\n",
        "# This simplifies the forecasting for demonstration purposes.\n",
        "# If county-level forecasts are needed, the approach would be different (e.g., loop through counties or use a hierarchical model).\n",
        "average_yearly_forest_cover = df_long.groupby('Year')['Forest_Cover'].mean().reset_index()\n",
        "\n",
        "# Ensure the index is the year for time series models\n",
        "average_yearly_forest_cover = average_yearly_forest_cover.set_index('Year')\n",
        "\n",
        "print(\"Average yearly forest cover data prepared for modeling:\")\n",
        "display(average_yearly_forest_cover.head())\n",
        "\n",
        "# Prepare data for linear regression\n",
        "X_lr = average_yearly_forest_cover.index.values.reshape(-1, 1) # Years as feature\n",
        "y_lr = average_yearly_forest_cover['Forest_Cover'].values       # Forest Cover as target\n",
        "\n",
        "# Prepare data for Holt's Exponential Smoothing\n",
        "# Holt's model works directly on the time series data (y_lr in this case)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try Basic Regression and Holt Smoothing"
      ],
      "metadata": {
        "id": "2WMkbLjLgDql"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41fd12e8"
      },
      "source": [
        "# Fit the Linear Regression model\n",
        "linear_model = LinearRegression()\n",
        "linear_model.fit(X_lr, y_lr)\n",
        "\n",
        "print(\"Linear Regression model fitted successfully.\")\n",
        "print(f\"Intercept: {linear_model.intercept_:.4f}\")\n",
        "print(f\"Coefficient: {linear_model.coef_[0]:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ab664a8"
      },
      "source": [
        "# Forecast for the next 5 years using the linear model\n",
        "last_year = average_yearly_forest_cover.index.max()\n",
        "future_years = np.arange(last_year + 1, last_year + 6).reshape(-1, 1)\n",
        "linear_forecast = linear_model.predict(future_years)\n",
        "\n",
        "print(\"Linear regression forecast for the next 5 years:\")\n",
        "for year, forecast in zip(future_years.flatten(), linear_forecast):\n",
        "    print(f\"Year {year}: {forecast:.4f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99a7002d"
      },
      "source": [
        "from statsmodels.tsa.holtwinters import Holt\n",
        "import pandas as pd\n",
        "\n",
        "# Fit Holt's Exponential Smoothing model\n",
        "# The Holt's method is for data with a trend.\n",
        "# We use the 'add' trend type as the trend appears additive.\n",
        "\n",
        "# Create a time series object with a DatetimeIndex, as expected by statsmodels\n",
        "# The data is yearly, so a frequency of 'AS' (Annual Start) is appropriate.\n",
        "time_series_data = pd.Series(\n",
        "    average_yearly_forest_cover['Forest_Cover'].values,\n",
        "    index=pd.to_datetime(average_yearly_forest_cover.index, format='%Y')\n",
        ")\n",
        "time_series_data.index.freq = 'AS' # Set the frequency\n",
        "\n",
        "holt_model = Holt(time_series_data).fit(smoothing_level = 0.3, smoothing_trend = 0.1) # Parameters can be tuned\n",
        "\n",
        "print(\"\\nHolt's Exponential Smoothing model fitted successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb0cb9a1"
      },
      "source": [
        "# Forecast for the next 5 years using the Holt's model\n",
        "# The forecast method needs the number of steps to forecast\n",
        "holt_forecast = holt_model.forecast(steps=5)\n",
        "\n",
        "print(\"\\nHolt's Exponential Smoothing forecast for the next 5 years:\")\n",
        "display(holt_forecast)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3a10c89"
      },
      "source": [
        "# IMPORTANT: Please avoid hardcoding Forest_Cover in this colab.\n",
        "# Send in through parameters.yaml settings.\n",
        "\n",
        "# Visualize the historical data and the fitted/forecasted lines from both models\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "\n",
        "# Plot historical data\n",
        "plt.plot(average_yearly_forest_cover.index, average_yearly_forest_cover['Forest_Cover'], marker='o', linestyle='-', color='blue', label='Historical Data (2015-2019)')\n",
        "\n",
        "# Create years for plotting fitted lines (2015 to 2024)\n",
        "years_for_plotting = np.arange(average_yearly_forest_cover.index.min(), future_years.max() + 1).reshape(-1, 1)\n",
        "\n",
        "# Plot Linear Regression fitted and forecasted line\n",
        "linear_fitted_and_forecast = linear_model.predict(years_for_plotting)\n",
        "plt.plot(years_for_plotting.flatten(), linear_fitted_and_forecast, linestyle='--', color='red', label='Linear Regression Fit & Forecast (2015-2024)')\n",
        "\n",
        "# Plot Holt's Exponential Smoothing fitted and forecasted line\n",
        "# Get fitted values for the historical period and concatenate with the forecast\n",
        "holt_fitted = holt_model.fittedvalues\n",
        "holt_fitted_and_forecast_index = holt_fitted.index.union(holt_forecast.index)\n",
        "holt_fitted_and_forecast_values = np.concatenate([holt_fitted.values, holt_forecast.values])\n",
        "\n",
        "# Align the combined fitted and forecasted values with the full range of years for plotting\n",
        "holt_plot_series = pd.Series(holt_fitted_and_forecast_values, index=holt_fitted_and_forecast_index).reindex(pd.to_datetime(years_for_plotting.flatten(), format='%Y'))\n",
        "\n",
        "\n",
        "plt.plot(holt_plot_series.index.year, holt_plot_series.values, linestyle='--', color='green', label=\"Holt's Exponential Smoothing Fit & Forecast (2015-2024)\")\n",
        "\n",
        "\n",
        "plt.title('Forest Canopy Cover Forecast (2015-2024) with Model Fits')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Average Forest Cover (%)')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.xticks(np.arange(average_yearly_forest_cover.index.min(), future_years.max() + 1, 1)) # Ensure all years are shown on x-axis\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Polynomial Regression"
      ],
      "metadata": {
        "id": "2_9CMk5qgUIi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a00e133"
      },
      "source": [
        "# IMPORTANT: Please avoid hardcoding Forest_Cover in this colab.\n",
        "# Send in through parameters.yaml settings.\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming average_yearly_forest_cover is available from previous steps\n",
        "# Ensure Year is the index and Forest_Cover is the value\n",
        "# Check if the index is already integer type, if not convert it\n",
        "if not pd.api.types.is_integer_dtype(average_yearly_forest_cover.index):\n",
        "     average_yearly_forest_cover.index = average_yearly_forest_cover.index.astype(int)\n",
        "\n",
        "# Prepare data for modeling\n",
        "# X should be the years, y should be the forest cover\n",
        "X_hist = average_yearly_forest_cover.index.values.reshape(-1, 1)\n",
        "y_hist = average_yearly_forest_cover['Forest_Cover'].values\n",
        "\n",
        "# Define years for forecasting (next 5 years)\n",
        "last_year = average_yearly_forest_cover.index.max()\n",
        "future_years_arr = np.arange(last_year + 1, last_year + 6).reshape(-1, 1)\n",
        "\n",
        "# Combine historical and future years for plotting\n",
        "all_years_arr = np.arange(average_yearly_forest_cover.index.min(), future_years_arr.max() + 1).reshape(-1, 1)\n",
        "\n",
        "# --- Fit Polynomial Models ---\n",
        "\n",
        "# Linear Model (Degree 1)\n",
        "linear_model = make_pipeline(PolynomialFeatures(1), LinearRegression())\n",
        "linear_model.fit(X_hist, y_hist)\n",
        "linear_forecast_all_years = linear_model.predict(all_years_arr)\n",
        "\n",
        "# Quadratic Model (Degree 2)\n",
        "quadratic_model = make_pipeline(PolynomialFeatures(2), LinearRegression())\n",
        "quadratic_model.fit(X_hist, y_hist)\n",
        "quadratic_forecast_all_years = quadratic_model.predict(all_years_arr)\n",
        "\n",
        "\n",
        "# Cubic Model (Degree 3)\n",
        "cubic_model = make_pipeline(PolynomialFeatures(3), LinearRegression())\n",
        "cubic_model.fit(X_hist, y_hist)\n",
        "cubic_forecast_all_years = cubic_model.predict(all_years_arr)\n",
        "\n",
        "\n",
        "print(\"Polynomial models fitted successfully.\")\n",
        "\n",
        "# --- Visualize Results ---\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "\n",
        "# Plot historical data\n",
        "plt.plot(average_yearly_forest_cover.index, average_yearly_forest_cover['Forest_Cover'], marker='o', linestyle='-', color='blue', label='Historical Data (2015-2019)')\n",
        "\n",
        "# Plot fitted and forecasted lines for each model\n",
        "plt.plot(all_years_arr.flatten(), linear_forecast_all_years, linestyle='--', color='red', label='Linear Fit & Forecast')\n",
        "plt.plot(all_years_arr.flatten(), quadratic_forecast_all_years, linestyle='--', color='purple', label='Quadratic Fit & Forecast')\n",
        "plt.plot(all_years_arr.flatten(), cubic_forecast_all_years, linestyle='--', color='orange', label='Cubic Fit & Forecast')\n",
        "\n",
        "\n",
        "plt.title('Forest Canopy Cover Forecast (2015-2024) with Polynomial Fits')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Average Forest Cover (%)')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.xticks(all_years_arr.flatten()) # Ensure all years are shown on x-axis\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ARIMA Time Series Specific Regression\n",
        "\n",
        "IMPORTANT: Please avoid hardcoding Forest_Cover in this colab.  \n",
        "Send in through parameters.yaml settings."
      ],
      "metadata": {
        "id": "1ritnL1TgbMA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e60b8e2"
      },
      "source": [
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "# Assuming average_yearly_forest_cover is available from previous steps\n",
        "ts_data = average_yearly_forest_cover['Forest_Cover']\n",
        "\n",
        "# Perform Augmented Dickey-Fuller test\n",
        "result = adfuller(ts_data)\n",
        "\n",
        "print('ADF Statistic: %f' % result[0])\n",
        "print('p-value: %f' % result[1])\n",
        "print('Critical Values:')\n",
        "for key, value in result[4].items():\n",
        "    print('\\t%s: %.3f' % (key, value))\n",
        "\n",
        "# Interpret the result\n",
        "if result[1] <= 0.05:\n",
        "    print(\"\\nResult: The time series is likely stationary (reject H0)\")\n",
        "else:\n",
        "    print(\"\\nResult: The time series is likely non-stationary (fail to reject H0)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fae1464"
      },
      "source": [
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming time_series_data is available from the previous step and has a DatetimeIndex\n",
        "\n",
        "# Fit a simple ARIMA(1, 1, 0) model\n",
        "# (p=1: autoregressive order, d=1: differencing order, q=0: moving average order)\n",
        "# The 'enforce_stationarity' and 'enforce_invertibility' are set to False\n",
        "# to allow fitting even with limited data points, but results should be interpreted with caution.\n",
        "try:\n",
        "    model = ARIMA(time_series_data, order=(1, 1, 0))\n",
        "    arima_result = model.fit()\n",
        "    print(arima_result.summary())\n",
        "except Exception as e:\n",
        "    print(f\"Error fitting ARIMA model: {e}\")\n",
        "    print(\"It might be challenging to fit even a simple ARIMA model with only 5 data points.\")\n",
        "    arima_result = None # Ensure arima_result is None if fitting fails"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fb4e82d"
      },
      "source": [
        "# Forecast for the next 5 years using the fitted ARIMA model\n",
        "if arima_result:\n",
        "    arima_forecast = arima_result.forecast(steps=5)\n",
        "\n",
        "    print(\"\\nARIMA Forecast for the next 5 years:\")\n",
        "    display(arima_forecast)\n",
        "else:\n",
        "    print(\"\\nARIMA model fitting failed, cannot generate forecast.\")\n",
        "    arima_forecast = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c710ef8"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming average_yearly_forest_cover and arima_forecast are available\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "\n",
        "# Plot historical data\n",
        "plt.plot(average_yearly_forest_cover.index, average_yearly_forest_cover['Forest_Cover'], marker='o', linestyle='-', color='blue', label='Historical Data (2015-2019)')\n",
        "\n",
        "# Plot ARIMA forecast\n",
        "if arima_forecast is not None:\n",
        "    # The index of arima_forecast is already in DatetimeIndex format, convert to year for plotting\n",
        "    plt.plot(arima_forecast.index.year, arima_forecast.values, marker='o', linestyle='--', color='purple', label='ARIMA Forecast (2020-2024)')\n",
        "\n",
        "plt.title('Forest Canopy Cover Historical Data and ARIMA Forecast')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Average Forest Cover (%)')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "# Set x-ticks to include all years from historical data start to forecast end\n",
        "all_years = pd.concat([average_yearly_forest_cover.reset_index()['Year'], pd.Series(arima_forecast.index.year)]).unique()\n",
        "plt.xticks(all_years)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Melt the DataFrame to long format\n",
        "df_long = pd.melt(\n",
        "    df_merged_all_years,\n",
        "    id_vars=['Fips', 'county_name', 'Forest_Cover_Change_2015_2019', 'Average_Forest_Cover', 'Forest_Cover_Std_Dev', 'Canopy_Change_Category'],\n",
        "    var_name='Year',\n",
        "    value_name='Forest_Cover'\n",
        ")\n",
        "\n",
        "# Display the head of the long-format DataFrame to verify\n",
        "display(df_long.head())"
      ],
      "metadata": {
        "id": "LI7VMvbeA3i5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "\n",
        "# Ensure Year is numeric\n",
        "df_long[\"Year\"] = df_long[\"Year\"].astype(int)\n",
        "\n",
        "# Container for results\n",
        "all_results = []\n",
        "\n",
        "# Forecast horizon (e.g., 10 years ahead)\n",
        "horizon = 10\n",
        "future_years_template = lambda last_year: np.arange(last_year+1, last_year+horizon+1)\n",
        "\n",
        "# Loop over each county (keep FIPS too)\n",
        "for (fips, county), grp in df_long.groupby([\"Fips\", \"county_name\"]):\n",
        "    grp = grp.sort_values(\"Year\")\n",
        "    X = grp[\"Year\"].values.reshape(-1, 1)\n",
        "    y = grp[\"Forest_Cover\"].values\n",
        "\n",
        "    if len(grp) < 3:\n",
        "        continue  # skip counties with too few data points\n",
        "\n",
        "    # ---- Linear Regression ----\n",
        "    lin_model = LinearRegression()\n",
        "    lin_model.fit(X, y)\n",
        "\n",
        "    # Future predictions\n",
        "    future_years = future_years_template(grp[\"Year\"].max()).reshape(-1,1)\n",
        "    lin_future_preds = lin_model.predict(future_years)\n",
        "\n",
        "    # ---- Holt’s Exponential Smoothing ----\n",
        "    try:\n",
        "        holt_model = ExponentialSmoothing(y, trend=\"add\", seasonal=None).fit()\n",
        "        holt_future_preds = holt_model.forecast(horizon)\n",
        "    except:\n",
        "        holt_future_preds = [np.nan]*horizon\n",
        "\n",
        "    # ---- Historical part (observed fills forecasts) ----\n",
        "    hist_df = pd.DataFrame({\n",
        "        \"Fips\": fips,\n",
        "        \"county_name\": county,\n",
        "        \"Year\": grp[\"Year\"].values,\n",
        "        \"Linear_Forecast\": y,   # observed values\n",
        "        \"Holt_Forecast\": y      # observed values\n",
        "    })\n",
        "\n",
        "    # ---- Future forecasts ----\n",
        "    future_df = pd.DataFrame({\n",
        "        \"Fips\": fips,\n",
        "        \"county_name\": county,\n",
        "        \"Year\": future_years.flatten(),\n",
        "        \"Linear_Forecast\": lin_future_preds,\n",
        "        \"Holt_Forecast\": holt_future_preds\n",
        "    })\n",
        "\n",
        "    # Combine\n",
        "    all_results.append(pd.concat([hist_df, future_df], ignore_index=True))\n",
        "\n",
        "# Final dataset with 2015 → latest year + 10yr forecast\n",
        "all_forecasts = pd.concat(all_results, ignore_index=True)\n",
        "\n",
        "# Ensure only years >= 2015\n",
        "all_forecasts = all_forecasts[all_forecasts[\"Year\"] >= 2015]\n",
        "\n",
        "# Preview\n",
        "display(all_forecasts.head(20))\n"
      ],
      "metadata": {
        "id": "Iv7W5do5LWt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pivot Linear forecasts\n",
        "linear_pivot = all_forecasts.pivot(\n",
        "    index=[\"Fips\", \"county_name\"],\n",
        "    columns=\"Year\",\n",
        "    values=\"Linear_Forecast\"\n",
        ").add_prefix(\"Linear_\")\n",
        "\n",
        "# Pivot Holt forecasts\n",
        "holt_pivot = all_forecasts.pivot(\n",
        "    index=[\"Fips\", \"county_name\"],\n",
        "    columns=\"Year\",\n",
        "    values=\"Holt_Forecast\"\n",
        ").add_prefix(\"Holt_\")\n",
        "\n",
        "# Merge side by side\n",
        "forecasts_wide = pd.concat([linear_pivot, holt_pivot], axis=1).reset_index()\n",
        "\n",
        "# Optional: sort columns for readability\n",
        "non_year_cols = [\"Fips\", \"county_name\"]\n",
        "year_cols = sorted([c for c in forecasts_wide.columns if c not in non_year_cols])\n",
        "forecasts_wide = forecasts_wide.loc[:, non_year_cols + year_cols]\n",
        "\n",
        "display(forecasts_wide.head())"
      ],
      "metadata": {
        "id": "mBM8bR6WNw2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "\n",
        "# Make sure Year is int\n",
        "all_forecasts[\"Year\"] = all_forecasts[\"Year\"].astype(int)\n",
        "\n",
        "# Also grab observed data from df_long for plotting\n",
        "observed = df_long.copy()\n",
        "observed[\"Year\"] = observed[\"Year\"].astype(int)\n",
        "\n",
        "# List of counties\n",
        "counties = sorted(all_forecasts[\"county_name\"].unique())\n",
        "\n",
        "# Create initial figure for the first county\n",
        "init_county = counties[0]\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Observed\n",
        "obs_grp = observed[observed[\"county_name\"] == init_county]\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=obs_grp[\"Year\"], y=obs_grp[\"Forest_Cover\"],\n",
        "    mode=\"markers+lines\", name=\"Observed\", marker=dict(color=\"black\")\n",
        "))\n",
        "\n",
        "# Linear forecast\n",
        "lin_grp = all_forecasts[all_forecasts[\"county_name\"] == init_county]\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=lin_grp[\"Year\"], y=lin_grp[\"Linear_Forecast\"],\n",
        "    mode=\"lines\", name=\"Linear Forecast\", line=dict(dash=\"dash\", color=\"red\")\n",
        "))\n",
        "\n",
        "# Holt forecast\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=lin_grp[\"Year\"], y=lin_grp[\"Holt_Forecast\"],\n",
        "    mode=\"lines\", name=\"Holt Forecast\", line=dict(color=\"blue\")\n",
        "))\n",
        "\n",
        "# Dropdown menu: one button per county\n",
        "dropdown_buttons = []\n",
        "for county in counties:\n",
        "    obs_grp = observed[observed[\"county_name\"] == county]\n",
        "    lin_grp = all_forecasts[all_forecasts[\"county_name\"] == county]\n",
        "\n",
        "    dropdown_buttons.append(\n",
        "        dict(\n",
        "            method=\"update\",\n",
        "            label=county,\n",
        "            args=[\n",
        "                {\"x\": [obs_grp[\"Year\"], lin_grp[\"Year\"], lin_grp[\"Year\"]],\n",
        "                 \"y\": [obs_grp[\"Forest_Cover\"], lin_grp[\"Linear_Forecast\"], lin_grp[\"Holt_Forecast\"]]},\n",
        "                {\"title\": f\"Forest Cover Trends for {county}\"}\n",
        "            ]\n",
        "        )\n",
        "    )\n",
        "\n",
        "fig.update_layout(\n",
        "    title=f\"Forest Cover Trends for {init_county}\",\n",
        "    xaxis_title=\"Year\",\n",
        "    yaxis_title=\"Forest Cover (%)\",\n",
        "    updatemenus=[dict(\n",
        "        active=0,\n",
        "        buttons=dropdown_buttons,\n",
        "        x=1.15,\n",
        "        y=1,\n",
        "        xanchor=\"left\",\n",
        "        yanchor=\"top\"\n",
        "    )]\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "AiYfKZimO6Ph"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "kpjiu4YiD0Lm",
        "SFLk-jXIc-jP",
        "JHNZTLUnjCf5"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d37bdb0080bb454d9857e0234df43eec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1bcacc57d8cd4dda865b2edbc841f0ec",
              "IPY_MODEL_8da877c0a66644ed9d5e26fee6105429",
              "IPY_MODEL_d080b766671f409a928065c2b08b6634",
              "IPY_MODEL_34f08580178a48fd88b3654b0ccf7b38",
              "IPY_MODEL_f3d9b0bf355c4359ba438f068852b329",
              "IPY_MODEL_e592bfb2e4bf4679b483514c77326869"
            ],
            "layout": "IPY_MODEL_bd9eebf384ef458aad0832cedabc6854"
          }
        },
        "1bcacc57d8cd4dda865b2edbc841f0ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "﻿parameters.yaml",
              "parameters-simple.yaml",
              "Parameters for years",
              "Industries by zip code",
              "Eye Blinks"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Params Path",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_27425d2c43da456c9580fef459116649",
            "style": "IPY_MODEL_f8d020ea01c749219e9b33d43ab40c97"
          }
        },
        "8da877c0a66644ed9d5e26fee6105429": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6574e8ecb3844955b48174cdfca3bdec",
              "IPY_MODEL_b09161c3bd204db49eba959714069613"
            ],
            "layout": "IPY_MODEL_7264cdcdbc4e4319b194ad47061fb8b3"
          }
        },
        "d080b766671f409a928065c2b08b6634": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextareaModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextareaModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextareaView",
            "continuous_update": true,
            "description": "Params",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_52dfaaf358064d06a3fdcef285723556",
            "placeholder": "​",
            "rows": null,
            "style": "IPY_MODEL_6ff9d2593af246028c03c13879db8638",
            "value": "folder: naics6-bees-counties\nsave-training: false\nfeatures:\n  data: industries\n  startyear: 2017\n  endyear: 2021\n  naics:\n  - 6\n  state: ME,NY\n  common: Fips\n  path: https://raw.githubusercontent.com/ModelEarth/community-timelines/main/training/naics{naics}/US/counties/{year}/US-{state}-training-naics{naics}-counties-{year}.csv\ntargets:\n  data: bees\n  path: https://raw.githubusercontent.com/ModelEarth/bee-data/main/targets/bees-targets-top-20-percent.csv\n"
          }
        },
        "34f08580178a48fd88b3654b0ccf7b38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2df6a5912e2c4cba91633fd6a6efe312",
              "IPY_MODEL_7af3635b00394a7cb5ca480957ffbbce",
              "IPY_MODEL_055818bbc99440f188f7271326430b5e",
              "IPY_MODEL_f44b82ce215f4e58bef977ced359fc9e",
              "IPY_MODEL_d1a020c51bf446b0b191b02575ab4a17",
              "IPY_MODEL_0fe12007c2df490a909c98c6e85e700e"
            ],
            "layout": "IPY_MODEL_8a994d6c051f4d19bf630a78c97f5d25"
          }
        },
        "f3d9b0bf355c4359ba438f068852b329": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "primary",
            "description": "Update",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_520a294e60e140ae84dc0f639b48b994",
            "style": "IPY_MODEL_e8c10d0d453449ad9ca70e4b914c37ec",
            "tooltip": ""
          }
        },
        "e592bfb2e4bf4679b483514c77326869": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_dc0993f481b54a818195b8a15300f961",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "\n",
                  "\n",
                  "YAML content unchanged since last update.\n",
                  "\n",
                  "URL unchanged: 'https://raw.githubusercontent.com/ModelEarth/RealityStream/main/parameters/parameters.yaml'\n",
                  "\n",
                  "Selected models: ['XGBoost']\n",
                  "Model selection unchanged.\n",
                  "save_pickle set to: False\n",
                  " Loaded XGBoost from xgboost\n",
                  "Parameters saved to report/parameters.yaml\n"
                ]
              }
            ]
          }
        },
        "bd9eebf384ef458aad0832cedabc6854": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27425d2c43da456c9580fef459116649": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8d020ea01c749219e9b33d43ab40c97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6574e8ecb3844955b48174cdfca3bdec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Params From",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_adf6a5e72df24316bf3d02827ac709e0",
            "placeholder": "​",
            "style": "IPY_MODEL_1e0e22369e6b4cf8a18438371c8aa8b2",
            "value": "https://raw.githubusercontent.com/ModelEarth/RealityStream/main/parameters/parameters.yaml"
          }
        },
        "b09161c3bd204db49eba959714069613": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "↓",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_3391185616294d11b7ee53780d743864",
            "style": "IPY_MODEL_184e70e2b24b4732ab3833727bbafa34",
            "tooltip": "Load parameters from URL into editor"
          }
        },
        "7264cdcdbc4e4319b194ad47061fb8b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52dfaaf358064d06a3fdcef285723556": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": "200px",
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "1200px"
          }
        },
        "6ff9d2593af246028c03c13879db8638": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2df6a5912e2c4cba91633fd6a6efe312": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "LR",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_91be8bd4ad3e49398beadfbe9e05b2c8",
            "style": "IPY_MODEL_690b96bc8929435ab50439caaa14eef5",
            "value": false
          }
        },
        "7af3635b00394a7cb5ca480957ffbbce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "RFC",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_e7a1889385b74f9ba8e0afcdc77e44a1",
            "style": "IPY_MODEL_94705b118ca845398982f9f74bc3ffcf",
            "value": false
          }
        },
        "055818bbc99440f188f7271326430b5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "RBF",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_af4144fafcdf48aabad19597fd5b6dba",
            "style": "IPY_MODEL_4971da0d9b834ee38f899adf06c0cbea",
            "value": false
          }
        },
        "f44b82ce215f4e58bef977ced359fc9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "SVM",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_cfab788e51134f18907c4e704afda5df",
            "style": "IPY_MODEL_6f114277db254341832b3242bb40f384",
            "value": false
          }
        },
        "d1a020c51bf446b0b191b02575ab4a17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "MLP",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_0860b3bc18034af89ed280fca64571c6",
            "style": "IPY_MODEL_4c921978f95d43e7bd9b5515963ef925",
            "value": false
          }
        },
        "0fe12007c2df490a909c98c6e85e700e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "XGBoost",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_9d492ee052594641a47162aac5892682",
            "style": "IPY_MODEL_9759c760ec2b42599cafac7488086433",
            "value": true
          }
        },
        "8a994d6c051f4d19bf630a78c97f5d25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "520a294e60e140ae84dc0f639b48b994": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8c10d0d453449ad9ca70e4b914c37ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "adf6a5e72df24316bf3d02827ac709e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "1200px"
          }
        },
        "1e0e22369e6b4cf8a18438371c8aa8b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3391185616294d11b7ee53780d743864": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": "28px",
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": "0 0 0 8px",
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": "28px",
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": "0",
            "right": null,
            "top": null,
            "visibility": null,
            "width": "28px"
          }
        },
        "184e70e2b24b4732ab3833727bbafa34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "91be8bd4ad3e49398beadfbe9e05b2c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "690b96bc8929435ab50439caaa14eef5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7a1889385b74f9ba8e0afcdc77e44a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94705b118ca845398982f9f74bc3ffcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af4144fafcdf48aabad19597fd5b6dba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4971da0d9b834ee38f899adf06c0cbea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cfab788e51134f18907c4e704afda5df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f114277db254341832b3242bb40f384": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0860b3bc18034af89ed280fca64571c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c921978f95d43e7bd9b5515963ef925": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d492ee052594641a47162aac5892682": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9759c760ec2b42599cafac7488086433": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc0993f481b54a818195b8a15300f961": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}